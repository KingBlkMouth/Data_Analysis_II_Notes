[
  {
    "objectID": "z8_learnR.html",
    "href": "z8_learnR.html",
    "title": "LearnR 8",
    "section": "",
    "text": "The topic of this lab is prediction in the generalized linear modeling setting. We’ll first talk about prediction (or classification) in the case of binary categories, and then about classification into \\(k\\) categories. We’ll also talk about prediction in the case of responses that are counts.\nWe’ll look at some R functions that perform cross-validation, and we’ll discuss issues of model comparison on the basis of prediction error.\nTo start off, please load these libraries that you’ll need for this lab. Note that you may have to install some of these packages first.\nlibrary(tidyverse)\nlibrary(openintro) # contains email data\nlibrary(ggplot2)\nlibrary(vcdExtra)\nlibrary(magrittr)\nlibrary(MASS)\nlibrary(lme4)     # access the mixed functions\nlibrary(VGAM)     # contains crash data\nlibrary(tree)     # for classification trees\nlibrary(pROC)     # ROC curves   \nlibrary(boot)     # contains the cv.glm function"
  },
  {
    "objectID": "z8_learnR.html#binary-predictionclassification",
    "href": "z8_learnR.html#binary-predictionclassification",
    "title": "LearnR 8",
    "section": "Binary Prediction/Classification",
    "text": "Binary Prediction/Classification\nWe will use the tree package in R for this part of the lab.\nNote: the email data set has some missing values, which we will ignore to save time. This admittedly is not the wisest thing to do: with an important dataset, you must consider why there are missing values, and take action accordingly. At the very least, in a report, you should tell your readers that you built your model ignoring missing values in the data set.\nFirst, let’s again use 75% of the data for training the model and use the remaining 25% for testing the model that we build, as you saw in the narrated lecture about the Classifier.Rmd code.\n\nset.seed(90210) ## so you get the same results I do\n(n &lt;- dim(email)[1])\n\n[1] 3921\n\n(r &lt;- round(n * .75))\n\n[1] 2941\n\nidx &lt;- 1:n\nnidx &lt;- sample(idx, r, replace = FALSE)\nemail75 &lt;- email[nidx, ]\nemail25 &lt;- email[-nidx, ]\n\nThe tree function follows the same form as a linear regression model: we give the function the response variable to the left of ~ and specify it as a factor so the function knows that the response is not numeric, and then we put all of the predictors to the right of ~.\n\ntr &lt;- tree(as.factor(spam) ~  to_multiple + from + cc + sent_email + \n             image + attach + dollar + winner + inherit +\n             viagra + password + num_char + line_breaks + format +\n             re_subj + exclaim_subj + urgent_subj + exclaim_mess +number, data = email75)\nsummary(tr)\n\n\nClassification tree:\ntree(formula = as.factor(spam) ~ to_multiple + from + cc + sent_email + \n    image + attach + dollar + winner + inherit + viagra + password + \n    num_char + line_breaks + format + re_subj + exclaim_subj + \n    urgent_subj + exclaim_mess + number, data = email75)\nVariables actually used in tree construction:\n[1] \"line_breaks\" \"sent_email\"  \"num_char\"    \"to_multiple\" \"winner\"     \n[6] \"dollar\"      \"format\"      \"number\"     \nNumber of terminal nodes:  14 \nResidual mean deviance:  0.366 = 1071 / 2927 \nMisclassification error rate: 0.08296 = 244 / 2941 \n\n\nThe summary information tells us that there were 8 variables that the function decided were “important” to include in the prediction model: line_breaks, sent_email, num_char, to_multiple, winner, dollar, format, and number (you can see these below the line that says Variables actually used in tree construction in the output above). We also see that on the training data the final tree misclassifies 8.296% of the emails. Let’s plot the tree to have a better idea about how the splits are made.\n\nplot(tr); text(tr)\n\n\n\n\nIf you zoom in on the plot above, you can better see what is happening. All emails are classified as not spam except at 5 of the nodes of the tree. Fr example, starting from the base of the tree, if there are less than 49.5 line breaks, the sent_email variable is 0, and the num_char variable is less than 0.7105, then the model calls the email spam. See if you can trace down to the other 4 nodes on the tree to see what other types of emails the model is classifying as spam.\nLet’s now use our the model suggested by tree to make predictions on the test data:\n\nemail.noresp &lt;- email25[, -1] ## dropping the response variable\nmod1 &lt;- glm(spam~line_breaks+sent_email+num_char+to_multiple+format,data=email75,family=\"binomial\")\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\npredictions &lt;- predict(mod1, email.noresp, type = \"link\")\nphats &lt;- plogis(predictions)\n\nNow, let’s take a look at the ROC Curve for this predictive model and see if we can decide on a good classification rule.\n\nroc(email25$spam,phats,plot=T)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\n\n\n\n\nCall:\nroc.default(response = email25$spam, predictor = phats, plot = T)\n\nData: phats in 892 controls (email25$spam 0) &lt; 88 cases (email25$spam 1).\nArea under the curve: 0.8432\n\n\nIt looks like we can simultaneously maximize specificity and sensitivity if we take the classification cutoff value to be around 0.75 – that is, we’ll classify an email as spam if \\(\\hat{p} \\ge 0.75\\). Then, we can use that to calculate the MSPE.\n\npreds &lt;- ifelse(phats&gt;=0.75,1,0)\n(MSPE &lt;- mean((as.integer(email25$spam) - preds -1)^2))\n\n[1] 0.08979592\n\n\nWe get a MSPE of 0.090. Note that this is slightly higher than the model misclassification rate from the summary(tr). This is typical, as the misclassification rate on the training data should be lower since the training data has an advantage over the test data in that the model is built using the training data.\nThis MSPE is quite similar to the value we calculated for several models in the narrated lectures, indicating that none of these models would necessarily be preferable. Please note that the models that we used in the narrated lectures were trained and tested on a different random split of the data. If we were truly comparing models, we should sure the we used the same training data and test data. The main advantage of the classification tree is its ease of interpretation, especially to someone who might not be familiar with statistics.\n\nWe find the classification tree to be a reasonable way to find a model with good predictive properties. There are certainly other methods of model selection that you can use."
  },
  {
    "objectID": "z8_learnR.html#classification-into-k-categories",
    "href": "z8_learnR.html#classification-into-k-categories",
    "title": "LearnR 8",
    "section": "Classification into \\(k\\) Categories",
    "text": "Classification into \\(k\\) Categories\nClassification trees extend very nicely to incorporating a response variable with more than 2 categories. On the other hand, if taking a more regression oriented approach, we would have to learn multinomial regression. It might not be a bad idea to familiarize yourself with multinomial regression, but the remainder of this section of lab will use classification trees instead.\nTo save the time of looking at a new data set, we will actually just use the email data set again. Now suppose that we want to classify whether emails contain no numbers, a small number, or a big number. I’ll leave it to your own imagination as to why we would want to classify emails into these three categories. Then we will use all of the other variables in the email data set as possible predictors.\nLet’s re-select training and test sets:\n\nset.seed(773355) ## so you get the same results I do\n(n &lt;- dim(email)[1])\n\n[1] 3921\n\n(r &lt;- round(n * .75))\n\n[1] 2941\n\nidx &lt;- 1:n\nnidx &lt;- sample(idx, r, replace = FALSE)\nemail75 &lt;- email[nidx, ]\nemail25 &lt;- email[-nidx, ]\n\nNow, we’ll run the classification tree algorithm using as.factor(number) as the response.\n\ntr2 &lt;- tree(as.factor(number) ~ spam + \n        to_multiple + from + cc + sent_email +\n        image + attach + dollar + winner + inherit +\n        viagra + password + num_char + line_breaks + format +\n        re_subj + exclaim_subj + urgent_subj + exclaim_mess, data = email75)\nsummary(tr2)\n\n\nClassification tree:\ntree(formula = as.factor(number) ~ spam + to_multiple + from + \n    cc + sent_email + image + attach + dollar + winner + inherit + \n    viagra + password + num_char + line_breaks + format + re_subj + \n    exclaim_subj + urgent_subj + exclaim_mess, data = email75)\nVariables actually used in tree construction:\n[1] \"line_breaks\" \"num_char\"    \"re_subj\"     \"spam\"        \"to_multiple\"\n[6] \"format\"     \nNumber of terminal nodes:  8 \nResidual mean deviance:  1.19 = 3490 / 2933 \nMisclassification error rate: 0.2414 = 710 / 2941 \n\n\n\nplot(tr2); text(tr2)\n\n\n\n\nWhen zooming in the plot, we see that the model will not classify any emails as containing a big number, but will for instance classify an email as containing no numbers when the number of line breaks is less than 65.5 and the number of characters is less than 0.5315. You can trace the other branches of the tree to see when emails will be classified as having a small number or having no numbers. The misclassification rate is 0.241.\nIn this approach, we’ll just use the default classification rule for tree objects in the predict.tree function:\n\npredictions &lt;- predict(tr2, email25, type = \"class\")\n\nHere, let’s calculate the misclassification rate – we’ll just sum up the number of times the predicted classification does not match the classification in the email25 data.\n\nsum(predictions != email25$number) / length(predictions)\n\n[1] 0.2285714\n\n\nThe misclassification rate is 0.229, which is actually quite similar to the misclassification rate provided in the output of the summary for the training data for this particular response. While this is unusual, it is not impossible: perhaps in this case, the test data set was just relatively easy to predict.\nWe will stop here with classification, but you can imagine a few extensions. What if we care more about an email being misclassified as having no numbers than an email being misclassified as having a large number? Then we might want to change our model to reflect this preference."
  },
  {
    "objectID": "z6_learnR.html",
    "href": "z6_learnR.html",
    "title": "LearnR 6",
    "section": "",
    "text": "Please load these libraries that you’ll need for this lab:\n\nlibrary(arm)\nlibrary(Sleuth3)\nlibrary(tidyverse)\nlibrary(vcdExtra)\nlibrary(magrittr)\nlibrary(MASS)\nlibrary(pscl)\n\nIn this lab, we’ll cover some more details about zero-inflated and hurdle models. We will discuss why zero-inflated models are sometimes needed, the difference between zero-inflated models and hurdle models, how to compare various types of fitted models, and how to check residuals to assess model assumptions."
  },
  {
    "objectID": "z6_learnR.html#some-data-exploration",
    "href": "z6_learnR.html#some-data-exploration",
    "title": "LearnR 6",
    "section": "Some data exploration",
    "text": "Some data exploration\n\n# ?quine\nschools &lt;- quine\nhead(schools)\n\n\n\n\n\nEth\nSex\nAge\nLrn\nDays\n\n\n\n\nA\nM\nF0\nSL\n2\n\n\nA\nM\nF0\nSL\n11\n\n\nA\nM\nF0\nSL\n14\n\n\nA\nM\nF0\nAL\n5\n\n\nA\nM\nF0\nAL\n5\n\n\nA\nM\nF0\nAL\n13\n\n\n\n\n\n\nFor each child in the data set, we have information on the child’s Ethnicity, Sex, Age, Learning Disability Status, and Days absent from school (which we will treat as the response variable). All of the explanatory variables are categorical with two categories each, except for Age which has four categories.\nWe’ll start with some exploration of the data.\n\nggplot(data = schools, aes(x = Days, y = after_stat(density))) +\n    geom_histogram(bins = 12, colour = \"black\", fill = \"white\") +\n    ggtitle(\"Days Absent from School\")\n\n\n\n\nYou can see from the histogram above that most of the children only missed between 5 and 10 days of school. However, there are perhaps more 0’s than we would expect from a standard Poisson or Negative Binomial Model. It’s rather difficult to tell what will be the best model for these data just based on this simple histogram, but we will investigate the various models proposed in lecture throughout the rest of the lab.\nNext, let’s next create some side-by-side boxplots to explore how each of the categorical explanatory variables is associated with the response.\n\nggplot(data = schools, aes(x = Eth, y = Days)) +\n    geom_boxplot() +\n    ggtitle(\"Days Absent vs. Ethnicity\") +\n    scale_x_discrete(labels = c(\"Aboriginal\", \"Not Aboriginal\"))\n\n\n\n\nHere is looks like there are generally higher numbers of days absent among the Aboriginal students, and there’s also more variability in that group.\n\nggplot(data = schools, aes(x = Sex, y = Days)) +\n    geom_boxplot() +\n    ggtitle(\"Days Absent vs. Gender\") +\n    scale_x_discrete(labels = c(\"Female\", \"Male\"))\n\n\n\n\nIt’s difficult to detect large differences here, though there is some indication of more variation in the distribution of male students.\n\nggplot(data = schools, aes(x = Age, y = Days)) +\n    geom_boxplot() +\n    ggtitle(\"Days Absent vs. Age\") +\n    scale_x_discrete(labels = c(\"Primary\", \"F1\", \"F2\", \"F3\"))\n\n\n\n\nThere seems to be some skewness in the F1 distribution, and you could investigate that further by cross-tabulating Age with some of the other explanatory variables.\n\nggplot(data = schools, aes(x = Lrn, y = Days)) +\n    geom_boxplot() +\n    ggtitle(\"Days Absent vs. Learning\") +\n    scale_x_discrete(labels = c(\"Average Learner\", \"Slow Learner\"))\n\n\n\n\nFinally, there are not clear differences in these two distributions, but we should investigate some more by fitting some models.\nOne thing to notice about many of the boxplots is that there are (at least what R is declaring to be) outliers on the upper end of all of the boxplots. But remember that count distributions tend to be right skewed (just revisit the histogram above!), and so these may not really be outliers, but rather just large counts that we can expect from some count distributions.\nAn as aside, take a look at the help file for the geom_boxplot() function.\n\n# ?geom_boxplot\n\nIf you scroll down in the help file to where it describes the “Computed Variables,” you’ll see that after_stat(ymax) is defined as “upper whisker = largest observation less than or equal to upper hinge + 1.5 * IQR” (“upper hinge” is the 75th percentile). This is a rather standard, though arbitrary rule for declaring something an outlier. Also remember that with Poisson count data, as the counts increase, so does the variance. Therefore, when looking at boxplots of count data, an “outlier” may simply be an indication of large variance, or it may be an indication of over dispersion."
  },
  {
    "objectID": "z6_learnR.html#zero-inflated-models",
    "href": "z6_learnR.html#zero-inflated-models",
    "title": "LearnR 6",
    "section": "Zero-Inflated Models",
    "text": "Zero-Inflated Models\nRecall from the Module 6 lectures that the zero-inflated Poisson regression model is a two part model with:\n\\(logit(\\pi_i) = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\beta_4 X_{4i} + \\beta_5 X_{5i} + \\beta_6 X_{6i}\\)\nand\n\\(log(\\lambda_i) = \\gamma_0 + \\gamma_1 X_{1i} + \\gamma_2 X_{2i} + \\gamma_3 X_{3i} + \\gamma_4 X_{4i} + \\gamma_5 X_{5i} + \\gamma_6 X_{6i}\\)\nwith \\(X_1\\) as the indicator for Ethnicity, \\(X_2\\) as the indicator for Sex, \\(X_3\\), \\(X_4\\), \\(X_5\\) as the three indicator variables for Age, and \\(X_6\\) as the indicator variable for Learning Status.\nNote 1: As usual, for categorical variables, we need \\(k - 1\\) indicator variables in the model for each explanatory variable, where \\(k\\) is the number of categories for a particular explanatory variable.\nIf we expect there to be over dispersion in the counts, we might also consider fitting a zero-inflated negative binomial model. This is really quite similar to the idea of the zero-inflated Poisson model (we even have the same link function, the log-link). The only difference is that the negative binomial model has an extra parameter to estimate and allows for the possibility of over dispersion in the counts."
  },
  {
    "objectID": "z6_learnR.html#hurdle-models",
    "href": "z6_learnR.html#hurdle-models",
    "title": "LearnR 6",
    "section": "Hurdle Models",
    "text": "Hurdle Models\nThe purpose of hurdle models is the same as the purpose of zero-inflated models: to account for excess 0’s. As discussed in the Sarul and Sahin reading, the results of the models can actually give quite different results sometimes.\nNote 2: As with the previous lab, fitting all of the different models we are about to fit does actually qualify as data snooping. In reality, we would want to think about whether a hurdle model or a zero-inflated Poisson model or a zero-inflated negative binomial model, etc., is most reasonable for the particular data we have before doing any model fitting."
  },
  {
    "objectID": "z6_learnR.html#model-evaluation-comparison-and-information-criteria",
    "href": "z6_learnR.html#model-evaluation-comparison-and-information-criteria",
    "title": "LearnR 6",
    "section": "Model Evaluation, Comparison, and Information Criteria",
    "text": "Model Evaluation, Comparison, and Information Criteria\nLet’s first fit a zero-inflated Poisson model using all four covariates, and compare it to the usual poisson regression model.\n\nmod.pois0 &lt;- zeroinfl(Days ~ Eth + Sex + Age + Lrn,\n  dist = \"poisson\", data = schools)\nsummary(mod.pois0)\n\n\nCall:\nzeroinfl(formula = Days ~ Eth + Sex + Age + Lrn, data = schools, dist = \"poisson\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-4.3969 -1.8974 -0.7468  1.4181  9.4340 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.71883    0.06480  41.956  &lt; 2e-16 ***\nEthN        -0.44061    0.04190 -10.517  &lt; 2e-16 ***\nSexM         0.18904    0.04253   4.445 8.77e-06 ***\nAgeF1       -0.32048    0.06968  -4.599 4.24e-06 ***\nAgeF2        0.24602    0.06212   3.960 7.49e-05 ***\nAgeF3        0.43720    0.06781   6.447 1.14e-10 ***\nLrnSL        0.34400    0.05155   6.674 2.50e-11 ***\n\nZero-inflation model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.799218   1.398261  -3.432 0.000599 ***\nEthN         2.061952   1.079103   1.911 0.056030 .  \nSexM         1.010530   0.759919   1.330 0.183588    \nAgeF1       -0.005473   1.050231  -0.005 0.995842    \nAgeF2       -0.326430   1.074230  -0.304 0.761224    \nAgeF3        0.061397   1.119582   0.055 0.956266    \nLrnSL        0.213368   0.862528   0.247 0.804618    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -1047 on 14 Df\n\n\nRecall that when we run the zero-inflated model, we get estimated regression coefficients for the zero-inflated part of the model and separate estimated regression coefficients for the Poisson part of the model.\n\nmod.pois &lt;- glm(Days ~ Eth + Sex + Age + Lrn,\n  family = \"poisson\", data = schools)\nsummary(mod.pois)\n\n\nCall:\nglm(formula = Days ~ Eth + Sex + Age + Lrn, family = \"poisson\", \n    data = schools)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.71538    0.06468  41.980  &lt; 2e-16 ***\nEthN        -0.53360    0.04188 -12.740  &lt; 2e-16 ***\nSexM         0.16160    0.04253   3.799 0.000145 ***\nAgeF1       -0.33390    0.07009  -4.764 1.90e-06 ***\nAgeF2        0.25783    0.06242   4.131 3.62e-05 ***\nAgeF3        0.42769    0.06769   6.319 2.64e-10 ***\nLrnSL        0.34894    0.05204   6.705 2.02e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2073.5  on 145  degrees of freedom\nResidual deviance: 1696.7  on 139  degrees of freedom\nAIC: 2299.2\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the zero-inflated model has a much lower AIC than the usual Poisson model; also, as expected, the zero-inflated model uses twice as many degrees of freedom as the usual Poisson model since we have twice as many parameters to estimate in the zero-inflated model.\nWe can also repeat what we did above using a negative binomial model to account for the (possible) overdispersion.\n\nmod.nb0 &lt;- zeroinfl(Days ~ Eth + Sex + Age + Lrn,\n  dist = \"negbin\", data = schools)\nsummary(mod.nb0)\n\n\nCall:\nzeroinfl(formula = Days ~ Eth + Sex + Age + Lrn, data = schools, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.1836 -0.7060 -0.2712  0.5118  3.5941 \n\nCount model coefficients (negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.88387    0.21756  13.256  &lt; 2e-16 ***\nEthN        -0.50956    0.14979  -3.402  0.00067 ***\nSexM         0.16743    0.15760   1.062  0.28808    \nAgeF1       -0.44161    0.22856  -1.932  0.05335 .  \nAgeF2        0.05128    0.23216   0.221  0.82519    \nAgeF3        0.30937    0.23604   1.311  0.18996    \nLrnSL        0.28252    0.17544   1.610  0.10732    \nLog(theta)   0.39640    0.13171   3.010  0.00262 ** \n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -26.6848   484.4629  -0.055    0.956\nEthN         13.7292   424.5594   0.032    0.974\nSexM         11.7614   233.3545   0.050    0.960\nAgeF1         0.1852     1.4154   0.131    0.896\nAgeF2        -1.1579     1.6862  -0.687    0.492\nAgeF3        -0.7733     1.5920  -0.486    0.627\nLrnSL        -0.6861     1.3114  -0.523    0.601\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 1.4865 \nNumber of iterations in BFGS optimization: 33 \nLog-likelihood: -542.1 on 15 Df\n\n# compare to the negative binomial model without zero-inflation\n\nmod.nb &lt;- glm.nb(Days ~ Eth + Sex + Age + Lrn, data = schools)\nsummary(mod.nb)\n\n\nCall:\nglm.nb(formula = Days ~ Eth + Sex + Age + Lrn, data = schools, \n    init.theta = 1.274892646, link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.89458    0.22842  12.672  &lt; 2e-16 ***\nEthN        -0.56937    0.15333  -3.713 0.000205 ***\nSexM         0.08232    0.15992   0.515 0.606710    \nAgeF1       -0.44843    0.23975  -1.870 0.061425 .  \nAgeF2        0.08808    0.23619   0.373 0.709211    \nAgeF3        0.35690    0.24832   1.437 0.150651    \nLrnSL        0.29211    0.18647   1.566 0.117236    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.2749) family taken to be 1)\n\n    Null deviance: 195.29  on 145  degrees of freedom\nResidual deviance: 167.95  on 139  degrees of freedom\nAIC: 1109.2\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.275 \n          Std. Err.:  0.161 \n\n 2 x log-likelihood:  -1093.151 \n\n\nThe structure of the output for these two models looks very similar to the structure of output for the Poisson regression models, except one additional parameter is estimate in each model (as compared to the analogous Poisson model): \\(\\theta\\), the dispersion parameter. In both models, \\(\\hat\\theta\\) is larger than 1, indicating that there is some over dispersion of the counts. We will test this more formally using a drop-in-deviance test in a moment, but for completeness, let’s also fit the Poisson hurdle model and the negative binomial hurdle model.\n\nmod.pois.hurdle &lt;- hurdle(Days ~ Eth + Sex + Age + Lrn,\n  dist = \"poisson\", data = schools)\nsummary(mod.pois.hurdle)\n\n\nCall:\nhurdle(formula = Days ~ Eth + Sex + Age + Lrn, data = schools, dist = \"poisson\")\n\nPearson residuals:\n   Min     1Q Median     3Q    Max \n-4.397 -1.897 -0.747  1.418  9.434 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.71879    0.06481  41.953  &lt; 2e-16 ***\nEthN        -0.44065    0.04190 -10.517  &lt; 2e-16 ***\nSexM         0.18907    0.04253   4.446 8.76e-06 ***\nAgeF1       -0.32057    0.06969  -4.600 4.23e-06 ***\nAgeF2        0.24600    0.06212   3.960 7.50e-05 ***\nAgeF3        0.43725    0.06782   6.448 1.14e-10 ***\nLrnSL        0.34408    0.05156   6.674 2.49e-11 ***\nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.799162   1.398219   3.432 0.000598 ***\nEthN        -2.062031   1.079091  -1.911 0.056018 .  \nSexM        -1.010397   0.759840  -1.330 0.183601    \nAgeF1        0.005372   1.050158   0.005 0.995918    \nAgeF2        0.326481   1.074203   0.304 0.761182    \nAgeF3       -0.061336   1.119559  -0.055 0.956309    \nLrnSL       -0.213461   0.862480  -0.247 0.804524    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 12 \nLog-likelihood: -1047 on 14 Df\n\nmod.nb.hurdle &lt;- hurdle(Days ~ Eth + Sex + Age + Lrn,\n  dist = \"negbin\", data = schools)\nsummary(mod.nb.hurdle)\n\n\nCall:\nhurdle(formula = Days ~ Eth + Sex + Age + Lrn, data = schools, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.1934 -0.7283 -0.2897  0.4978  3.6377 \n\nCount model coefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.85794    0.21858  13.075  &lt; 2e-16 ***\nEthN        -0.49846    0.15231  -3.273  0.00107 ** \nSexM         0.14525    0.15953   0.910  0.36257    \nAgeF1       -0.45409    0.23094  -1.966  0.04927 *  \nAgeF2        0.07481    0.23408   0.320  0.74926    \nAgeF3        0.35777    0.23801   1.503  0.13280    \nLrnSL        0.31736    0.17734   1.790  0.07352 .  \nLog(theta)   0.39929    0.15466   2.582  0.00983 ** \nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.799162   1.398219   3.432 0.000598 ***\nEthN        -2.062031   1.079091  -1.911 0.056018 .  \nSexM        -1.010397   0.759840  -1.330 0.183601    \nAgeF1        0.005372   1.050158   0.005 0.995918    \nAgeF2        0.326481   1.074203   0.304 0.761182    \nAgeF3       -0.061336   1.119559  -0.055 0.956309    \nLrnSL       -0.213461   0.862480  -0.247 0.804524    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 1.4908\nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -542.5 on 15 Df\n\n\nWe can now compare all six of these models using AIC or BIC to see if any of the models are preferable. Based on intuition, we would expect the negative binomial models to be better than the Poisson models since, from the exploratory analysis, we expected there to be over dispersion in the counts of days missed from school.\n\nAIC(mod.pois0, mod.pois, mod.pois.hurdle, mod.nb0, mod.nb, mod.nb.hurdle)\n\n\n\n\n\n\ndf\nAIC\n\n\n\n\nmod.pois0\n14\n2121.462\n\n\nmod.pois\n7\n2299.184\n\n\nmod.pois.hurdle\n14\n2121.451\n\n\nmod.nb0\n15\n1114.118\n\n\nmod.nb\n8\n1109.151\n\n\nmod.nb.hurdle\n15\n1115.070\n\n\n\n\n\n\nAs expected, the negative binomial models all have much lower AIC than the Poisson models. However, after we account for this over dispersion, the three negative binomial models are relatively similar in terms of AIC. Therefore, we would probably prefer the simplest model (the model that does not incorporate zero-inflation) here.\nWe mentioned above that, sometimes the hurdle model gives very similar estimates and results as the zero-inflated model, but sometimes the results are quite different. We can compare the zero-inflated negative binomial model to the negative binomial hurdle model here to see if there are any major differences in the coefficient estimates and/or their standard errors. Again, this is just an academic exercise, because based on the AIC analysis above, we’d recommend using the model without zero-inflation.\n\ntab1 &lt;- cbind(round(summary(mod.nb0)$coefficients[[1]][, 1], 3),\n    round(summary(mod.nb.hurdle)$coefficients[[1]][, 1], 3))\ncolnames(tab1) &lt;- c(\"Zero-Inf Coefs\", \"Hurdle Coefs\")\ntab1\n\n            Zero-Inf Coefs Hurdle Coefs\n(Intercept)          2.884        2.858\nEthN                -0.510       -0.498\nSexM                 0.167        0.145\nAgeF1               -0.442       -0.454\nAgeF2                0.051        0.075\nAgeF3                0.309        0.358\nLrnSL                0.283        0.317\nLog(theta)           0.396        0.399\n\ntab2 &lt;- cbind(round(summary(mod.nb0)$coefficients[[1]][, 2], 3),\n    round(summary(mod.nb.hurdle)$coefficients[[1]][, 2], 3))\ncolnames(tab2) &lt;- c(\"Zero-Inf SEs\", \"Hurdle SEs\")\ntab2\n\n            Zero-Inf SEs Hurdle SEs\n(Intercept)        0.218      0.219\nEthN               0.150      0.152\nSexM               0.158      0.160\nAgeF1              0.229      0.231\nAgeF2              0.232      0.234\nAgeF3              0.236      0.238\nLrnSL              0.175      0.177\nLog(theta)         0.132      0.155\n\n\nIn this particular instance, the coefficients for the negative binomial part of the model and their standard errors are quite similar. You are asked to compare the coefficients and their standard errors for the “extra zero” part of these two models at the end of the lab.\nLet’s next carry out a more formal Vuong test to compare the zero-inflated negative binomial model to the usual negative binomial model.\n\nVuong test\nBecause the zero-inflated poisson model and the usual Poisson model do not nest, we can’t use a drop in deviance test to compare the models (and same goes for the negative binomial model vs. the zero-inflated negative binomial model).\nThe Vuong test has a null hypothesis that the models are indistinguishable with a large positive test statistic indicating that the first model that is input into the function is better (below, we put the zero-inflated model first) and a large negative test statistic indicating that the second model is better.\n\nvuong(mod.nb0, mod.nb)\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A    p-value\nRaw                    1.315020 model1 &gt; model2   0.094252\nAIC-corrected         -0.723184 model2 &gt; model1   0.234783\nBIC-corrected         -3.763784 model2 &gt; model1 8.3681e-05\n\n\nNote 1: The Vuong test is asymptotic so, if the sample size of the data set is not very large, then the test is unreliable and should not be used.\nNote 2: We can also use the test to compare the Poisson models just for fun.\n\nvuong(mod.pois0, mod.pois)\n\nVuong Non-Nested Hypothesis Test-Statistic: \n(test-statistic is asymptotically distributed N(0,1) under the\n null that the models are indistinguishible)\n-------------------------------------------------------------\n              Vuong z-statistic             H_A   p-value\nRaw                    2.667898 model1 &gt; model2 0.0038164\nAIC-corrected          2.473082 model1 &gt; model2 0.0066977\nBIC-corrected          2.182454 model1 &gt; model2 0.0145380\n\n\nHere, there is fairly strong evidence that the zero-inflated Poisson model is better than the non-zero inflated Poisson model. However, as discussed above, neither seem appropriate since there is over dispersion in the counts of days absent from school.\nTo conclude, it seems evident that there is over dispersion in the days absent variable, but that these counts are not zero-inflated. In this instance the appropriate model to use for interpretation and inference is the negative binomial regression model.\n\n\nCoefficient Interpretation\nEven though zero-inflation is not apparent in the days absent counts, we’re going to proceed with interpreting the estimated regression coefficients for the zero-inflated negative binomial model just so you can see one approach. Recall that the coefficient estimates for this model are:\n\nsummary(mod.nb0)$coefficients[[1]][, 1]\n\n(Intercept)        EthN        SexM       AgeF1       AgeF2       AgeF3 \n  2.8838678  -0.5095556   0.1674285  -0.4416084   0.0512790   0.3093699 \n      LrnSL  Log(theta) \n  0.2825202   0.3964049 \n\nsummary(mod.nb0)$coefficients[[2]][, 1]\n\n(Intercept)        EthN        SexM       AgeF1       AgeF2       AgeF3 \n-26.6847779  13.7291994  11.7614329   0.1852184  -1.1578503  -0.7732624 \n      LrnSL \n -0.6861468 \n\n\nThe first set of coefficients is from the negative binomial part of the model and the second set of coefficients is from the zero-inflation part of the model. If we think in the framework of some of the zeros as true zeros and some of the zeros as excess zeros, then we might interpret the Ethnicity coefficient estimate in the negative binomial model, -0.5096, in the following way:\n“A person of non-Aboriginal descent is predicted to be absent from school \\(1 - exp(-0.5096) = 39.93%\\) less than a person of Aboriginal descent at this particular school among all of those with a risk of being absent (i.e., among all of those that are not the excess zeroes), provided these two people are of the same gender, age and learner status.\nThat’s rather a mouthful of an interpretation, but we have to be careful when interpreting regression coefficient estimates from a model with multiple explanatory variables – the interpretation of a single coefficient estimate has to be made while holding the values of the other explanatory variables fixed.\nNotice that this interpretation is exactly the same as the interpretation in the usual negative binomial model except for the extra parenthetical we added about the excess zeroes.\nSimilarly, we interpret regression coefficient estimates from the zero-inflated part of the model in the same way that we would interpret estimates from a logistic regression model.\n\n\nLooking at Residuals\nLet’s new consider a few residual plots from the zero-inflated negative binomial model.\n\nschools$residuals.pearson &lt;- residuals(mod.nb0, type = \"pearson\")\nschools$fitted.vals &lt;- mod.nb0$fitted.values\nggplot(data = schools, aes(x = Eth, y = residuals.pearson)) +\n    geom_point()\n\n\n\nggplot(data = schools, aes(x = Sex, y = residuals.pearson)) +\n    geom_point()\n\n\n\nggplot(data = schools, aes(x = Age, y = residuals.pearson)) +\n    geom_point()\n\n\n\nggplot(data = schools, aes(x = Lrn, y = residuals.pearson)) +\n  geom_point()\n\n\n\nggplot(data = schools, aes(x = fitted.vals, y = residuals.pearson)) +\n    geom_point()\n\n\n\n\nWe see from these residual plots that there are a few residuals that are larger than 3. However, given such a large sample, it is not too surprising that a few points have large residuals. We also see in the residuals vs. fitted values plot, that the spread of the residuals is relatively constant across all fitted values. Overall, there is no cause for concern in using the zero-inflated negative binomial model for this data. As discussed above, however, since the zero-inflated model has a similar AIC (actually a slightly larger AIC) than the non-zero-inflated model, we would probably prefer the simpler non-zero-inflated model."
  },
  {
    "objectID": "z4_learnR.html",
    "href": "z4_learnR.html",
    "title": "LearnR 4",
    "section": "",
    "text": "Please load these libraries that you’ll need for this lab:\n\nlibrary(arm)\n\nWarning: package 'arm' was built under R version 4.3.3\n\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is C:/Users/cmeck/Desktop/D_Anal/Anal_II/Anal_II_Notes/Data_Analysis_II_Notes\n\nlibrary(Sleuth3)\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ dplyr::select() masks MASS::select()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(vcdExtra)\n\nWarning: package 'vcdExtra' was built under R version 4.3.3\n\n\nLoading required package: vcd\n\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\nLoading required package: gnm\n\n\nWarning: package 'gnm' was built under R version 4.3.3\n\n\n\nAttaching package: 'vcdExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nIn this lab, we’ll go over some of the finer details of binomial logistic regression that you only saw briefly in the narrated lectures. We’ll cover the drop in deviance and deviance goodness of fit tests, and show how to perform them using R. We’ll also talk more about residuals from binomial logistic regression and about the dispersion parameter and over dispersion. Two additional topics are (1) a pathological (but not altogether rare) situation that can arise in logistic regression and (2) using logistic regression to perform a test of the difference in two proportions."
  },
  {
    "objectID": "z4_learnR.html#aflatoxin-data",
    "href": "z4_learnR.html#aflatoxin-data",
    "title": "LearnR 4",
    "section": "Aflatoxin data",
    "text": "Aflatoxin data\nIn this lab we will continue with the ex2116 data from Sleuth3, introduced in lecture. Each of 20 tanks was stocked with fishes that had been exposed as embryos to one of 5 doses of a carcinogen. When the fishes were dissected a year later, the number of fishes that had developed liver tumors was recorded.\n\ndata(ex2116)\ntumors &lt;- ex2116\nhead(tumors)\n\n\n\n\n\nDose\nTumor\nTotal\n\n\n\n\n0.010\n9\n87\n\n\n0.010\n5\n86\n\n\n0.010\n2\n89\n\n\n0.010\n9\n85\n\n\n0.025\n30\n86\n\n\n0.025\n41\n86\n\n\n\n\n\n\nHere we’ll add a column that records the number of fish in each tank which did not develop liver tumors, and a column that assigns a unique label to each of the 20 tanks. We’ll also add a column for log(Dose) and log(Dose)^2 since we used those as explanatory variables in the model you saw in the narrated lectures.\n\ntumors %&lt;&gt;% mutate(Dose = Dose, Tumor = Tumor, NoTumor = Total - Tumor, TankID = factor(1:nrow(tumors)),logDose = log(Dose), logDose2 = log(Dose)^2)\nhead(tumors)\n\n\n\n\n\nDose\nTumor\nTotal\nNoTumor\nTankID\nlogDose\nlogDose2\n\n\n\n\n0.010\n9\n87\n78\n1\n-4.60517\n21.20759\n\n\n0.010\n5\n86\n81\n2\n-4.60517\n21.20759\n\n\n0.010\n2\n89\n87\n3\n-4.60517\n21.20759\n\n\n0.010\n9\n85\n76\n4\n-4.60517\n21.20759\n\n\n0.025\n30\n86\n56\n5\n-3.68888\n13.60783\n\n\n0.025\n41\n86\n45\n6\n-3.68888\n13.60783\n\n\n\n\n\n\nWe’ll also create a case-format version that records the binary status (tumor or not) for each of the 1739 individual fishes.\n# check code tumors_case$Outcome should be a factor\n\n# The syntax dplyr::select() ensures that the select() function from the dplyr package is \n# used instead of the select() function from the MASS package, in case you still have MASS\n# loaded in the library.\ntumors_freq &lt;- tumors %&gt;% dplyr::select(-Total) %&gt;%\n  pivot_longer(Tumor:NoTumor,names_to=\"Outcome\",values_to=\"Freq\",cols_vary = \"slowest\") %&gt;%\n  mutate(Outcome=as.factor(Outcome))\ntumors_case &lt;- expand.dft(tumors_freq) %&gt;%\n  mutate(Outcome=as.factor(Outcome))\nhead(tumors_case)\n\n\n\n\n\nDose\nTankID\nlogDose\nlogDose2\nOutcome\n\n\n\n\n0.01\n1\n-4.60517\n21.20759\nTumor\n\n\n0.01\n1\n-4.60517\n21.20759\nTumor\n\n\n0.01\n1\n-4.60517\n21.20759\nTumor\n\n\n0.01\n1\n-4.60517\n21.20759\nTumor\n\n\n0.01\n1\n-4.60517\n21.20759\nTumor\n\n\n0.01\n1\n-4.60517\n21.20759\nTumor\n\n\n\n\n\n\n(With most of the lab examples so far oriented around examples of death, discrimination, and disease, you might accuse us statisticians of attempting to usurp from economics the title of “dismal science”! (Economists think they’re pessimists – what they call “maximizing utility”, statisticians call “minimizing loss”). We promise nobody dies in the next lab. Probably."
  },
  {
    "objectID": "z4_learnR.html#binary-vs-binomial",
    "href": "z4_learnR.html#binary-vs-binomial",
    "title": "LearnR 4",
    "section": "Binary vs binomial",
    "text": "Binary vs binomial\nThere are several equivalent ways of fitting a logistic regression model with glm() to data such as the ex2116 data. We will see why the fitting methods that preserve information about the counts within each tank are preferable for our purposes.\n\nModeling a binomial count for each tank\nFirst, the response can be specified as a 2-column matrix containing counts of successes in the first column and counts of failures in the second.\n\nmod1 &lt;- glm(data = tumors, cbind(Tumor, NoTumor) ~ logDose + logDose2, family = \"binomial\")\nsummary(mod1)\n\n\nCall:\nglm(formula = cbind(Tumor, NoTumor) ~ logDose + logDose2, family = \"binomial\", \n    data = tumors)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.02921    0.49343   2.086  0.03699 *  \nlogDose     -1.03048    0.35743  -2.883  0.00394 ** \nlogDose2    -0.39195    0.06136  -6.388 1.68e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 667.195  on 19  degrees of freedom\nResidual deviance:  26.048  on 17  degrees of freedom\nAIC: 119.45\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe response can also be specified as a vector of proportions of successes in each group, with the total in each group given through the weights argument:\n\nmod2 &lt;- glm(data = tumors, Tumor/Total ~ logDose + logDose2, weights = Total, family = \"binomial\")\nsummary(mod2)\n\n\nCall:\nglm(formula = Tumor/Total ~ logDose + logDose2, family = \"binomial\", \n    data = tumors, weights = Total)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.02921    0.49343   2.086  0.03699 *  \nlogDose     -1.03048    0.35743  -2.883  0.00394 ** \nlogDose2    -0.39195    0.06136  -6.388 1.68e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 667.195  on 19  degrees of freedom\nResidual deviance:  26.048  on 17  degrees of freedom\nAIC: 119.45\n\nNumber of Fisher Scoring iterations: 4\n\n\nBe sure to verify that mod1 and mod2 are equivalent.\n\n\nModeling a binary outcome for each fish\nWe have converted the data to case format, where each row contains an individual binary outcome corresponding to an individual fish. These outcomes can be modeled directly, just as we saw with the (ungrouped) Donner data in Lab 3.\n\nmod3 &lt;- glm(data = tumors_case, Outcome ~ logDose + logDose2, family = \"binomial\")\nsummary(mod3)\n\n\nCall:\nglm(formula = Outcome ~ logDose + logDose2, family = \"binomial\", \n    data = tumors_case)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.02921    0.49341   2.086  0.03698 *  \nlogDose     -1.03048    0.35740  -2.883  0.00394 ** \nlogDose2    -0.39195    0.06135  -6.388 1.68e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2395.8  on 1738  degrees of freedom\nResidual deviance: 1754.7  on 1736  degrees of freedom\nAIC: 1760.7\n\nNumber of Fisher Scoring iterations: 4\n\n\nMake a quick comparison of the coefficient estimates from mod2 and mod3:\n\ncbind(coefficients(mod2),coefficients(mod3))\n\n                  [,1]      [,2]\n(Intercept)  1.0292126  1.029213\nlogDose     -1.0304804 -1.030480\nlogDose2    -0.3919491 -0.391949\n\n\nThe two models give the same coefficient estimates, and you can also verify that the corresponding standard errors are the same. Therefore, in terms of inferences about the regression coefficients, treating the data as binary (one 0/1 outcome for each fish) or binomial (one count outcome for each tank) doesn’t seem to matter.\nBut, let’s remember that when we looked at the binomial logistic regression model in class, we noticed some over dispersion in the counts. We’ll first examine one more approach to modeling the binomial counts, and then turn to talking about the over dispersion.\n\n\nModeling as binomial counts within each Dose\nNote that “TankID” is not a term in any of the models we fit above. The binomial logistic models treat the 4 tanks in each of the 5 dose groups as independent binomial observations – within each Dose level, the 4 tanks are supposed to be draws from 4 independent binomial random variables, with potentially different \\(n\\) (different number of fish in each tank) but all with the same \\(p\\). In the binary logistic model, the tanks are ignored and the fish-level outcomes are treated as independent Bernoulli random variables, with a common \\(p\\) at each Dose. In either form, a single \\(p\\) applies to every fish in the Dose level, regardless of tank. Indeed, we can explicitly collapse across the tanks before fitting the model, without affecting any of the inferences:\n\n(summed_tumor &lt;- summarize(group_by(tumors, Dose, logDose, logDose2), sum(Tumor), sum(NoTumor)))\n\n`summarise()` has grouped output by 'Dose', 'logDose'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\nDose\nlogDose\nlogDose2\nsum(Tumor)\nsum(NoTumor)\n\n\n\n\n0.010\n-4.605170\n21.207592\n25\n322\n\n\n0.025\n-3.688880\n13.607832\n132\n214\n\n\n0.050\n-2.995732\n8.974412\n226\n127\n\n\n0.100\n-2.302585\n5.301898\n281\n74\n\n\n0.250\n-1.386294\n1.921812\n286\n52\n\n\n\n\n\n\nNotice that we’ve now reduced the dataset down to five observations! And now we’ll fit the model to these summed responses:\n\nmod4 &lt;- glm(data = summed_tumor, cbind(`sum(Tumor)`, `sum(NoTumor)`) ~ logDose + logDose2, family = \"binomial\")\nsummary(mod4)\n\n\nCall:\nglm(formula = cbind(`sum(Tumor)`, `sum(NoTumor)`) ~ logDose + \n    logDose2, family = \"binomial\", data = summed_tumor)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.02921    0.49343   2.086  0.03699 *  \nlogDose     -1.03048    0.35743  -2.883  0.00394 ** \nlogDose2    -0.39195    0.06136  -6.388 1.68e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 641.234089  on 4  degrees of freedom\nResidual deviance:   0.086879  on 2  degrees of freedom\nAIC: 35.091\n\nNumber of Fisher Scoring iterations: 3\n\n\nAnd compare coefficient estimates:\n\ncbind(coefficients(mod2),coefficients(mod3),coefficients(mod4))\n\n                  [,1]      [,2]       [,3]\n(Intercept)  1.0292126  1.029213  1.0292126\nlogDose     -1.0304804 -1.030480 -1.0304804\nlogDose2    -0.3919491 -0.391949 -0.3919491\n\n\nAnd once again you can verify that all of the corresponding standard errors are also the same. So what’s going on here, and which of these models is “the best one” to use?\n\n\nOk, so now what?!\nThe parameter estimates and standard errors are all identical – we reach the same conclusions from each model. But let’s look at the residual deviances for each of the models:\n\ncbind(deviance(mod2),deviance(mod3),deviance(mod4))\n\n         [,1]     [,2]       [,3]\n[1,] 26.04806 1754.692 0.08687945\n\n\nIt’s also important to recognize that in mod2, mod3 and mod4, the sample sizes are n = 20, n = 1739 and n = 5, respectively. And, the null deviances for each model are also substantially different; again in order of mod2, mod3 and mod4 these are 667.20, 2395.84 and 641.23. The difference in deviance (the deviance accounted for by a model), however, is the same for every form of the model.\nTo see more plainly that the methods above are equivalent, we can view minimal summaries from each using the display() function from arm. The abbreviated output (compared to summary) facilitates comparison between several models on the same page.\n\n# counts in tanks\ndisplay(mod2)\n\nglm(formula = Tumor/Total ~ logDose + logDose2, family = \"binomial\", \n    data = tumors, weights = Total)\n            coef.est coef.se\n(Intercept)  1.03     0.49  \nlogDose     -1.03     0.36  \nlogDose2    -0.39     0.06  \n---\n  n = 20, k = 3\n  residual deviance = 26.0, null deviance = 667.2 (difference = 641.1)\n\n# binary outcomes per fish\ndisplay(mod3)\n\nglm(formula = Outcome ~ logDose + logDose2, family = \"binomial\", \n    data = tumors_case)\n            coef.est coef.se\n(Intercept)  1.03     0.49  \nlogDose     -1.03     0.36  \nlogDose2    -0.39     0.06  \n---\n  n = 1739, k = 3\n  residual deviance = 1754.7, null deviance = 2395.8 (difference = 641.1)\n\n# counts in dose level ignoring tanks\ndisplay(mod4)\n\nglm(formula = cbind(`sum(Tumor)`, `sum(NoTumor)`) ~ logDose + \n    logDose2, family = \"binomial\", data = summed_tumor)\n            coef.est coef.se\n(Intercept)  1.03     0.49  \nlogDose     -1.03     0.36  \nlogDose2    -0.39     0.06  \n---\n  n = 5, k = 3\n  residual deviance = 0.1, null deviance = 641.2 (difference = 641.1)\n\n\nAll the same assumptions go into each of these ways of fitting the models, and all the same inferences come out. In particular, we are always assuming that once we know the Dose a fish received, knowing the particular tank in which that fish was housed cannot give us any more information about that fish’s chance of having a tumor – not even if we know that the fish came from a tank in which an especially large (or small) number of other fish got tumors, compared to other tanks within the same dose level. In the binomial-tank-counts model, this assumption reads like “every binomial within a dose level has the same \\(p\\).” In the binary-fish model, it reads “every Bernoulli within a dose level has the same \\(p\\).” In the binomial-dose-count model, it reads “the Bernoulli’s across all tanks within a dose must be independent.” We could call this the assumption of “irrelevant groups (tanks).”\nIf we have some grouping at all, however, we often suspect that this last assumption about independence is not sound. Fish in the same tank are expected to be more similar to one another than fish in different tanks. That is, we should not expect same binomial \\(p\\) in every tank, nor should we suppose that all the Bernoulli responses in a dose level are independent, regardless of tank.\nIn what follows, we will see that fitting the model in the “tank-count” binomial form, as opposed to per-fish binary form or the dose-count binomial form, is the way that will allow us to check whether the group-irrelevance of assumption is reasonable. We’ll see that not making this assumption comes with a steep price, and preview another technique that will sometimes allow us to avoid paying it."
  },
  {
    "objectID": "z4_learnR.html#empirical-logits",
    "href": "z4_learnR.html#empirical-logits",
    "title": "LearnR 4",
    "section": "Empirical logits",
    "text": "Empirical logits\nThe empirical logit is just the logit transformation applied to the proportion within a bin (this isn’t the log-odds of the probability within the bin, it’s just an estimate based on the observed proportion – hence “empirical”).\nRemember the logit transformation of a proportion/probability is given by qlogis():\n\nempirical_logits &lt;- with(tumors, qlogis(Tumor/Total))\n\nWe plot the empirical logits against Dose to see the shape of the relationship we’re trying to capture on the log-odds scale, along with a loess smooth.\n\nggplot(data = tumors, aes(x = Dose, y = empirical_logits))+ \n  geom_jitter(width = 0.005, height = 0.01) + \n  geom_smooth(se = FALSE) +\n  ggtitle(\"Empirical Logits vs Dose\") \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis appearance of a logarithmic relationship between the Dose and the logits prompts the a log-transformation of the dose to straighten it out (that is, it suggests that the odds of tumor are nearly linear in dose).\n\nggplot(data = tumors, aes(x = logDose, y = empirical_logits)) + \n  geom_jitter(width = 0.1, height = 0.1) + \n  geom_smooth(se = FALSE) +\n  ggtitle(\"Empirical Logits vs log(Dose)\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis plot suggests that a logistic regression model which includes both log(Dose) and log(Dose)^2 would be appropriate to capture the observed trend in the empirical logits. Unlike with binary logistic regression, the binomial model lets us measure the lack-of-fit to a proposed model, and compare the fit between models."
  },
  {
    "objectID": "z4_learnR.html#deviance-goodness-of-fit",
    "href": "z4_learnR.html#deviance-goodness-of-fit",
    "title": "LearnR 4",
    "section": "Deviance goodness of fit",
    "text": "Deviance goodness of fit\nRecall that the deviance goodness of fit test compares a fitted model to a saturated model, or one in which there are as many parameters as there are data points. In these goodness of fit comparisons, the null hypothesis corresponds to the fitted model (which is a reduced model relative to the saturated model), and the alternative hypothesis corresponds to the saturated model. Therefore, a small p-value indicating evidence for rejection of the fitted model in favor of the saturated model is evidence of lack of fit.\nThe LRstats() from vcdExtra function shows a convenient summary of the fits of one or more model objects.\n\nmod5 &lt;- glm(data = tumors, (Tumor/Total) ~ Dose, weights = Total, family = \"binomial\")\nmod6 &lt;- glm(data = tumors, (Tumor/Total) ~ logDose, weights = Total, family = \"binomial\")\nmod7 &lt;- glm(data = tumors, (Tumor/Total) ~ logDose + logDose2, weights = Total, family = \"binomial\")\nLRstats(mod5, mod6, mod7)\n\n\n\n\n\n\nAIC\nBIC\nLR Chisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nmod5\n368.4449\n370.4364\n277.04682\n18\n0.0000000\n\n\nmod6\n160.2947\n162.2862\n68.89661\n18\n0.0000001\n\n\nmod7\n119.4461\n122.4333\n26.04806\n17\n0.0735894\n\n\n\n\n\n\nThe LR Chisq is the residual deviance, which is the sum of the squared deviance residuals. For instance, for mod5, the residual deviance value of 277 comes can be obtained as\n\nresid(mod5, type = 'deviance') %&gt;% raise_to_power(2) %&gt;% sum\n\n[1] 277.0468\n\n\nor as\n\nmod5$deviance\n\n[1] 277.0468\n\n\nThis quantity has an approximate chi-squared distribution if the model is correct, which is the basis for the deviance goodness of fit test. Pr(&gt;Chisq) provides the p-value for this test. For mod6, let’s check that this is equivalent to doing this test “by hand,” using the residual deviance and residual df:\n\npchisq(mod6$deviance, df = mod6$df.residual, lower.tail = FALSE)\n\n[1] 6.940025e-08\n\n\nEach successive model shows a better fit, as evidenced by the successively larger p-values from the goodness of fit tests."
  },
  {
    "objectID": "z4_learnR.html#drop-in-deviance-test",
    "href": "z4_learnR.html#drop-in-deviance-test",
    "title": "LearnR 4",
    "section": "Drop-in-deviance test",
    "text": "Drop-in-deviance test\nJust a reminder that the drop in deviance test is different from the deviance goodness of fit test. Whereas the deviance goodness of fit test provides a comparison between a single fitted model and a saturated model, a drop in deviance test provides a way to compare two fitted models when one of those models is nested within the other one. Put another way, the drop in deviance test is a comparison between a reduced model (null hypothesis) and a full model (alternative hypothesis) – and we use it in cases where the reduced model is reduced from (or nested in) the full model.\nUsing the models we have already fit, let’s use the anova() function to perform a drop in deviance test comparing a reduced model (only logDose included) to a full model (logDose and logDose2 included):\n\nanova(mod6, mod7,test=\"Chisq\")\n\n\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n18\n68.89661\nNA\nNA\nNA\n\n\n17\n26.04806\n1\n42.84856\n0\n\n\n\n\n\n\nThe p-value of the drop in deviance test is quite small, p &lt; 0.0001. This provides convincing evidence in favor of the full model; namely, the one that includes logDose and logDose2."
  },
  {
    "objectID": "z4_learnR.html#information-criteria-for-model-comparison",
    "href": "z4_learnR.html#information-criteria-for-model-comparison",
    "title": "LearnR 4",
    "section": "Information criteria for model comparison",
    "text": "Information criteria for model comparison\nLet’s look at the LRstats again, this time with a focus on the first two columns:\n\nLRstats(mod5, mod6,mod7)\n\n\n\n\n\n\nAIC\nBIC\nLR Chisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nmod5\n368.4449\n370.4364\n277.04682\n18\n0.0000000\n\n\nmod6\n160.2947\n162.2862\n68.89661\n18\n0.0000001\n\n\nmod7\n119.4461\n122.4333\n26.04806\n17\n0.0735894\n\n\n\n\n\n\nThe AIC and BIC, as you may recall from Data Analytics I, are likelihood-based methods for comparing models. Both penalize models with more parameters, but the BIC generally applies a larger penalty (and hence promotes selection of simpler models). Models with smaller values are preferred. The AIC or BIC of a single model is not a measure of the goodness of fit for that model – the information criteria are only meaningful as comparisons between models. The information criteria can be applied to compare models which are not nested (neither model’s parameters are a strict subset of the other’s), as long as each model has a likelihood.\n\nWhich of these three candidate models would we choose, based on the information criteria? Does this align with the conclusion from the drop-in-deviance test, given that mod6 is nested within mod7?"
  },
  {
    "objectID": "z4_learnR.html#deviance-residuals-pearson-residuals---dispersion-parameter-estimated-from-pearson-chi-squared",
    "href": "z4_learnR.html#deviance-residuals-pearson-residuals---dispersion-parameter-estimated-from-pearson-chi-squared",
    "title": "LearnR 4",
    "section": "Deviance residuals, Pearson residuals - dispersion parameter estimated from Pearson – chi squared",
    "text": "Deviance residuals, Pearson residuals - dispersion parameter estimated from Pearson – chi squared\nIn the case of binomial logistic regression it can be helpful to look at the deviance and/or Pearson residuals to (a) evaluate the model fit and (b) check for outliers. Provided that the binomial counts are fairly large, both the deviance and Pearson residuals should look like draws from a standard Normal distribution, so too many residuals outside of the [-2,2] interval may be cause for concern. Here’ we’ll look at a few plots of both the deviance and Pearson residuals.\n\ntumors$residuals_deviance &lt;- residuals(mod7)\ntumors$residuals_pearson &lt;- residuals(mod7, type = \"pearson\")\nggplot(data = tumors, aes(logDose,residuals_deviance)) + geom_point()\n\n\n\nggplot(data = tumors, aes(logDose,residuals_pearson)) + geom_point()\n\n\n\nggplot(data = tumors, aes(residuals_deviance,residuals_pearson)) + geom_point()\n\n\n\n\nThere are no obvious patterns in either of the plots showing the residuals against logDose, and there are also no outliers. This is all good – it suggests that we’ve done a good job at modeling the log odds of tumors (although we still have to talk about the over dispersion). We created the scatterplot of the Pearson residuals versus the deviance residuals just so you could see how similar they are – for the most part, they fall along the y = x diagonal.\nWhen looking at residuals, it can also be useful to plot the residuals versus the fitted values of a model. Again, we’re hoping that we don’t see any patterns in such a plot:\n\ntumors$fitted = predict.glm(mod7,scale=\"link\")\nggplot(data = tumors, aes(fitted,residuals_deviance)) + geom_point()\n\n\n\n\nThere are no clear patterns or problems with this plot, so again we have confirmation that we’re using a decent model at this point."
  },
  {
    "objectID": "z4_learnR.html#question",
    "href": "z4_learnR.html#question",
    "title": "LearnR 4",
    "section": "Question",
    "text": "Question\nHere we will return to the UCBAdmissions dataset and fit a logistic regression model for the count of students admitted (out of the total applicants) for each combination of the factors gender and department. The usual drill applies – submit your answers as an RMarkdown document, following the instructions given in the previous labs.\n\nConstruct an informative ggplot() of the empirical logits of admission proportion vs gender and department. It’s up to you what aesthetics to map to which variables – there is more than one right answer here.\n\nSome tips for part (a):\n\nYou can use the group argument to geom_line() to connect points within a group – for instance, given a plot with Gender on the x axis and a variable called eLogits on the y, you could add geom_line(aes(group = Dept, x = Gender, y = eLogits)), where the slope of the connecting lines would correspond to the sign and magnitude of the difference in empirical logits (log of observed odds ratio) between genders within each department.\nYou can also incorporate information about the total number of applicants to each department into your plot. For instance, supposing you had a data frame with separate columns containing counts of “Admitted” and “Rejected” by sex and department, you could map the number of applicants to the size of the plot points using geom_point(size = Admitted + Rejected).\n\n\nBased on your plot from (a), which variable (gender or department) appears to account for more of the variability in admissions? Explain.\nFit an appropriate (binomial) logistic regression model for admissions. What is the estimated dispersion parameter? Is there evidence of lack of fit?\nConstruct a plot of residuals vs fitted values (try just plot-ing your fitted model object). From this plot, can you identify a source for any fit problems encountered in part (c)?\nRefit the binomial model above, but excluding the data from department A. Now what is the estimated dispersion parameter? Based on the p-value, what would you conclude about the effect of Gender on admissions (to departments other than A) using this model?\nThe approach in part (d) allowed us to keep the binomial likelihood model, but only by performing an unprincipled exclusion of some apparently-legitimate data that happened to be “outlying”.\n\nTo avoid this, we’ll refit the model for all departments with the quasibinomial family.\nUsing the quasibinomial model for all departments, what do you conclude about the effect of Gender on admissions? Support your conclusion by constructing and interpreting a 95% confidence interval for\n\\(P_{diff} = [P(Admit | (Department, Male)) - P(Admit | (Department, Female))]\\)\nThat is, construct an interval on the model scale, then backtransform to the data scale. Be careful with the direction (male higher or female higher) of the observed difference in conditional probability of admission."
  },
  {
    "objectID": "z2_learnR.html",
    "href": "z2_learnR.html",
    "title": "LearnR 2",
    "section": "",
    "text": "In the previous lab, we showed how to produce descriptive statistics that quantify relationships between categorical variables observed on a given sample. No formal attempt was made to extend conclusions beyond the sample. In this lab, we will explore methods for making inference about the association between two categorical variables in a larger population of interest. The usual caveats apply, including but not limited to:\n\nThese methods cannot be interpreted in a straightforward way unless the data are an independent sample from a well-defined population of interest, obtained using a probability (random) mechanism.\nThese methods are not necessarily robust to extreme outliers or large quantities of missing data. Intending to do inference does not eliminate the need for exploratory checks on data quality.\nThe probabilistic “guarantees” attached to these inferential methods do not hold in the context of using the same data to generate hypotheses and test them. See R for Data Science, Chapter 22 for advice about using “holdout data” when you’re serious about formal inference.\n\n\n\nHere is a glossary of terms and notation that we’ll use throughout the lab (and the course).\nMathematical expressions are formatted with dollar signs for improved appearance in the HTML document. When reading the .Rmd script, in RStudio, moving your cursor over something between dollar signs “pops out” the formatted mathematical expression from that location on the screen.\n\n\\(I\\) denotes the number of rows in a 2-way table of counts, \\(J\\) denotes the number of columns\n\n\\(X\\) denotes the “row variable” – a categorical random variable (factor) taking on I possible values \\(x_1, ..., x_I\\)\n\\(Y\\) denotes the “column variable” – a categorical random variable taking on \\(J\\) possible values \\(y_1,...,y_J\\)\n\nThe cells in a 2-way table are indexed by \\(i\\) and \\(j\\), where \\(i\\) in \\(1:I\\) is the row index and \\(j\\) in \\(1:J\\) is the column index\n\n\\(N\\) is the total count over the whole table, and \\(n_{ij}\\) is the count in the \\(i,j\\) cell – that is, the number of observation for which \\(X = x_i\\) and \\(Y = y_j\\)\n\\(p_{ij}\\) is the proportion, out of the N observations, which fall in the \\(i,j\\) cell (so \\(p_{ij} = n_{ij}/N\\))\n\\(p_i.\\) is the ith row proportion, and \\(p._j\\) the \\(j\\)th column proportion – the . notation suggests that we’ve summed over/collapsed the index it replaces\n\n\n\n\nWe’ll use the following packages in this lab:\n\nlibrary(magrittr)\nlibrary(vcdExtra)\n\nWarning: package 'vcdExtra' was built under R version 4.3.3\n\n\nLoading required package: vcd\n\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\n\n\nLoading required package: gnm\n\n\nWarning: package 'gnm' was built under R version 4.3.3\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract()   masks magrittr::extract()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ purrr::set_names() masks magrittr::set_names()\n✖ dplyr::summarise() masks vcdExtra::summarise()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "z2_learnR.html#notation-for-i-x-j-tables",
    "href": "z2_learnR.html#notation-for-i-x-j-tables",
    "title": "LearnR 2",
    "section": "",
    "text": "Here is a glossary of terms and notation that we’ll use throughout the lab (and the course).\nMathematical expressions are formatted with dollar signs for improved appearance in the HTML document. When reading the .Rmd script, in RStudio, moving your cursor over something between dollar signs “pops out” the formatted mathematical expression from that location on the screen.\n\n\\(I\\) denotes the number of rows in a 2-way table of counts, \\(J\\) denotes the number of columns\n\n\\(X\\) denotes the “row variable” – a categorical random variable (factor) taking on I possible values \\(x_1, ..., x_I\\)\n\\(Y\\) denotes the “column variable” – a categorical random variable taking on \\(J\\) possible values \\(y_1,...,y_J\\)\n\nThe cells in a 2-way table are indexed by \\(i\\) and \\(j\\), where \\(i\\) in \\(1:I\\) is the row index and \\(j\\) in \\(1:J\\) is the column index\n\n\\(N\\) is the total count over the whole table, and \\(n_{ij}\\) is the count in the \\(i,j\\) cell – that is, the number of observation for which \\(X = x_i\\) and \\(Y = y_j\\)\n\\(p_{ij}\\) is the proportion, out of the N observations, which fall in the \\(i,j\\) cell (so \\(p_{ij} = n_{ij}/N\\))\n\\(p_i.\\) is the ith row proportion, and \\(p._j\\) the \\(j\\)th column proportion – the . notation suggests that we’ve summed over/collapsed the index it replaces"
  },
  {
    "objectID": "z2_learnR.html#load-packages-for-the-lab",
    "href": "z2_learnR.html#load-packages-for-the-lab",
    "title": "LearnR 2",
    "section": "",
    "text": "We’ll use the following packages in this lab:\n\nlibrary(magrittr)\nlibrary(vcdExtra)\n\nWarning: package 'vcdExtra' was built under R version 4.3.3\n\n\nLoading required package: vcd\n\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\n\n\nLoading required package: gnm\n\n\nWarning: package 'gnm' was built under R version 4.3.3\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract()   masks magrittr::extract()\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ purrr::set_names() masks magrittr::set_names()\n✖ dplyr::summarise() masks vcdExtra::summarise()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "z2_learnR.html#statistical-independence-for-contingency-tables",
    "href": "z2_learnR.html#statistical-independence-for-contingency-tables",
    "title": "LearnR 2",
    "section": "Statistical Independence for Contingency Tables",
    "text": "Statistical Independence for Contingency Tables\nIn terms of the sample proportions that we can estimate from an \\(I \\times J\\) contingency table, the independence relation is (check the Notation Section above):\n\\[p_{ij} = p_i.p._j\\]\nIn words: the cell probability for the \\(i,j\\) cell is equal to the marginal probability of the \\(i^{th}\\) row times the marginal probability of the \\(j^{th}\\) column.\nBecause we don’t know the underlying joint probability distribution that leads to a specific, observed contingency table, we can only evaluate the independence assumption on the basis of the sample independence relation, \\(p_ij = p_i.p._j\\). We say that a contingency table is the most consistent with statistical independence if \\(p_{ij} = p_i.p._j\\) for all \\(i,j\\). And, the extent to which the observed counts in a table deviate from \\(p_{ij} = p_i.p._j\\) for all \\(i,j\\) provides us with evidence against the statistical independence hypothesis."
  },
  {
    "objectID": "z2_learnR.html#plots",
    "href": "z2_learnR.html#plots",
    "title": "LearnR 2",
    "section": "Plots",
    "text": "Plots\nYou can use mosaic plots to get a sense for the independence or lack of independence in \\(I \\times J\\) tables. In general, when the two variables of an \\(I \\times J\\) table are independent, then the vertical splits in the mosaic plot of that table should (approximately) line up. Actually, depending on the complexity of the mosaic plot, it may be that you’re looking for the horizontal splits to line up, but in the examples here, it’s the vertical splits.\nRun the following chunk of R code to see some examples.\n\n# Approximately independent \n  tab1 &lt;- matrix(c(45,10,50,14,35,12),3,2,byrow=TRUE)\n  mosaic(tab1)\n\n\n\n# Probably not independent\n  tab2 &lt;- matrix(c(45,10,14,50,35,12),3,2,byrow=TRUE)\n  mosaic(tab2)\n\n\n\n\nIntuitively, the closer the vertical splits are to being aligned, the closer the observed table of counts will be to the expected table of counts. You can play around with adjusting the cell counts in the code above give yourself a better understanding of the mosaic plot as a tool for evaluating statistical independence visually.\nA closely-related visualization is the sieve plot, which first draws the outline of the mosaic plot that we would expect to see under the assumption of independence (i.e., with the splits lined up). It then fills in each cell of this outline with a grid of squares, scaled so that \\(n_{ij}\\) squares fit within the \\(ij^{th}\\) cell. Thus, cells with bigger-than-expected counts are filled with denser grids, and cells with smaller-than-expected counts are filled with sparser grids. For extra clarity, the “underfilled” cells are shaded red and the “overfilled” cells shaded blue.\n\n# Approximately independent \n  sieve(tab1, shade = TRUE)\n\n\n\n# Probably not independent\n  sieve(tab2, shade = TRUE)\n\n\n\n\nThe grid sizes on the first sieve plot are approximately the same, indicating independence; whereas the grid sizes on the second plot are different, indicating a departure from independence."
  },
  {
    "objectID": "z2_learnR.html#standard-case-large-sample-size",
    "href": "z2_learnR.html#standard-case-large-sample-size",
    "title": "LearnR 2",
    "section": "Standard Case (large sample size)",
    "text": "Standard Case (large sample size)\nWhen the sample size is large, the reference distribution for the chi-squared statistic is a Chi-squared distribution with \\((I-1) \\times (J-1)\\) degrees of freedom.\nYou saw the Berkeley Admissions data in the M1L4 narrated lecture. These data are available in R as the object UCBAdmissions. We’ll use the UCBAdmissions data collapsed over department to consider the independence of admission and gender (this is a perilous aggregation – we know that department matters here, but using the aggregated table is useful as an illustration).\nUCBAdmissions stores the data in a 2 x 2 x 6 array, so we’ll first collapse over the third dimension (department) using the margin.table() function, and then perform the \\(\\chi^2\\) test. Please run this chunk of R code (if you haven’t already).\n\n  (UCBA_sum &lt;- margin.table(UCBAdmissions,c(1,2)))\n\n          Gender\nAdmit      Male Female\n  Admitted 1198    557\n  Rejected 1493   1278\n\n  chisq.test(UCBA_sum)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  UCBA_sum\nX-squared = 91.61, df = 1, p-value &lt; 2.2e-16\n\n\nThere are several things to notice.\n\nThe R output for the test is titled Pearson's Chi-squared test with Yates' continuity correction. We’ll have some comments about the continuity correction below.\nNotice that the value of X-squared, which is the chi-squared statistic is 91.61, df = 1 since UCBA_sum is a 2 x 2 table.\nThe p-value associated with this test statistic is very small – we can report it as p &lt; 0.0001. This provides convincing evidence that admission and gender are not independent. Let’s also take a quick look at the mosaic plot:\n\n\n  mosaic(UCBA_sum)\n\n\n\n\nThe vertical split between Male and Female in the Admitted group is substantially farther to the right than the vertical split between Male and Female in the Rejected group – evidence that the two variables, Admit and Gender, are not independent.\nWhat about the continuity correction? There’s actually quite a bit of data in the UCBAdmissions dataset, with N = 4526. In this case, the continuity correction doesn’t make that much difference. In fact, run the following code and compare the X-squared values and the p-values when correct = TRUE (the default) and when correct = FALSE.\n\n  chisq.test(UCBA_sum)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  UCBA_sum\nX-squared = 91.61, df = 1, p-value &lt; 2.2e-16\n\n  chisq.test(UCBA_sum,correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  UCBA_sum\nX-squared = 92.205, df = 1, p-value &lt; 2.2e-16\n\n\nYou should notice that the X-squared value changed a little bit without the continuity correction, but the p-value didn’t change at all. This is fairly typical with really large sample sizes like we have here. The continuity correction doesn’t really make any difference for our inference – we still have really strong evidence that admission and gender are not independent.\nThe continuity correction was introduced to accommodate the use of a continuous probability distribution (the \\(\\chi^2\\) distribution) as the reference distribution for a statistic calculated from non-continuous (i.e., categorical) data. Simply put, it makes the performance of the \\(\\chi^2\\) statistic better (in terms of type II error) when the sample size is of moderate size. We recommend that you simply use the default, correct = TRUE.\nThe \\(\\chi^2\\) test is also called a test for homogeneity in some situations, and the UCBAdmissions dataset provides a good example. Another way that we could consider the independence (or lack of association) between admission and gender is to ask whether the proportion of males admitted is the same as the proportion of females admitted. Put another way, we ask whether the proportions of admits are the same (homogeneous) between the two groups, males and females:\n\n# transpose the UCBA_sum matrix, so Gender is row the variable:\n  (UCBA_sum2 &lt;- t(UCBA_sum))\n\n        Admit\nGender   Admitted Rejected\n  Male       1198     1493\n  Female      557     1278\n\n# perform the test for a difference in two proportions:  \n  prop.test(UCBA_sum2)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  UCBA_sum2\nX-squared = 91.61, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.1129887 0.1703022\nsample estimates:\n   prop 1    prop 2 \n0.4451877 0.3035422 \n\n\nWhen you run this R chunk, you see the identical X-squared statistic to what we got from running chisq.test(UCBA_sum).\nYou’ll also get some output that tells you about the difference between the two admit proportions. That the 95% confidence interval for the difference in the two proportions, 0.113 to 0.170, does not contain zero is another way for us to communicate that we have strong evidence that the two admit proportions are not the same – i.e., that the two genders are not homogeneous in terms of admissions.\nRecall again: simply summarizing this particular dataset using this 2 x 2 table is perilous – we know there’s more to the story here. Indeed, if we collapsed over the admission decision to see the total numbers of applicants within each department, we could use the chi-squared test on this table to see that gender-specific application rates are not homogeneous, which suggests that collapsing over the departments could be a problem.\n\n(UCBApplicants &lt;- margin.table(UCBAdmissions, c(2,3)))\n\n        Dept\nGender     A   B   C   D   E   F\n  Male   825 560 325 417 191 373\n  Female 108  25 593 375 393 341\n\nchisq.test(UCBApplicants)\n\n\n    Pearson's Chi-squared test\n\ndata:  UCBApplicants\nX-squared = 1068.4, df = 5, p-value &lt; 2.2e-16\n\n\nNevertheless, it’s been useful to use the summary data for illustrating equivalence between the chi-squared test and the difference in proportions test."
  },
  {
    "objectID": "z2_learnR.html#data-lady-tasting-tea",
    "href": "z2_learnR.html#data-lady-tasting-tea",
    "title": "LearnR 2",
    "section": "Data: Lady Tasting Tea",
    "text": "Data: Lady Tasting Tea\nThe Lady Tasting Tea is a classic (and classically British) example given by Sir Ronald Fisher to motivate the “exact test” which now bears his name. Briefly, in the tea-tasting experiment, a woman was presented with eight cups of tea – four of which she knew to have had the tea poured first, and four of which she knew to have had the milk poured first – and she was asked to distinguish which cups had tea first and which had milk first.\nThe data from this experiment naturally fall into a 2x2 table where one dimension represents the lady’s guesses and the other dimension represents the truth. See the Examples in the help file for fisher.test() for a full description of these data.\n\nThere’s also a good book called, not surprisingly, “The Lady Tasting Tea,” by David Salsburg that gives an entertaining early history of Statistics.\n\nThe following R chunk recreates the Lady Tasting Tea data.\n\nTeaTasting &lt;-\nmatrix(c(3, 1, 1, 3),\n       nrow = 2,\n       dimnames = list(Guess = c(\"Milk\", \"Tea\"),\n                       Truth = c(\"Milk\", \"Tea\")))\nTeaTasting\n\n      Truth\nGuess  Milk Tea\n  Milk    3   1\n  Tea     1   3\n\n\nThere’s nothing to prevent us from applying the chisq.test() function to the TeaTasting data:\n\nchisq.test(TeaTasting, correct = FALSE)\n\nWarning in chisq.test(TeaTasting, correct = FALSE): Chi-squared approximation\nmay be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  TeaTasting\nX-squared = 2, df = 1, p-value = 0.1573\n\n\nWe do get an answer, but also the helpful warning that “Chi-squared approximation may be incorrect.” Why so? (It’s not because we said correct = FALSE – the same warning will appear either way).\nWe now present a simulation that should give you some insight into the problem here."
  },
  {
    "objectID": "z2_learnR.html#parametric-bootstrapping",
    "href": "z2_learnR.html#parametric-bootstrapping",
    "title": "LearnR 2",
    "section": "Parametric bootstrapping",
    "text": "Parametric bootstrapping\nThe hypothesis of row-column independence can be expressed mathematically in terms of a probability distribution over the cell counts in a table. With R, if you can “draw” a new table from that distribution, you can draw a thousand. Then, you can compute a \\(\\chi^2\\) statistic for each new table, and it turns out that you can use all of those recomputed statistics to approximate the sampling (reference) distribution of that statistic.\nThis idea (sample from a known distribution, calculate a test statistic, repeat) is closely related to the (perhaps familiar) idea of bootstrapping, in which a test statistic is calculated on re-samples (with replacement) from the original sample of data. There are entire books written about bootstrapping, and it’s well beyond the scope of this course to go into details here. We use this technique here to help illustrate the limitations of the Chi-squared distribution as the reference distribution for the \\(\\chi^2\\) statistic in the case of small counts.\nThe essential result is that the reference distribution we get using our re-sampling technique is closer to the true reference distribution of the \\(\\chi^2\\) statistic than is the \\(\\chi^2\\) distribution when the sample size is small (simulation works just fine when the sample size is large as well, it’s just a waste of computation when the known large-sample distribution applies).\nThere are some other tricky details here, because there are multiple probability distributions over cell counts that are consistent with the verbal statement “rows and columns are independent.” For now we’ll just take the simulation script below as given. If you’re interested in some of these details, please see the optional “Sampling Models” section below.\nOnce the re-sampling script computes a lot of chi-squared statistics (stored in resamp$stats) for tables drawn under the hypothesis of independence, we see where we stand by plotting the distribution of those re-sampled chi-squared statistics, and comparing it to the theoretical Chi-squared distribution on a plot.\n\nPlease note: this script is dense and a little complicated. You are not responsible for figuring out how it all works. Running this R chunk just creates the function we need for re-sampling. We’ll use the function further down in the lab.\n\n\ntwo_way_resample &lt;- function(tab, nsim = 2000, fixed_margins = \"none\"){\n  stopifnot(fixed_margins %in% c(\"none\", \"columns\", \"rows\", \"both\"))\n  \n  new_two_way &lt;- function(tab, fixed_margins = \"none\"){\n    I &lt;- nrow(tab)\n    J &lt;- ncol(tab) \n    n_i. &lt;- rowSums(tab)  \n    n._j &lt;- colSums(tab)\n    N &lt;- sum(tab)\n    null_probs &lt;- (n_i. %*% t(n._j))/(N^2)\n    new_table &lt;-  matrix(0, I, J)\n    \n    # Generate random table as SINGLE multinomial sample of size N, preserving neither row nor column margins.\n    if(fixed_margins == \"none\"){\n      new_table &lt;- rmultinom(n = 1, size = N, as.vector(null_probs)) %&gt;% matrix(nrow = I, ncol = J)\n    } \n    # Generate random table as independent multinomial samples in each ROW, preserving original row margins.\n    else if(fixed_margins == \"rows\"){\n      for(i in 1:I){\n        new_table[i, ] &lt;- rmultinom(n = 1, size = n_i.[i], prob = null_probs[i ,]) # rmultinom automatically renormalizes the row i probs.\n      }\n      new_table %&lt;&gt;% matrix(nrow = I, ncol = J)\n    } \n    # Generate random table as independent multinomial samples in each COLUMN, preserving original column margins.\n    else if(fixed_margins == \"columns\"){\n      for(j in 1:J){\n        new_table[, j] &lt;- rmultinom(n = 1, size = n._j[j], prob = null_probs[, j]) # rmultinom automatically renormalizing the column j probs.\n      }\n      new_table %&lt;&gt;% matrix(nrow = I, ncol = J)\n    }\n    # Generate random table by sampling from among all possible I x J tables with the given row and column margins.\n    else if(fixed_margins == \"both\"){\n      new_table &lt;- r2dtable(n = 1, r = n_i., c = n._j)[[1]]\n    }\n    return(new_table)\n  }\n\n  chisqs &lt;- rep(NA, nsim)\n  for(j in 1:nsim){\n    chisqs[j] &lt;- new_two_way(tab, fixed_margins) %&gt;% chisq_stat0\n  }\n  obs_chisq &lt;- chisq_stat0(tab)\n  pval &lt;- sum(chisqs &gt;= obs_chisq)/nsim  # One-sided\n  return(list(stats = chisqs, obs_chisq = obs_chisq, pval = pval))\n}"
  },
  {
    "objectID": "z2_learnR.html#sampling-models-optional",
    "href": "z2_learnR.html#sampling-models-optional",
    "title": "LearnR 2",
    "section": "Sampling Models (Optional)",
    "text": "Sampling Models (Optional)\nIn this section, we describe some of the details of the code for re-sampling, in particular the different types of probability distributions from which we can draw the re-samples. In the example of running the code below for the Lady Tasting Tea data, we perform the re-sampling only based on the method imposed by the experimental design in the actual Lady Tasting Tea experiment.\nAs mentioned above, to use the re-sampling function, we need to make the verbal statement “sample a new 2-way table under the hypothesis of independence” precise enough to do some computing. There are several different ways that the Lady Tasting Tea experiment could have been carried out. Each possible sampling model below corresponds to a different way the tea-tasting experiment could have been designed and performed – statistical analysis depends on the experiment design (as it should!). We’ll also point out which way the experiment was actually performed.\n\nUnrestricted Sampling\nConsider this scenario: we give the lady however many cups of tea she feels like tasting, and we don’t tell her anything about how many of each type (milk-first or tea-first) there are. This “unrestricted” sampling can be modeled using a Poisson distribution. [This case is included for sake of completeness – in the context of inference, this and the next situation, where the total sample size is fixed can be treated the same.]\n\n\nTotal Sample Size Fixed\nNow consider this scenario: The lady is to be presented with eight cups of tea, but the number that are milk-first is not predetermined. In the actual Lady Tasting Tea data, there were four milk-first cups, but in this scenario, there could have been two, five, eight, any number between zero and eight. In this scenario, the lady is not told how many cups of each type (milk-first or tea-first) there were. To perform this sampling, we use a multinomial distribution for the entire table (i.e., with N = 8 and four categories).\n\n\nOne Margin Fixed\nIn another scenario that could have produced the Lady Tasting Tea data, suppose there are four cups of each type of tea, but the lady doesn’t know there are four of each. In this case “The Truth” margins are fixed (at four), but the “Guess” margins are not. For this type of sampling, we use independent multinomials within rows or within columns (i.e., with N = 4 and two categories).\n\n\nBoth Margins Fixed\n\nThis is the actual situation of Fisher’s original Lady Tasting Tea experiment. Four cups of each type were presented, and the lady knew that she would taste four of each. Because of this, the row margins are fixed at four and the column margins also fixed at four. In this case, we base the sampling on (generalized) hypergeometric distribution."
  },
  {
    "objectID": "z2_learnR.html#examples-of-re-sampling-lady-tasting-tea",
    "href": "z2_learnR.html#examples-of-re-sampling-lady-tasting-tea",
    "title": "LearnR 2",
    "section": "Examples of Re-sampling: Lady Tasting Tea",
    "text": "Examples of Re-sampling: Lady Tasting Tea\nIn the following R chunk, we re-sample the Lady Tasting Tea data, based on the sampling design imposed by the original experimental setup (both margins fixed). Go ahead and run the R chunk and take a look at the resulting plot.\n\nresamp_tea &lt;- two_way_resample(TeaTasting, nsim = 2000, fixed_margins = \"both\") \nchisq_val &lt;- seq(0,8,length=2000)\nchisq_theory &lt;- dchisq(chisq_val,1)\n# fix both row and column margins\nggplot() +\n  ggtitle(\"Re-sampling distribution of chi-squared stat vs \n        theoretical chi-squared distribution\") +\n  xlab(expression(X^2)) +\n  ylab(\"\") +\n  geom_line(mapping = aes(x = chisq_val, y = chisq_theory), color = \"blue\") +\n  geom_density(mapping = aes(x = resamp_tea$stats), bw = 0.05) + \n  geom_vline(aes(xintercept = chisq_stat0(TeaTasting)), color = \"red\")\n\n\n\n\nIn this plot, the smooth blue line is the theoretical Chi-squared (df = 1) reference distribution. The black line with a few spikes is the reference distribution we get by running the re-sampling code. There are actually only a few values of the chi-squared statistic that are possible with such a sparse table. Finally, the vertical red line is the value of the chi-squared statistic calculated from the observed table.\nThe problem here is that with such small counts in the table, there are only a small number of rearranged (re-sampled) tables possible. There just aren’t that many ways to rearrange eight cups of tea into a 2x2 table of counts, especially if we also fix both column and row totals! Therefore, only a limited number of tables, and an even more limited number of chi-squared statistics, can possibly be obtained.\nThis is precisely why using the large sample reference distribution (blue line) isn’t such a good idea here, because is does a really poor job of approximating the resampled reference distribution (black line).\nIt might seem like it was overkill to simulate 2000 samples and calculate the chi-squared statistic for each one, when there were actually only three possible chi-squared statistics for this situation! That is, maybe there is some way we could have have worked this out by hand. Indeed, working it out “exactly” by hand is what Fisher did – his test involves (essentially) listing every possible way that the chi-squared statistic could have been bigger than it was, and calculating the probability of each of those ways.\nExecute the following R chunk, which applies Fisher’s Exact Test to the Lady Tasting Tea data.\n\nfisher.test(TeaTasting)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  TeaTasting\np-value = 0.4857\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.2117329 621.9337505\nsample estimates:\nodds ratio \n  6.408309 \n\n\nFisher’s Exact Test agrees pretty closely with the final simulation above – because Fisher’s exact test supposes that both rows and columns of the tables are fixed by design. Restricting it in this way, as we see above, reduces the number of possible tables to deal with, which makes it easier to calculate the p-value “by hand.” And in the tea-tasting example, these restrictions actually do make sense – there are 4 cups of each kind of tea, the lady knows there are 4 of each, there are only a few ways this thing can go.\nWe’d also like to call your attention to the odds ratio and 95% confidence interval estimates that appear in the fisher.test() output. Rather than using a chi-squared statistic for making inference, the fisher.test() function uses an odds ratio test.\n\nImportant note: tests for equal odds ratios, equal proportions and homogeneity will yield similar inferences in the case of 2x2 tables. You just have to sort out which procedure makes the most sense for the data you are dealing with."
  },
  {
    "objectID": "z2_learnR.html#beyond-fixed-margins",
    "href": "z2_learnR.html#beyond-fixed-margins",
    "title": "LearnR 2",
    "section": "Beyond Fixed Margins",
    "text": "Beyond Fixed Margins\nIn many (most) cases, the assumption that all the row and column totals are fixed doesn’t make sense and isn’t needed. For example, in a medical study, we might recruit 30 people with a disease; give treatment A to half of them and treatment B to the other half; and record whether or not they were cured after some period of time. In this case, we’ve fixed the row totals by design, because we put 15 patients in each group, but we haven’t fixed the column totals – we can’t say in advance know how many patients will be cured under either treatment. And, in this case, we still have the potential problem of small cell counts.\nFisher’s exact test was designed for precisely the situation of the Lady Tasting Tea (statistically, both margins are fixed). Nevertheless, it is used in many cases to test independence when the cell counts are small, as in the hypothetical medical study just described.\nYou are welcome to replicate the call to the two_way_resample() function, changing the fixed_margins = \"both\" argument to any of \"none\", \"rows\" or \"columns\". Using the Lady Tasting Tea data, you’ll see that the Chi-squared (df = 1) distribution gives a poor approximation to the re-sampled distribution, because the cell counts are so small!"
  },
  {
    "objectID": "z2_learnR.html#beyond-2-x-2-tables",
    "href": "z2_learnR.html#beyond-2-x-2-tables",
    "title": "LearnR 2",
    "section": "Beyond 2 x 2 Tables",
    "text": "Beyond 2 x 2 Tables\nYou can apply chisq.test() and fisher.test() to general \\(I \\times J\\) tables, but you will only learn about the independence hypothesis. That is, you will not get any estimates. For estimates proportions and odds ratios from \\(I \\times J\\) tables, we have to turn to more sophisticated statistical models such as generalized linear models – the topic of upcoming course modules."
  },
  {
    "objectID": "z2_learnR.html#r-markdown-for-submission",
    "href": "z2_learnR.html#r-markdown-for-submission",
    "title": "LearnR 2",
    "section": "R Markdown for submission",
    "text": "R Markdown for submission\nLast week’s instructions are repeated here for your convenience.\nNOTE: If you are reading these instructions from the HTML preview in the Viewer pane, they will be overwritten in the following steps. You can avoid this by “popping out” the current HTML preview into an external browser window, by clicking the the “Show in new window” icon (to the right of the broom icon in the Viewer pane). \nAlternatively, you can read the instructions right from the .Rmd script, keeping in mind that you will need to switch back to the lab script tab to view the rest of the instructions once you create a new script. You could also copy and paste the whole Lab 2 Assignment Instructions section into your new document while you’re working on it, and then delete the instructions before you submit it.\nClick “File” -&gt; “New File” -&gt; “R Markdown”, and dialog box will pop up. Change the title to “Lab Assignment 1” and name yourself as author. Select PDF as the Default Output Format, then click OK. The header of your new file should look something like this:\n---\ntitle: \"Lab Assignment 2\"\nauthor: \"Ronald Fisher\"\ndate: \"2024-04-04\"\noutput: pdf_document\n---\n\nThe file will initially contain some examples to get you started with RMarkdown, which you should replace with your lab content. Save the notebook as something like “Lab_Assignment_1” using “File” –&gt; “Save As…”\nIn your new .Rmd script, answer the questions in the “Questions” section below. Include all the code you need to produce the requested outputs. Your script should include a top-level section heading for each question, for example:\n# Question 1\n\nstuff here\n\n# Question 2\n\nother stuff\nWhether or not you include the text of the questions in your script is up to you.\n\nDo be sure to include, near the top of your script, a code chunk that loads any non-default packages you use (such as vcdExtra or Sleuth3).\n\nWithin the question sections, you can chunk your code in whatever way seems reasonable. Incorporate any written answers outside the code chunks using Markdown formatting as needed (see “Help” -&gt; “RMarkdown Quick Reference” for text formatting help).\nTo ensure that your .Rmd script will be fully self-contained (i.e. it will not depend on objects that were defined during the lab, and could be run as-is if you sent it to someone else), you should clear the workspace before you begin.\n\nTo clear the workspace, click the broom icon in the Environment pane.\n\nOnce you’ve answered the questions in your new .Rmd script and you have verified your code is self contained, you should Run -&gt; Run All Chunks and generate a .pdf file of your document, to check that everything looks like you want it to. Having concluded that your .Rmd script produces a pdf document that includes all the output you want, submit both the Lab_Assignment_2.Rmd file and the pdf document on Canvas as your Lab Assignment 2.\nFeel free to post on the discussion board if you have any questions or encounter any difficulties with this process."
  },
  {
    "objectID": "z2_learnR.html#questions",
    "href": "z2_learnR.html#questions",
    "title": "LearnR 2",
    "section": "Questions:",
    "text": "Questions:\n\nNOTE: The first two questions are for credit. The next two are optional/for fun.\n\n\nQ1.\nThe following data come from one of Gregor Mendel’s famous experiments with pea plants. In this particular experiment, Mendel examined two categorical traits in pea seeds, each with two possible values: seed color – yellow or green – and seed shape – round or angular. According to Mendel’s hypothesis, the inheritance pattern of these traits was characterized by “independent assortment” – that is, there was no “genetic linkage” between these traits.\n\nmendel &lt;- as.table(\n  matrix(c(315, 108, 101, 32),\n         nrow = 2,\n         dimnames = \n           list(Color= c(\"Yellow\", \"Green\"),\n                Shape = c(\"Round\", \"Angular\"))))\nmendel\n\n        Shape\nColor    Round Angular\n  Yellow   315     101\n  Green    108      32\n\n\nHow consistent are these data with Mendel’s hypothesis of independence? Notice that in this particular case, we’re not interested in a departure from independence, but rather a confirmation of independence. Is a Chi-squared test appropriate here? Why or why not?\n\n\nQ2.\nWe mentioned previously, and it’s mentioned in the narrated lecture materials that there’s an equivalence between the Chi-squared test and the difference in proportions test in the case of 2x2 tables. Take a look for the Mendel data:\n\nchisq.test(mendel, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  mendel\nX-squared = 0.11634, df = 1, p-value = 0.733\n\n\n\nprop.test(mendel, correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  mendel\nX-squared = 0.11634, df = 1, p-value = 0.733\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.09506177  0.06662771\nsample estimates:\n   prop 1    prop 2 \n0.7572115 0.7714286 \n\n\nThe output from the second chunk refers to a two.sided test, with an interpretation in terms of a signed difference between two proportions, whereas the chisq.test() is a one-sided test, with an interpretation involving deviation from the hypothesis of independent rows and columns. It’s not entirely obvious why these results should be the same. The following exercise can help you see what’s going on.\nUse rnorm() and rchisq() to produce and store the following vectors:\n1000 draws from the standard normal distribution, 1000 draws from the Chi-squared distribution with 1 degree of freedom.\n\nConstruct separate histograms/density plots of the raw chi-squared values and the squares of the standard normal values. What do you observe?\nRecall (or obtain from qnorm(0.975) that the 2-sided 95% critical values for the standard normal distribution are ± 1.96. Use qchisq() to compare the square of these 2-sided 95% critical values from the standard normal to the one-sided 95% critical value for the Chi-squared(1). What does this suggest about tests based on these two reference distributions?\n\nEND REQUIRED QUESTIONS\n\nThe next two questions are optional – you do not need to submit anything about them for your assignment.\n\n\n\nQ3.\nThis isn’t actually a question, just some reading and code to work through. It’s an interesting continuation of Mendel’s independence hypothesis, and a slightly different application of chisq.test() in R.\nMendel actually had a more sophisticated hypothesis than “seed color and seed shape are independent” – he knew how they should be independent based on his mechanistic understanding of the process of inheritance of dominant and recessive traits. On these grounds, Mendel hypothesized specific proportions for each cell in this particular table. He knew that if his ideas were correct, the cell proportions for his experiment should have been, specifically:\n\n(mendel_expect &lt;- cbind(c(9, 3), c(3,1)))/16\n\n       [,1]   [,2]\n[1,] 0.5625 0.1875\n[2,] 0.1875 0.0625\n\n\nThis is a more stringent requirement than row/column independence, since there would be many possible tables consistent with some kind of independence relationship, but radically inconsistent with Mendel’s scientific understanding. For instance, if the upper right and lower left cell counts had been reversed, we would get the same chi-squared statistic for the table (check!) but seeing this would have suggested that Mendel was somehow seriously mistaken.\nHere, we need a different approach than the chi-squared test of independence – we want to test an entire proposed distribution over the 4 cells. The tool for this job is the chi-squared test of goodness of fit, which in R is also performed by chisq.test().\nThe goodness of fit test is supposed to test the distribution of a single variable, so chisq.test() will perform the goodness of fit test if the data are provided as a single vector, rather than as a 2 x 2 table. Since we have particular distribution in mind, we’ll need to provide those hypothesized probabilities as well:\n\n(mendel_vec &lt;- as.vector(mendel)) # stretch out the table and make sure we know HOW it was stretched out, in order to get the hypothesized probabilities associated with the correct table cells.\n\n[1] 315 108 101  32\n\n\nAnd now preform the Chi-squared test passing in the vector of observed counts and the vector of specific proportions:\n\nchisq.test(mendel_vec, p = c(9/16, 3/16, 3/16, 1/16))\n\n\n    Chi-squared test for given probabilities\n\ndata:  mendel_vec\nX-squared = 0.47002, df = 3, p-value = 0.9254\n\n\nNote that the degrees of freedom are different – 4-1 = 3 instead of (2-1)(2-1) = 1. A very rough idea of why this should be is that with a more specific hypothesis (“each cell probability should be exactly so”) there are more ways the data could deviate from it.\nAn interesting historical note: R.A. Fisher, when reviewing Mendel’s original data, applied the chi-squared test to some data like these, and reached the conclusion that deviations were too small to have arisen by chance. He thought that the independence model fit so closely that Mendel’s data must have been deliberately doctored – though in his great respect for Mendel, Fisher laid the blame on “some gardening assistant” who must have known too well what his supervisor hoped to see.\n\n\nQ4.\nSuppose we performed the tea-tasting experiment, and produced a table with 4’s in the diagonal and 0’s in the off-diagonal cells – that is, suppose the lady performed as well as the experiment allowed. If we had observed this (best possible) performance under the fixed-columns design (where the lady didn’t know the number of cups of each type), what p-value would she score? (use resampling script). What if we observed this same performance under the fixed-rows and fixed-columns design (where she knew there were 4 cups of each)?\nDoes it make sense that the best-possible performance earns a smaller p-value under the more “challenging” experimental setup than under the “easier” one? What does this say about which design we might prefer to use?"
  },
  {
    "objectID": "z0_learnR.html",
    "href": "z0_learnR.html",
    "title": "LearnR 0",
    "section": "",
    "text": "Learn R 0"
  },
  {
    "objectID": "z0_learnR.html#viewing-html",
    "href": "z0_learnR.html#viewing-html",
    "title": "LearnR 0",
    "section": "Viewing HTML",
    "text": "Viewing HTML\nWithin RStudio, generate an HTML preview from the Rmd script by clicking the “Preview” icon, located at the top of source (code) pane. If you don’t see a “Preview” icon, you may need to update RStudio. Check your RStudio version with “Help” -&gt; “About RStudio.” The current version is 2023.03.0.\nBy default, the preview will appear in RStudio Viewer pane (usually found in the lower right-hand section of the RStudio window), so that you can see the two versions (Rmd and HTML) side by side. If the preview doesn’t appear (or pops out in its own window), check the preview settings, in the dropdown menu from the gear icon next to the Preview icon. If you want, you can customize the arrangement of the four RStudio panes through “Tools” -&gt; “Global Options” -&gt; “Pane Layout”.\nThe HTML preview will contain text and code, along with the output of any code chunks that were run before the preview was created. You can refresh the preview at any time without affecting the script. If the preview doesn’t appear after you click the “Preview” icon, please click it once more.\n\nIf you prefer to view complete code output (including graphs) in your HTML view, select “Knit to HTML” from the “Preview” dropdown menu."
  },
  {
    "objectID": "z0_learnR.html#navigation",
    "href": "z0_learnR.html#navigation",
    "title": "LearnR 0",
    "section": "Navigation",
    "text": "Navigation\nWithin the R Markdown script, you can jump to another section by clicking on “Navigation” (or some of you might see “#”) in the lower left corner of the source pane.\nPressing Alt+Shift+K brings up a pane that lists all of the RStudio keyboard shortcuts (you can also access this from the RStudio Help menu – Keyboard Shortcuts Help). One problem is that the command sequences on the Windows, iOS and Linux platforms can be slightly different. If you want to use these navigation shortcuts, please familiarize yourselves with what’s appropriate for your operating system. Press Esc to close the shortcut pane.\nSome examples of keyboard shortcuts: Ctrl+1 and Ctrl+2 switch the cursor location between the console and the source windows, and Ctrl+Shift+0 restores all windows (in case some have been minimized or pushed out of sight)."
  },
  {
    "objectID": "z0_learnR.html#folds",
    "href": "z0_learnR.html#folds",
    "title": "LearnR 0",
    "section": "Folds",
    "text": "Folds\nA “fold” is a subset of the R Markdown (Rmd) file that you can collapse and expand. This is not necessary to do, but it can be useful if you’re working on just one portion of your document. For example, place your cursor HERE in the R Markdown file, and use the collapse fold command (Alt+L) and see what happens. You can expand the fold again using the expand fold command (Shift+Alt+L). You can also collapse and expand folds by clicking on the small triangles (downward pointing when the fold is expanded; pointing right when the fold is collapsed) in the left hand margin of the R Markdown file, just to the right of the line numbers.\nThere is also a keyboard shortcut to “collapse all folds.” For a “cleaner” viewing experience, you may wish to collapse all folds at the beginning of each lab, and expand individual sections as you work through them. Note that collapsing or expanding only applies to the R Markdown and does not affect the HTML preview (even if you generate it again after collapsing or expanding).\nOccasionally, expanding and collapsing too much too fast can lead to persistent screen artifacts that interfere with the usability of the notebook. If this happens, you can select “Clear All Output” from the gear icon at the top of the source pane, and proceed."
  },
  {
    "objectID": "z0_learnR.html#chunks",
    "href": "z0_learnR.html#chunks",
    "title": "LearnR 0",
    "section": "Chunks",
    "text": "Chunks\n“Chunks” in the R Markdown file are delimited by three backticks: ```. For example, below is a chunk of R code.\nExecute the code chunk by placing your cursor inside the chunk (i.e., in the light gray area) and pressing Ctrl+Shift+Enter, or by clicking the green arrow in the upper right corner of the chunk.\n\nimage(volcano)\n\n\n\n\n\nIn general, you should run each successive code chunk as you encounter it.\n\nIf you prefer, you can produce all the lab output in advance using “Run All Chunks” (from the “Run” menu at the top of the source pane), but we recommend trying the interactive experience.\nWhen you execute a code chunk in an R Notebook, the result appears inline directly beneath chunk. If you did not see a plot appear beneath the chunk when you execute it, you may need to update RStudio. Assuming the output appeared, it can be hidden again by clicking on the “x” in upper right hand corner of the output area.\nIf you would like to see code output in the HTML preview, after executing the chunk, click Preview again to refresh the HTML. Over in the HTML preview, you’ll see the volcano image, and you’ll see the R code that created that image. You have the option in the HTML preview to hide the code by clicking the “Hide” button above the code. You can specify that code from a particular chunk is not to be included in the HTML by heading the chunk with {r, echo=F}, instead of just {r}. Go ahead and try this if you want, remembering that you have to execute the chunk and then refresh the Preview.\nThe {r} (or {R}) after the backticks in the first line of the chunk is essential – it specifies that this chunk of the R Markdown file should be interpreted as R code. R Markdown documents can also contain chunks of code written in some other languages (like {python} and {sql}). However, in this class, we will only use R code. Contents of chunks without an {r} will not be evaluated as R code. For example:\n# Here is some code that opens a new tab in RStudio,\n# which could disrupt the flow of the lab if it were executed by default.  \n# You can make the chunk executable by putting an {r} after the first ``` that delimits the chunk.\n\nView(mtcars)\nNotice that without the {r} after the opening backticks, you don’t even have the option of executing the code. Similarly, code written outside a code chunk will not be interpreted. For example, the following line is valid R code:\ny &lt;- c(1,2)\nyet it will not be executed in this script, because it is not enclosed in an R code chunk.\n\nR Chunks and Global Propagation\nWhile chunk output will (typically) appear inline in the script (i.e., in the R Markdown file), the actions taken within a chunk propagate globally – once a chunk is executed, the variables assigned, functions defined, etc, are available in R’s global environment for the current project, and are therefore accessible from the console window.\nHere’s an example. The following code chunk simulates rolling a fair 6-sided die 20 times, and shows different representations of the outcomes. Notice that several of the commands in the R chunk below are enclosed in parentheses (e.g., (die &lt;- 1:6)). This is a nice feature in that it will not only assign the variable as directed by the code inside the parentheses, but it also then prints the value of that variable. Go ahead and execute the chunk and take a look at the inline output below it.\n\n# Simulate 20 rolls of a fair 6-sided die\nset.seed(1066)\nk &lt;- 6 # number of sides\nn &lt;- 20 # number of trials\n\n# Possible outcomes for each trial\n(sides &lt;- 1:k)\n\n[1] 1 2 3 4 5 6\n\n# Vector of equal probabilities for each outcome\n(p_vec &lt;- rep(1/k, k)) \n\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\n\n# Use the sample function to simulate n trials\n(outcomes &lt;- sample(sides, n, replace = TRUE, prob = p_vec))\n\n [1] 3 6 5 3 3 1 5 4 6 6 3 6 1 2 2 2 1 1 5 3\n\n# Frequency of each outcome\n(counts &lt;- table(outcomes)) \n\noutcomes\n1 2 3 4 5 6 \n4 3 5 1 3 4 \n\n\nNow type in barplot(counts) at the console. You should get a plot – but it will not appear inline beneath the chunk, since it was not called from within the chunk. Objects like counts that are defined in a script chunk can be accessed from the console, and when you access those objects from the console, they behave like any other objects in the global environment.\n\n\nManipulating Chunks\nDuring every lab, you are encouraged to experiment in the console and/or source panes. You can modify existing chunks, at the minor risk of breaking the lab script and necessitating a reload. Preferably, you can create your own chunks anywhere within the document, by typing out the triple backticks and bracketed {r} as above. As a shortcut, you can Insert a new chunk by pressing Ctrl+Alt+I (or the equivalent on your system).\nYou can use the console to access objects defined in the chunks, but using the console to modify such objects is generally inadvisable – you don’t want the contents of an apparently-self-contained chunk of script to depend on unrecorded console activity. This is actually quite important to keep in mind throughout your work in this course.\n\nChunks should either be self-contained or depend only on chunks above them.\n\nAs a corollary to the statement above: if you ever wish to skip some material and resume the lab activity at further down, you can “Run All Chunks Above” (in the “Run” menu in the upper right corner of the source pane), to ensure that any objects defined in the skipped portion are available going forward.\nIf you want to the modify objects in a code chunk, edit the chunk itself (or a copy of it) so that the behavior of each chunk remains clear.\n\nIn the Rmd script, copy the previous chunk of R code here, modify it so that it simulates 50 flips of a fair coin, and execute it. For simplicity, regard an outcome of 1 as “heads” and an outcome of 2 as “tails”. (Hint: you only need to change 2 characters in the whole chunk to accomplish this).\n\nObserve that running the modified chunk has changed the state of outcomes, counts, etc. If you scroll back up and re-run the original chunk, these will be changed back to their previous states.\nFor more information about using the RMarkdown format, see the Markdown Quick Reference, the RMarkdown Reference Guide, and/or the RMarkdown Cheatsheet – all available from within the RStudio Help menu."
  },
  {
    "objectID": "z0_learnR.html#upgrading-visualizations-from-qplot-to-ggplot",
    "href": "z0_learnR.html#upgrading-visualizations-from-qplot-to-ggplot",
    "title": "LearnR 0",
    "section": "Upgrading Visualizations from qplot() to ggplot()",
    "text": "Upgrading Visualizations from qplot() to ggplot()\nIn previous labs, you’ve seen the qplot() function, which is a simplified wrapper for ggplot(). In this class, we will sometimes need to draw on the full power of ggplot() for more customized visualizations.\nTo illustrate, we’ll bring in a data set you will see again in Lab 3 – the Donner Party data from the vcdExtra package.\n\nhead(Donner)\n\n\n\n\n\n\nfamily\nage\nsex\nsurvived\ndeath\n\n\n\n\nAntoine\nOther\n23\nMale\n0\n1846-12-29\n\n\nBreen, Edward\nBreen\n13\nMale\n1\nNA\n\n\nBreen, Margaret I.\nBreen\n1\nFemale\n1\nNA\n\n\nBreen, James\nBreen\n5\nMale\n1\nNA\n\n\nBreen, John\nBreen\n14\nMale\n1\nNA\n\n\nBreen, Mary\nBreen\n40\nFemale\n1\nNA\n\n\n\n\n\n\nWe want a plot that shows whether individuals survived, as a function of their sex and age. With qplot(), we could do this as follows:\n\nqplot(data = Donner, x = age, y = survived, color = sex, geom = \"point\", main = \"Needs some jitter\") \n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nBut there’s a subtle problem with this plot. There are 90 observations in this data set,\n\nnrow(Donner) \n\n[1] 90\n\n\nbut it looks like there are a lot fewer than 90 points on that plot. There must be some overplotting happening – that is, there must be some combinations of age and survived that contain contain multiple individuals, and those individuals are not being distinguished in the plot.\nHere’s a table that shows the number of individuals at each unique combination of age and survived.\n\nxtabs(data = Donner, ~ survived + age) # you will learn how to use xtabs() in Lab 1; don't worry about it now\n\n        age\nsurvived 1 2 3 4 5 6 7 8 9 11 12 13 14 15 18 20 21 22 23 24 25 27 28 30 32 35\n       0 5 0 3 1 2 0 0 0 0  0  2  0  0  1  0  0  0  0  3  2  6  1  2  3  0  1\n       1 2 1 2 2 1 2 2 2 4  1  1  3  3  1  1  3  1  1  2  1  2  0  2  2  3  0\n        age\nsurvived 36 40 44 45 46 47 51 57 60 62 65 70\n       0  1  1  1  1  0  1  0  1  1  1  1  1\n       1  0  1  0  0  1  0  1  0  0  0  0  0\n\n\nThis table shows the number of individuals who must be represented by each unique point on the current plot. Whenever a number is &gt;1, there’s overplotting happening.\nWith qplot(), we can jitter the points with geom = \"jitter\" to help with this overplotting:\n\nqplot(data = Donner, x = age, y = survived, color = sex, geom = \"jitter\", main = \"Way too much jitter\")\n\n\n\n\nBut this plot looks terrible – we need finer control over the degree of jitter. To achieve this, we’ll have to step up from qplot() to full ggplot(), where we’ll specify the jittering parameters in a separate geom, outside the main plot call.\n\nggplot(data = Donner, mapping = aes(x = age, y = survived, color = sex)) + \n  geom_jitter(height = 0.05, width = 0.5) + # see ?geom_jitter for details\n  ggtitle(\"Reasonable jitter\")\n\n\n\n\nThe pseudocode blocks below compare the structure of a basic qplot() call and basic ggplot() call:\nqplot(\ndata = &lt;yourdata&gt;, \n&lt;plot attribute&gt; = &lt;variable&gt;\n&lt;another plot attribute&gt; = &lt;another variable&gt;, \ngeom = \"&lt;typeofplot&gt;\"\n)\nggplot(\ndata = &lt;yourdata&gt;, \nmapping = aes(\n          &lt;plot attribute&gt; = &lt;variable&gt;,\n          &lt;another plot attribute&gt; = &lt;another variable&gt;\n          )\n) + geom_&lt;typeofplot&gt;(\n                      &lt;special plotting arguments&gt;\n                      )\nFor quick reference, see the cheatsheet, accessible in RStudio via “Help” -&gt; “Cheatsheets” -&gt; “Data Visualization with ggplot2. For more in-depth coverage, see R for Data Science, Chapter 3."
  },
  {
    "objectID": "z0_learnR.html#transforming-data-with-tidyr-and-dplyr",
    "href": "z0_learnR.html#transforming-data-with-tidyr-and-dplyr",
    "title": "LearnR 0",
    "section": "Transforming Data with tidyr and dplyr",
    "text": "Transforming Data with tidyr and dplyr\ndplyr and tidyr are tidyverse packages containing utilities for manipulating data frames – these are the “spreadsheet” functions, which let you do in R what you might otherwise be inclined to do in Excel. Here’s brief summary of the 7 key data-manipulation functions, all of which take a data frame as input and produce a data frame as output.\nThe first two functions are from the tidyr package, which helps you get your data frame into what’s known as the “tidy format”:\n\nTidy data has one column for each variable and one row for each observation\n\nThese functions help fix the two most obvious ways that a data set can be untidy. Some of this may seem rather abstract at this point, but you will see concrete examples as the course progresses.\n\npivot_longer() collects a variable whose values occupy multiple columns into one column. Thus, it makes “wide” data frames “taller” by stacking them up into more rows.\npivot_wider() is the reverse of pivot_longer(). It collects observations whose values occupy multiple rows. Thus, it makes “tall” data frames “wider” by spreading them out over more columns.\n\nThe next five functions are from the dplyr package.\n\nmutate() creates new columns as a function of existing columns.\nsummarize() calculates group-level summaries for some column, within groups defined by another column.\nselect() subsets columns by name (as opposed to subsetting by column number with brackets with [, x])\nfilter() subsets the rows by a logical condition about the columns (as opposed to subsetting by row number with [x, ])\narrange() reorders rows – for instance, sort the whole data frame by order of the values from a chosen column\n\nFor quick reference on these functions, see the cheatsheet accessible in RStudio via “Help” -&gt; Cheatsheets” -&gt; “Data transformation with dplyr” and Data tidying with tidyr. For more in-depth coverage, see R for Data Science, Chapter 5.\n\nIf you have not used dplyr before, you are encouraged read Chapter 5 of R for Data Science and practice some of the exercises therein to familiarize yourself with these utilities.\n\nNo discussion of using RStudio like a spreadsheet would be complete with mentioning View(). Using this on any data frame will bring up a new tab with a simple spreadsheet-like interface (not editable, but with built-in sorting and filtering capabilities).\nNote: View() can only display the first 100 columns of a data frame, and does not explicitly warn you of this limitation. If you want to View() a data frame with more than 100 columns, you’ll need to choose which set of 100 or fewer to View() at one time.\n\nThe Pipe Operator\nAll of the data-manipulation functions described are designed to work best with the pipe operator %&gt;%, described in next section (optional, though a useful reference going forward). The pipe provides a particularly convenient syntax for chaining together a sequence of data-manipulation steps.\nPerhaps the shortest useful description of the pipe is that, given functions f and g and an object x, it lets you express this:\ng(f(x))\nlike this:\nx %&gt;% f %&gt;% g\nThis is more useful than it looks, as described in the next (optional) section.\nIf you prefer to skip that last section, you’re done and you can move on to Lab 1. There is nothing you need to submit for Lab 0, although you may wish to return to it later if you need to remind yourself how to use some features of the R notebook system. There will be a short graded assignment attached to the end of each subsequent lab, starting with Lab 1.\nRemember post to the discussion board if you were unable to install required packages. Do try updating R and RStudio first, though."
  },
  {
    "objectID": "z0_learnR.html#composing-functions-with-the-pipe-optional",
    "href": "z0_learnR.html#composing-functions-with-the-pipe-optional",
    "title": "LearnR 0",
    "section": "Composing Functions with The Pipe (optional)",
    "text": "Composing Functions with The Pipe (optional)\n\nPrograms should be written for people to read, and only incidentally for machines to execute - SICP.\n\nWhen we use the output of one function as the input of another function, we call that composing one function with the other – read f(g(x)) as “f composed with g applied to x”. In later labs we will make extensive use of the generalized linear model, which extends the regular linear model by composing a nonlinear function with the usual linear function of the predictors. Deep neural networks implement extremely complex classification rules that ultimately consist of thousands of simpler functions composed together. Function composition is a powerful concept – it’s what allows us to build up bigger and better functions out of simple, re-usable components. In this section we’ll introduce a convenient idiom for expressing function composition in R.\nLet’s start with a simple example of ordinary function composition. We’ll compose the log() function with the factorial() function:\n\nlog(factorial(5))\n\n[1] 4.787492\n\n\nYou can think of this as taking the output of the factorial function and feeding it in as the input to the log function. You can also think of it as effectively applying a whole new “log-factorial” function,\n\nlog.factorial &lt;- function(x)log(factorial(x))\nlog.factorial(5)\n\n[1] 4.787492\n\n\nSame difference.\nThis is also an example of function composition:\n\nexp(2 * (7 + 1))\n\n[1] 8886111\n\n\nBut what are the functions? We could write it out in a less familiar but more “standard” way like so:\n\nexp(multiply_by(2, add(7, 1))) # you can see that `+` and `*` operators are really just functions with a weird syntax.  ?extract shows a full list of available 'functional aliases' for common operators (like the subsetting brackets, []).\n\n[1] 8886111\n\n\nEvidently we’re composing all the time, without even thinking about it. Unfortunately, it’s not always so easy. The more operations we want to compose, the more nesting parentheses we’ll need, and less readable our nested expression becomes. As a rather silly example, imagine if we didn’t have the + syntax for addition, and adding up the numbers 1:5 looked like this:\n\nadd(5, add(4, add(3, add(2, 1)))) # We need to go deeper!\n\n[1] 15\n\n\nAlternatively, we could avoid that by reassigning each intermediate step to a placeholder, which might look like this:\n\nu &lt;- 1\nu &lt;- add(u, 2)\nu &lt;- add(u, 3)\nu &lt;- add(u, 4)\nu &lt;- add(u, 5)\nu\n\n[1] 15\n\n\nWhile both of those approaches have their places, sometimes we’d like something else.\n\nThe %&gt;% operator pipes the output of any function into the input of another.\n\nWe can replace repeated + with piped add() like so:\n\n1 %&gt;% add(2) %&gt;% add(3) %&gt;% add(4) %&gt;% add(5)\n\n[1] 15\n\n\nadd() takes two inputs and produces one output. Each pipe sends output of evaluating the thing to the left into the first open argument position of the function to the right. The numbers in parentheses supply the second arguments to each add function.\nOf course, we could have done that all with +, or even more easily with sum(). Here’s an example with a sequence of different functions, where such an approach isn’t available.\n\n# Illustration of the same computation using progressively more pipe/less parentheses:\nx &lt;- 7\nexp(((x + 1)/2)^2)\n\n[1] 8886111\n\n(((x + 1)/2)^2) %&gt;% exp()\n\n[1] 8886111\n\n((x + 1)/2) %&gt;% raise_to_power(2) %&gt;% exp()\n\n[1] 8886111\n\n(x + 1) %&gt;% divide_by(2) %&gt;% raise_to_power(2)  %&gt;% exp()\n\n[1] 8886111\n\nx %&gt;% add(1) %&gt;% divide_by(2) %&gt;% raise_to_power(2) %&gt;% exp()\n\n[1] 8886111\n\n\nAs mentioned above, by default, the output from the left is piped into the first open argument position of the function on the right. What if you wanted to pipe an output into a different argument of the right-hand function? You could try to pipe a data frame\n\ndf &lt;- data.frame(x = 1:10, y = rnorm(10))\n\ninto qplot(), but it will fail:\n# Running this will produce an error\ndf %&gt;% qplot(x, y)\nThis fails because qplot() doesn’t understand that we’re trying to send df into the data argument of qplot(). We can fix that by specifying the correct argument explicitly with the . placeholder:\n\ndf %&gt;% qplot(x, y, data = .)\n\n\n\n\nOf course, in this example, df already had a name, so we could have used that directly in the data argument. In the context of a data-processing pipeline, df could have been the unnamed output of a chain of previous piped operations, and the . would be a convenient handle.\nThis concludes Lab Zero. Don’t forget to work through Lab 1 as well this week."
  },
  {
    "objectID": "9_Imp_Pred_Validate.html",
    "href": "9_Imp_Pred_Validate.html",
    "title": "9 Imp, Pred, Validate",
    "section": "",
    "text": "Learning Objectives\nAfter successful completion of this module, you will be able to:\n\nPerform a complete analysis of a dataset involving categorical or count responses.\nSpecifically articulate the question(s) of interest that you will explore using the data you chose for your final project.\n\nTask list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview/work through Birdkeeping.Rmd\nReview/work through Badgers.Rmd with accompanying data Badgers.csv\nReview/work through Respiratory.Rmd with accompanying data respiratory.csv\nRead through the Final Project.pdf assignment and complete the tasks for Week 9."
  },
  {
    "objectID": "7_Random_Effects.html",
    "href": "7_Random_Effects.html",
    "title": "7 Random_Effects",
    "section": "",
    "text": "Learning Objectives\nAfter successful completion of this module, you will be able to:\n\nWrite out the expression of a generalized linear mixed model.\nDescribe the situation in which adding a random effect is important\nDescribe the correct method for interpreting fixed effect coefficient estimates from GLMM and be able to give an example.\n\nIn R:\n\nFit GLMM in R, and perform model selection for both the random and fixed parts.\nUse some examples to understand the issue involved with interpreting fixed effect estimates from GLMM.\nPrepare and submit an R markdown script.\n\nTask list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 7 Readings and Lectures\nSubmit Module 7 Homework\nSubmit Module 7 Lab\nTake the Module 7 Content Quiz and R Quiz\nParticipate in the Module 7 Discussion"
  },
  {
    "objectID": "5_Log_linear_Regression.html",
    "href": "5_Log_linear_Regression.html",
    "title": "5 Log-lin-Reg",
    "section": "",
    "text": "After successful completion of this module, you will be able to:\n\nWrite out the expression of a Poisson regression model, including the response distribution and the model equation.\nInterpret the results of a Poisson regression.\nIdentify potential problems with the fit of a Poisson regression model.\nDescribe models for over dispersion of Poisson counts in the regression setting.\nEvaluate and compare models in the Poisson regression setting.\nDescribe how log linear models can be used for general contingency table data.\n\nIn R:\n\nFit Poisson, quasi-Poisson and negative binomial regression models, and make comparisons among them.\nFit log linear models to general contingency table data.\nUse simulations to understand problems that can arise when the sample size is small and/or the counts are small.\nPrepare and submit an R markdown script.\n\nTask list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 5 Readings and Lectures\nSubmit Module 5 Homework\nSubmit Module 5 Lab\nTake the Module 5 Content Quiz and R Quiz\nParticipate in the Module 5 Discussion"
  },
  {
    "objectID": "5_Log_linear_Regression.html#learning-objectives",
    "href": "5_Log_linear_Regression.html#learning-objectives",
    "title": "5 Log-lin-Reg",
    "section": "",
    "text": "After successful completion of this module, you will be able to:\n\nWrite out the expression of a Poisson regression model, including the response distribution and the model equation.\nInterpret the results of a Poisson regression.\nIdentify potential problems with the fit of a Poisson regression model.\nDescribe models for over dispersion of Poisson counts in the regression setting.\nEvaluate and compare models in the Poisson regression setting.\nDescribe how log linear models can be used for general contingency table data.\n\nIn R:\n\nFit Poisson, quasi-Poisson and negative binomial regression models, and make comparisons among them.\nFit log linear models to general contingency table data.\nUse simulations to understand problems that can arise when the sample size is small and/or the counts are small.\nPrepare and submit an R markdown script.\n\nTask list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 5 Readings and Lectures\nSubmit Module 5 Homework\nSubmit Module 5 Lab\nTake the Module 5 Content Quiz and R Quiz\nParticipate in the Module 5 Discussion"
  },
  {
    "objectID": "3_Logistic_Regression_I.html",
    "href": "3_Logistic_Regression_I.html",
    "title": "3 Log Reg I",
    "section": "",
    "text": "After successful completion of this module, you will be able to:\n\nDescribe the logistic regression model, which includes a probability model for the responses and the model equation.\nWrite a few sentences in non-technical language that explain the results of a logistic regression model fit.\nDescribe how to compare two logistic regression models.\nExplain the need for the link function in logistic regression.\n\nIn R:\n\nFit logistic regression models and interpret the output.\nCreate exploratory plots for logistic regression, and be able to explain any limitations of these plots.\nPrepare and submit an R markdown script."
  },
  {
    "objectID": "3_Logistic_Regression_I.html#learning-objectives",
    "href": "3_Logistic_Regression_I.html#learning-objectives",
    "title": "3 Log Reg I",
    "section": "",
    "text": "After successful completion of this module, you will be able to:\n\nDescribe the logistic regression model, which includes a probability model for the responses and the model equation.\nWrite a few sentences in non-technical language that explain the results of a logistic regression model fit.\nDescribe how to compare two logistic regression models.\nExplain the need for the link function in logistic regression.\n\nIn R:\n\nFit logistic regression models and interpret the output.\nCreate exploratory plots for logistic regression, and be able to explain any limitations of these plots.\nPrepare and submit an R markdown script."
  },
  {
    "objectID": "3_Logistic_Regression_I.html#task-list",
    "href": "3_Logistic_Regression_I.html#task-list",
    "title": "3 Log Reg I",
    "section": "Task list",
    "text": "Task list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 3 Readings and Lectures\nSubmit Module 3 Homework\nSubmit Module 3 Lab\nTake the Module 3 Content Quiz and R Quiz\nParticipate in the Module 3 Discussion"
  },
  {
    "objectID": "1_Types_of_Categoricals.html",
    "href": "1_Types_of_Categoricals.html",
    "title": "1 Types of Categoricals",
    "section": "",
    "text": "At the end of this week you should be able to:\n\nDescribe the different types of categorical data that you can encounter.\nDescribe the Bernoulli probability distribution.\nDescribe the mathematical representation of categorical data with more than two categories.\nIdentify a key feature of the variances of categorical and count random variables.\nExplain the perils of aggregation and give an example.\n\nIn R:\n\nIn Lab Zero: Familiarize yourself with the new lab format, R markdown.\nIn Lab Zero: Familiarize yourself with some new R packages and functions (e.g., dplyr, the ggplot function in the ggplot2 package).\nIn Lab One: Be able to create different tabulations of categorical data, either for summary purposes or for more efficient data analysis.\nIn Lab One: Prepare and submit your first R markdown script.\n\n\n\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 1 Readings and Lectures\nSubmit Module 1 Homework\nSubmit Module 1 Labs Zero and One\nTake the Module 1 Content Quiz and R Quiz\nParticipate in the Module 1 Discussion"
  },
  {
    "objectID": "1_Types_of_Categoricals.html#learning-objectives",
    "href": "1_Types_of_Categoricals.html#learning-objectives",
    "title": "1 Types of Categoricals",
    "section": "",
    "text": "At the end of this week you should be able to:\n\nDescribe the different types of categorical data that you can encounter.\nDescribe the Bernoulli probability distribution.\nDescribe the mathematical representation of categorical data with more than two categories.\nIdentify a key feature of the variances of categorical and count random variables.\nExplain the perils of aggregation and give an example.\n\nIn R:\n\nIn Lab Zero: Familiarize yourself with the new lab format, R markdown.\nIn Lab Zero: Familiarize yourself with some new R packages and functions (e.g., dplyr, the ggplot function in the ggplot2 package).\nIn Lab One: Be able to create different tabulations of categorical data, either for summary purposes or for more efficient data analysis.\nIn Lab One: Prepare and submit your first R markdown script.\n\n\n\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 1 Readings and Lectures\nSubmit Module 1 Homework\nSubmit Module 1 Labs Zero and One\nTake the Module 1 Content Quiz and R Quiz\nParticipate in the Module 1 Discussion"
  },
  {
    "objectID": "10_Final_Project.html",
    "href": "10_Final_Project.html",
    "title": "10 Final Project",
    "section": "",
    "text": "Data Analysis Writing\nFinal Project.pdf\nThis isn’t publishing?"
  },
  {
    "objectID": "2_Props_Risks_Odds.html",
    "href": "2_Props_Risks_Odds.html",
    "title": "2 Props, Risk & Odds",
    "section": "",
    "text": "At the end of this week you should be able to:\n\nIdentify potential issues with small sample sizes in tables of counts.\nDescribe the situations in which a comparison of odds ratios is more appropriate than a comparison of proportions.\nIndicate the one-to-one relationship between proportions and odds.\nDescribe the multinomial and Poisson distributions and the connection between them.\n\nIn R:\n\nPerform tests for a difference in proportions and an odds ratio.\nUse simulations to understand the performance of statistical tests for count data when the sample sizes are small.\nPrepare and submit an R markdown script."
  },
  {
    "objectID": "2_Props_Risks_Odds.html#learning-objectives",
    "href": "2_Props_Risks_Odds.html#learning-objectives",
    "title": "2 Props, Risk & Odds",
    "section": "",
    "text": "At the end of this week you should be able to:\n\nIdentify potential issues with small sample sizes in tables of counts.\nDescribe the situations in which a comparison of odds ratios is more appropriate than a comparison of proportions.\nIndicate the one-to-one relationship between proportions and odds.\nDescribe the multinomial and Poisson distributions and the connection between them.\n\nIn R:\n\nPerform tests for a difference in proportions and an odds ratio.\nUse simulations to understand the performance of statistical tests for count data when the sample sizes are small.\nPrepare and submit an R markdown script."
  },
  {
    "objectID": "2_Props_Risks_Odds.html#task-list",
    "href": "2_Props_Risks_Odds.html#task-list",
    "title": "2 Props, Risk & Odds",
    "section": "Task list",
    "text": "Task list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 2 Readings and Lectures\nSubmit Module 2 Homework\nSubmit Module 2 Lab\nTake the Module 2 Content Quiz and R Quiz\nParticipate in the Module 2 Discussion"
  },
  {
    "objectID": "4_Logistic_Regression_II.html",
    "href": "4_Logistic_Regression_II.html",
    "title": "4 Log-Reg II",
    "section": "",
    "text": "After successful completion of this module, you will be able to:\n\nDescribe how to examine the goodness of fit of a logistic regression model.\nExplain what an empirical logit is, why it’s useful and how to calculate it.\nDescribe how to compare two logistic regression models.\nDefine over dispersion (or extra-binomial variation), and indicate why it can cause problems for inference in logistic regression.\nList some reasons that counts may exhibit extra-binomial variation.\n\nIn R:\n\nEstimate and interpret the dispersion parameter.\nCreate exploratory plots for logistic regression using the empirical logit.\nUse the deviance residuals to examine goodness of fit of the logistic regression model.\nPrepare and submit an R markdown script.\n\nTask list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 4 Readings and Lectures\nSubmit Module 4 Homework\nSubmit Module 4 Lab\nTake the Module 4 Content Quiz and R Quiz\nParticipate in the Module 4 Discussion"
  },
  {
    "objectID": "4_Logistic_Regression_II.html#learning-objectives",
    "href": "4_Logistic_Regression_II.html#learning-objectives",
    "title": "4 Log-Reg II",
    "section": "",
    "text": "After successful completion of this module, you will be able to:\n\nDescribe how to examine the goodness of fit of a logistic regression model.\nExplain what an empirical logit is, why it’s useful and how to calculate it.\nDescribe how to compare two logistic regression models.\nDefine over dispersion (or extra-binomial variation), and indicate why it can cause problems for inference in logistic regression.\nList some reasons that counts may exhibit extra-binomial variation.\n\nIn R:\n\nEstimate and interpret the dispersion parameter.\nCreate exploratory plots for logistic regression using the empirical logit.\nUse the deviance residuals to examine goodness of fit of the logistic regression model.\nPrepare and submit an R markdown script.\n\nTask list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 4 Readings and Lectures\nSubmit Module 4 Homework\nSubmit Module 4 Lab\nTake the Module 4 Content Quiz and R Quiz\nParticipate in the Module 4 Discussion"
  },
  {
    "objectID": "6_Dispersion_Zero_Inflation.html",
    "href": "6_Dispersion_Zero_Inflation.html",
    "title": "6 Disp & 0 Inflation",
    "section": "",
    "text": "Learning Objectives\nAfter successful completion of this module, you will be able to:\n\nWrite out the expression of a zero-inflated Poisson regression model, including the response distribution and the model equations.\nDescribe the distinctions between over dispersion and zero-inflation.\nDescribe the distinction between “true zeroes” and “excess zeroes” and provide some examples.\nDescribe the key difference between zero-inflated models and hurdle models.\n\nIn R:\n\nFit ZIP and ZINB regression models, and Poisson and negative binomial hurdle regression models, and make comparisons among these models.\nUse simulations to understand what zero-inflation and over dispersion look like in certain situations.\nPrepare and submit an R markdown script.\n\nTask list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 6 Readings and Lectures\nSubmit Module 6 Homework\nSubmit Module 6 Lab\nTake the Module 6 Content Quiz and R Quiz\nParticipate in the Module 6 Discussion"
  },
  {
    "objectID": "8_Evaluating_Models.html",
    "href": "8_Evaluating_Models.html",
    "title": "8 Model Eval",
    "section": "",
    "text": "Learning Objectives\nAt the end of this week you should be able to:\n\nDiscuss issues having to with making predictions of binary outcomes\nDescribe methods for prediction of categorical and count data\nDescribe the statistical concerns to be aware of in the big data setting\n\nIn R:\n\nMake predictions from generalized linear models\nEvaluate different models using cross validation methods\nPrepare and submit an R markdown script.\n\nTask list\nIn order to achieve these learning outcomes, please make sure to complete the following:\n\nReview Module 8 Readings and Lectures\nSubmit Module 8 Homework\nSubmit Module 8 Lab\nTake the Module 8 Content Quiz and R Quiz\nParticipate in the Module 8 Discussion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reference",
    "section": "",
    "text": "Gradescope\nLectures"
  },
  {
    "objectID": "index.html#git-stuff",
    "href": "index.html#git-stuff",
    "title": "Reference",
    "section": "Git stuff",
    "text": "Git stuff\nAdd, commit, and publish the folder to the github repo\n\ngit add .\ngit commit -m “comment”\ngit push origin main\n\nSet the location of the github repo\n\ngit remote set-url origin url\ngit remote -v\ngit remote show origin\n\n\nHere are some quarto commands:\n\nquarto publish gh-pages\nquarto publish gh-pages document.qmd\nquarto publish gh-pages –no-prompt"
  },
  {
    "objectID": "index.html#reading-list",
    "href": "index.html#reading-list",
    "title": "Reference",
    "section": "Reading list",
    "text": "Reading list\nCat Data Anal 3rd\nStat. Meth 3rd\nOpen Stats 4th ed\nR4DS 1st\n\nExtra Reading\nMathmatical Stats 8th\nPractical Stats for DS\n\n\nModule 1\n\nCategorical Data Analysis, Sections 1.1, 1.2\nStatistical Methods, Section 12.1\nOpenIntro Statistics, Section 6.1\nR for Data Science, Look over Chapters 1, 2, and 3\n\n\n\nModule 2\n\nCategorical Data Analysis, Sections 1.2, 1.3.\nStatistical Methods, Section 12.2-12.4.\nOpenIntro Statistics, Section 6.2-6.4\nR for Data Science (Lab references Chapter 22)\n\n\n\nModule 3\n\nCategorical Data Analysis, Sections 4.1, 4.2 (through 4.2.5); 5.1-5.3 (through 5.3.6). Please note: content not in lectures.\nStatistical Methods, Sections 13.1-13.2.\nOpenIntro Statistics, Section 8.4. and Section 9.5\n\n\n\nModule 4\n\nCategorical Data Analysis, Sections 4.7, 6.2\n\n\n\nModule 5\n\nCategorical Data Analysis, Sections 4.3, 9.1 (through 9.1.3; some of the notation here may be difficult, but it is equivalent to using indicator functions to denote different categories).\n\n\n\nModule 6\n\nAn Application of Claim Frequency Data using Zero Inflated and Hurdle Models in General Insurance\n\n\n\nModule 7\n\nCategorical Data Analysis, Chapter 13, through Section 13.1.4; Section 13.3. Please note: models are given in matrix notation.\n\n\n\nModule 8\n\nCategorical Data Analysis, Chapter 6, 6.3.1, 6.3.3 and 6.3.4\nOpenIntro Statistics, Section 8.4"
  },
  {
    "objectID": "index.html#website",
    "href": "index.html#website",
    "title": "Reference",
    "section": "Website",
    "text": "Website\nThis is from the github repo creation page\nPush an existing repository from the command line:\n\ngit remote add origin url\ngit branch -M main\ngit push -u origin main\n\n\nSetting up the website:\n\nCreate a new git repository\nCreate a quarto project website\nRender the project\nAdd _site & .quarto to the .gitignore file\ngit remote add origin\ngit branch -M main\ngit add .\ngit commit\ngit push\nrefresh github.com/name\nview all branches -&gt; New -&gt; “gh-pages”\nsettings -&gt; pages -&gt; deploy from gh-pages\nquarto publish gh-pages\n\nIt should work ever after by just typing quarto publish gh-pages. There may be a need to get a personal access token at the start. I did that my first go around so I’m not sure. Sometimes the site doesn’t render immediately. You have to wait for it to finish in the actions tab of github."
  },
  {
    "objectID": "z1_learnR.html",
    "href": "z1_learnR.html",
    "title": "1 Types of Categoricals",
    "section": "",
    "text": "LearnR 1"
  },
  {
    "objectID": "z1_learnR.html#case-format",
    "href": "z1_learnR.html#case-format",
    "title": "1 Types of Categoricals",
    "section": "Case Format",
    "text": "Case Format\nThis familiar format, applicable to both continuous and categorical data, is what R for Data Science calls “tidy data” (see vignette('tidy-data') for much more on this idea). The data are arranged in an n x p rectangular array (e.g. a data frame), where n is the number of observations and p the number of variables. That is, there is one observation per row and one variable per column. This is a good starting point for most analyses.\n\nX &lt;- data.frame(\n  V1 = c('Up', 'Up', 'Down', 'Down', 'Up', 'Up'), \n  V2 = c('Left', 'Left', 'Right', 'Right', 'Left', 'Right'),\n  V3 = c('B','B','A','B', 'A', 'B'))\nX\n\n\n\n\n\nV1\nV2\nV3\n\n\n\n\nUp\nLeft\nB\n\n\nUp\nLeft\nB\n\n\nDown\nRight\nA\n\n\nDown\nRight\nB\n\n\nUp\nLeft\nA\n\n\nUp\nRight\nB\n\n\n\n\n\n\n\nHow many variables are in X? How many observations?\n\nThe case or “tidy” format is fully general – it’s applicable to data sets consisting of (complete) observations on any combination of continuous and/or categorical variables. In contrast, the next two formats are specific to data sets where all the variables are categorical (or where any continuous variables can be ignored or discretized)."
  },
  {
    "objectID": "z1_learnR.html#frequency-format",
    "href": "z1_learnR.html#frequency-format",
    "title": "1 Types of Categoricals",
    "section": "Frequency Format",
    "text": "Frequency Format\nIn this format, the data are still arranged in a 2-D rectangular array, but with the number of rows equal to the number of combinations of factor levels, and the number of columns is p + 1. The extra column (typically the last) contains the counts, or frequencies, observed at each of the possible combinations of factor levels.\n\nX %&gt;% table %&gt;% as.data.frame # don't worry about this code for now\n\n\n\n\n\nV1\nV2\nV3\nFreq\n\n\n\n\nDown\nLeft\nA\n0\n\n\nUp\nLeft\nA\n1\n\n\nDown\nRight\nA\n1\n\n\nUp\nRight\nA\n0\n\n\nDown\nLeft\nB\n0\n\n\nUp\nLeft\nB\n2\n\n\nDown\nRight\nB\n1\n\n\nUp\nRight\nB\n1\n\n\n\n\n\n\nNotice that I used the pipe operator in this last R chunk to first perform the table() function on X and then perform the as.data.frame() function on that. The equivalent, nested functions are shown in the table below. Please refer back to M1Lab0.Rmd to learn more about the pipe operator.\n\nCan you see why there are 8 rows in this display? Think about the number of levels for each of the three categorical variables."
  },
  {
    "objectID": "z1_learnR.html#tabular-format",
    "href": "z1_learnR.html#tabular-format",
    "title": "1 Types of Categoricals",
    "section": "Tabular Format",
    "text": "Tabular Format\nIn this format, the data are arranged in a p-dimensional array of frequencies, where p is the number of variables. The dimensions of the array are the number of factor levels for each variable. The elements in the array are the observed frequencies at each of the possible combinations of factor levels.\n\nX_tab &lt;- table(X)\n\nObserve that since there are a 3 variables in X, the table object is a 3-dimensional array.\n\n(dims &lt;- dim(X_tab))  # Dimensions of the 3-dimensional array that contains the frequencies of the factor-level combinations of the 3 variables in `X`.\n\n[1] 2 2 2\n\n# This is why there are 8 rows in the frequency table \nprod(dims)\n\n[1] 8\n\n# 2-D slices of 3-D table\nX_tab\n\n, , V3 = A\n\n      V2\nV1     Left Right\n  Down    0     1\n  Up      1     0\n\n, , V3 = B\n\n      V2\nV1     Left Right\n  Down    0     1\n  Up      2     1\n\n\nSince this table is a 3-D object, trying to print it to the 2-D screen disassembles it. You’ll see much more on this issue in later sections. The R terminology makes no distinction between the multidimensional table and its 2-D representation – both are “tables”. When confusion may arise between these concepts later on, I will generally refer to the higher-dimensional object as a (tabular) array and a 2-D view of it as a (tabular) display."
  },
  {
    "objectID": "z1_learnR.html#frequency-format-1",
    "href": "z1_learnR.html#frequency-format-1",
    "title": "1 Types of Categoricals",
    "section": "Frequency Format",
    "text": "Frequency Format\nLet’s see if we can first get these data into frequency format. This means figuring out what the variables are, and putting each variable into its own column. The key thing to notice is that the actual unit of observation is the post-conviction sentencing decision for a single murder trial, and the outcome of sentencing from a single conviction can be either Death or NoDeath. That is, as we mentioned above, Death and NoDeath are not separate, logically-independent variables, when measured at the level of individual murder trials, but are two levels of the same (as yet unnamed) variable.\ntidyr to the rescue:\n\nYou use pivot_longer() when you notice that you have columns that are not variables, but rather levels of a single variable.\n\nWe’ll use pivot_longer() to collapse the Death and NoDeath columns into a new column named Sentence.\n\npenalty_freq &lt;- pivot_longer(data = penalty, cols = c(\"Death\", \"NoDeath\"), names_to = \"Sentence\", values_to = \"Freq\", cols_vary = \"slowest\")\n\n# cols_vary = \"slowest\" keeps individual columns from cols close together in the output. Here, this keeps the data ordered by Aggravation.\n\nObserve how the data in each row of penalty takes up two rows in penalty_freq (namely, rows 1 and 13).\n\npenalty[1,]\n\n\n\n\n\nAggravation\nVictim\nDeath\nNoDeath\n\n\n\n\n1\nWhite\n2\n60\n\n\n\n\n\n\n\npenalty_freq[c(1,13),]\n\n\n\n\n\nAggravation\nVictim\nSentence\nFreq\n\n\n\n\n1\nWhite\nDeath\n2\n\n\n1\nWhite\nNoDeath\n60\n\n\n\n\n\n\nYou can View() penalty_freq to check that it looks like it should."
  },
  {
    "objectID": "z1_learnR.html#case-format-1",
    "href": "z1_learnR.html#case-format-1",
    "title": "1 Types of Categoricals",
    "section": "Case Format",
    "text": "Case Format\nNow suppose we want to see all the individual observations – each trial on a separate row. Referring to the table of conversion functions, we can use expand.dft() to get from a frequency format to a case format.\n\n# the other functions in that table are available in base R, but expand.dft() is from vcdExtra.\npenalty_case &lt;- expand.dft(penalty_freq)\nhead(penalty_case)\n\n\n\n\n\nAggravation\nVictim\nSentence\n\n\n\n\n1\nWhite\nDeath\n\n\n1\nWhite\nDeath\n\n\n1\nBlack\nDeath\n\n\n2\nWhite\nDeath\n\n\n2\nWhite\nDeath\n\n\n2\nBlack\nDeath\n\n\n\n\n\n\n# Add {r} after backticks to make executable, or just type this command in the console.\nView(penalty_case)\nWhen expanded to case format, there are 362 rows (observations) and 3 columns (variables). Note that from case format, we could encode the categorical variables using binary indicators. Each categorical variable takes two possible values, so we only need one binary indicator for each.\n\npenalty_indicators&lt;- mutate(penalty_case, .keep = \"none\",\n                               Aggravation = Aggravation,\n                               White = ifelse(Victim == \"White\", 1, 0), \n                               Death = ifelse(Sentence == \"Death\", 1, 0))\n# another binary encoding: model.matrix( ~ Sentence + Victim + Aggravation, data = penalty_case)\nhead(penalty_indicators)\n\n\n\n\n\nAggravation\nWhite\nDeath\n\n\n\n\n1\n1\n1\n\n\n1\n1\n1\n\n\n1\n0\n1\n\n\n2\n1\n1\n\n\n2\n1\n1\n\n\n2\n0\n1\n\n\n\n\n\n\nTypically we will leave the indicator variables behind the scenes, where they will be created as needed by modeling functions. If you want to see what this looks like, you can produce an explicit indicator-encoding for a whole data frame at once, without writing out a page of if-else statements, using the model.matrix() function."
  },
  {
    "objectID": "z1_learnR.html#tabular-format-1",
    "href": "z1_learnR.html#tabular-format-1",
    "title": "1 Types of Categoricals",
    "section": "Tabular Format",
    "text": "Tabular Format\nStarting from either the frequency or case format, we can create a variety of tabular structures using table() or xtabs(). I find the xtabs() interface more convenient. With a minor change in the formula specification, xtabs() can tabulate either case form or frequency form data.\n\nTo tabulate frequency-format data, write the name of the frequency column on the left-hand side of the formula, and the other variables on the right-hand side. For case-format data, leave the left-hand side of the formula empty.\n\n\n# The following are equivalent:\npenalty_tab &lt;- xtabs(~ Sentence + Victim + Aggravation, penalty_case)\n# penalty_tab &lt;- xtabs(Freq ~ Sentence + Victim + Aggravation, penalty_freq)\n\nUnderstanding the tabular form is very important when dealing with categorical data. The next section treats tabular data with more depth."
  },
  {
    "objectID": "z1_learnR.html#conditioning",
    "href": "z1_learnR.html#conditioning",
    "title": "1 Types of Categoricals",
    "section": "Conditioning",
    "text": "Conditioning\nIn general, to slice an n-dimensional array into m-dimensional pieces, we must fix – “condition on” – each of the possible values of the other n - m variables. For penalty_tab, we can get 2-D slices by conditoning on just one other variable. Let’s try this with Sentence as the conditioning variable – the relevant function is co_table() (“condition table”). Based on the table dimensions we just saw, this should create two 2-D (2x6) slices.\n\n(penalty_given_sentence &lt;- co_table(penalty_tab,'Sentence'))\n\n$Death\n       Aggravation\nVictim   1  2  3  4  5  6\n  Black  1  1  2  2  4  4\n  White  2  2  6  9  9 17\n\n$NoDeath\n       Aggravation\nVictim    1   2   3   4   5   6\n  Black 181  21   9   4   3   0\n  White  60  15   7   3   0   0\n\n\nConditioning preserves all of the information in the array – we can display the whole tabular array by laying out each slice end to end. The slices are named by the levels of the conditioning variable, and naming a particular level returns that slice:\n\npenalty_given_sentence$Death\n\n       Aggravation\nVictim   1  2  3  4  5  6\n  Black  1  1  2  2  4  4\n  White  2  2  6  9  9 17\n\n\nOf course, if you condition on two out of three variables at once (using a vector of conditioning variables) you’ll get 1-D slices, and conditioning on all three variables cuts up the table into all of its individual frequency components. In higher-dimensional tables this will be different – for instance, if we started with a 6-D table, conditioning on three variables would return 3-D slices. You are welcome to play around with using the death penalty data and see what weirdness you can produce!\nWe can arrange 2D array slices into a single tabular display by using ftable() (“flat table”) or structable(). Both of these functions can take an existing table object as input, or create new tables directly from data using a formula interface.\nHere’s an example with pre-tabulated data from above:\n\npenalty_tab %&gt;% aperm(c(3, 1, 2)) %&gt;% ftable\n\n                     Victim Black White\nAggravation Sentence                   \n1           Death               1     2\n            NoDeath           181    60\n2           Death               1     2\n            NoDeath            21    15\n3           Death               2     6\n            NoDeath             9     7\n4           Death               2     9\n            NoDeath             4     3\n5           Death               4     9\n            NoDeath             3     0\n6           Death               4    17\n            NoDeath             0     0\n\n\nAnd here’s an example of the formula interface for structable() and ftable(), which is somewhat different from the xtabs() formula interface (because it’s intended to produce a different kind of object), but still quite intuitive.\nThe following tables are fairly wide, so to view them in the R Markdown file, you’ll have to expand the width of the source pane (or, you could click Preview and see the table in the Viewer pane, which you may also have to widen!).\n\nftable(Aggravation + Victim ~ Sentence, penalty_case)\n\n         Aggravation     1           2           3           4           5           6      \n         Victim      Black White Black White Black White Black White Black White Black White\nSentence                                                                                    \nDeath                    1     2     1     2     2     6     2     9     4     9     4    17\nNoDeath                181    60    21    15     9     7     4     3     3     0     0     0\n\n\n\nstructable(Sentence + Aggravation ~ Victim, penalty_case)\n\n       Sentence    Death                     NoDeath                    \n       Aggravation     1   2   3   4   5   6       1   2   3   4   5   6\nVictim                                                                  \nBlack                  1   1   2   2   4   4     181  21   9   4   3   0\nWhite                  2   2   6   9   9  17      60  15   7   3   0   0\n\n\nYou can see that the variables on right hand side of the formula control the recursive column splits, and the variables on the left hand side control the recursive row splits (variable order matters on both sides).\nAnalogous to the way that the table orientation defined by the order of an xtabs() formula can be “rearranged” with aperm(), a structable()’s split orientation can be “rearranged” using the direction argument:\n\npenalty_tab %&gt;% aperm(c(2, 1, 3)) %&gt;% structable(direction = c(\"v\", \"v\", \"h\"))\n\n            Victim   Black         White        \n            Sentence Death NoDeath Death NoDeath\nAggravation                                     \n1                        1     181     2      60\n2                        1      21     2      15\n3                        2       9     6       7\n4                        2       4     9       3\n5                        4       3     9       0\n6                        4       0    17       0\n\n\n\nTry playing with the order of dimensions and/or directions of table splits in the structable() call until you get a flat display arranged like the original case1902 data frame."
  },
  {
    "objectID": "z1_learnR.html#marginalizing",
    "href": "z1_learnR.html#marginalizing",
    "title": "1 Types of Categoricals",
    "section": "Marginalizing",
    "text": "Marginalizing\nIn addition to slicing the tabular array, we can also squash it by summing across one or more dimension(s). We can do this simply by omitting from the xtabs() formula specification the variables we wish to collapse along.\n\nxtabs(~ Victim + Sentence, penalty_case) # collapse along Aggravation axis\n\n       Sentence\nVictim  Death NoDeath\n  Black    14     218\n  White    45      85\n\n\n\nxtabs(~ Sentence, penalty_case) # collapse along Aggravation and Victim axes\n\nSentence\n  Death NoDeath \n     59     303 \n\n\nAlternatively, starting with a complete array, we can collapse it using margin.table(). As with the formula specification above, omitted variables get collapsed.\n\nmargin.table(penalty_tab, margin = c(1,2)) \n\n         Victim\nSentence  Black White\n  Death      14    45\n  NoDeath   218    85\n\n# penalty_tab was defined with the formula ~ Sentence + Victim + Aggravation.  Sentence and Victim, as the first two variables in the formula, will be retained, while Aggravation will get squashed. \n\n\n# Aggravation is the 3rd variable in the formula, so here it will be retained while `Victim` and `Sentence` get squashed.\nmargin.table(penalty_tab, margin = c(3))\n\nAggravation\n  1   2   3   4   5   6 \n244  39  24  18  16  21 \n\n\nmargin.table(penalty_tab) with no other arguments predictably returns the table total – all variables are omitted, so all dimensions are collapsed.\nWe’ve been talking about “collapsing” tables, and somewhat confusingly there is a function named collapse.table(), that behaves slightly differently. Instead of squashing omitted dimensions flat, it shortens a specified dimension by reducing the number of factor levels along that axis. Say we want a table where Aggravation levels 1-3 are grouped together as “Low” and levels 4-6 “High”. Without editing the original data, we could use collapse.table():\n\npenalty_tab %&gt;% collapse.table('Aggravation' = c(rep(\"Low\", 3), c(rep(\"High\", 3))))\n\n, , Aggravation = Low\n\n         Victim\nSentence  Black White\n  Death       4    10\n  NoDeath   211    82\n\n, , Aggravation = High\n\n         Victim\nSentence  Black White\n  Death      10    35\n  NoDeath     7     3"
  },
  {
    "objectID": "z1_learnR.html#tables-of-proportions",
    "href": "z1_learnR.html#tables-of-proportions",
    "title": "1 Types of Categoricals",
    "section": "Tables of Proportions",
    "text": "Tables of Proportions\nFrom a table of counts, we often wish to make a table of proportions or percentages, where the proportions may be taken with respect to rows, columns, tables, etc.\n\n# Table of counts\n(sentence_victim_tab &lt;- margin.table(penalty_tab, margin = c(1,2)))\n\n         Victim\nSentence  Black White\n  Death      14    45\n  NoDeath   218    85\n\n# Table of proportions\nprop.table(sentence_victim_tab) %&gt;% round(2) # rounding all proportions to 2 decimal points to avoid visual clutter\n\n         Victim\nSentence  Black White\n  Death    0.04  0.12\n  NoDeath  0.60  0.23\n\n\nThe proportions above are taken with respect to the whole table. We’re actually more interested in the proportions of Death Sentences within each Victim level:\n\n# Table of column proportions\ncol_prop_tab &lt;- prop.table(sentence_victim_tab, margin = 2)\ncol_prop_tab %&gt;% round(2)\n\n         Victim\nSentence  Black White\n  Death    0.06  0.35\n  NoDeath  0.94  0.65\n\n# Table of column percents\n100 * col_prop_tab %&gt;% round(3)\n\n         Victim\nSentence  Black White\n  Death     6.0  34.6\n  NoDeath  94.0  65.4"
  },
  {
    "objectID": "z1_learnR.html#r-markdown-for-submission",
    "href": "z1_learnR.html#r-markdown-for-submission",
    "title": "1 Types of Categoricals",
    "section": "R Markdown for submission",
    "text": "R Markdown for submission\nThe following describes the process of creating and submitting .Rmd and .pdf files as your lab assignment. You will follow this procedure after each lab in this course.\nNOTE: If you are reading these instructions from the HTML preview in the Viewer pane, they will be overwritten in the following steps. You can avoid this by “popping out” the current HTML preview into an external browser window, by clicking the the “Show in new window” icon (to the right of the broom icon in the Viewer pane). \nAlternatively, you can read the instructions right from the .Rmd script, keeping in mind that you will need to switch back to the lab script tab to view the rest of the instructions once you create a new script. You could also copy and paste the whole Lab 1 Assignment Instructions section into your new document while you’re working on it, and then delete the instructions before you submit it.\nClick “File” -&gt; “New File” -&gt; “R Markdown”, and dialog box will pop up. Change the title to “Lab Assignment 1” and name yourself as author. Select PDF as the Default Output Format, then click OK. The header of your new file should look something like this:\n---\ntitle: \"Lab Assignment 1\"\nauthor: \"Ronald Fisher\"\ndate: \"2024-04-04\"\noutput: pdf_document\n---\n\nThe file will initially contain some examples to get you started with RMarkdown, which you should replace with your lab content. Save the notebook as something like “Lab_Assignment_1” using “File” –&gt; “Save As…”\nIn your new .Rmd script, answer the questions in the “Questions” section below. Include all the code you need to produce the requested outputs. Your script should include a top-level section heading for each question, for example:\n# Question 1\n\nstuff here\n\n# Question 2\n\nother stuff\nWhether or not you include the text of the questions in your script is up to you.\n\nDo be sure to include, near the top of your script, a code chunk that loads any non-default packages you use (such as vcdExtra or Sleuth3).\n\nWithin the question sections, you can chunk your code in whatever way seems reasonable. Incorporate any written answers outside the code chunks using Markdown formatting as needed (see “Help” -&gt; “RMarkdown Quick Reference” for text formatting help).\nTo ensure that your .Rmd script will be fully self-contained (i.e. it will not depend on objects that were defined during the lab, and could be run as-is if you sent it to someone else), you should clear the workspace before you begin.\n\nTo clear the workspace, click the broom icon in the Environment pane.\n\nOnce you’ve answered the questions in your new .Rmd script and you have verified your code is self contained, you should Run -&gt; Run All Chunks and generate a .pdf file of your document, to check that everything looks like you want it to. Having concluded that your .Rmd script produces a pdf document that includes all the output you want, submit both the Lab_Assignment_1.Rmd file and the pdf document on Canvas as your Lab Assignment 1.\nFeel free to post on the discussion board if you have any questions or encounter any difficulties with this process."
  },
  {
    "objectID": "z1_learnR.html#questions",
    "href": "z1_learnR.html#questions",
    "title": "1 Types of Categoricals",
    "section": "Questions:",
    "text": "Questions:\n\nQ1.\nStarting from the case1902 data file in the Sleuth3 package, produce a flat table With Aggravation level as the rows, with Death/NoDeath as the two wide columns that contain Black/White within them. That is, the skeleton of your table should look like (you should look at this in Preview):\n\n\n\n.\nDeath\n\nNoDeath\n\n\n\n\n\n.\nBlack\nWhite\nBlack\nWhite\n\n\n1\n.\n.\n.\n.\n\n\n2\n.\n.\n.\n.\n\n\n3\n.\n.\n.\n.\n\n\n4\n.\n.\n.\n.\n\n\n5\n.\n.\n.\n.\n\n\n6\n.\n.\n.\n.\n\n\n\n\n\nQ2.\nCreate a mosaic plot of the flat table you produced in question 1. Are there problems with your plot? Is it easy to understand? You are welcome to play around with making it more interpretable and/or visually appealing, but that’s optional for this assignment."
  },
  {
    "objectID": "z3_learnR.html",
    "href": "z3_learnR.html",
    "title": "LearnR 3",
    "section": "",
    "text": "In the previous lab, we introduced some simple statistical inference procedures for analyzing data in contingency tables. These procedures were “randomization-based,” in that they involved either directly constructing a randomization distribution of the data under a hypothesis of complete independence between two variables or approximating such a randomization distribution.\nIn most real data analysis situations, we will want to ask and answer more sophisticated questions than “are any variables related to any others.” We may have research questions like “how does the relationship between X and Y depend on the changing level of Z?” Or, “How do the odds of success change when we increase the value of a certain explanatory variable?” In these situations, we will need to move beyond randomization methods and begin using statistical models for categorical responses.\nAs you will see in this lab, modeling categorical responses with statistical methods designed for continuous ones can go sideways almost immediately, so we will use new approaches – such as generalized linear models, of which logistic regression is a special case.\nPlease run the following chunk of R code to load the packages we will use for this lab.\n\nlibrary(vcdExtra)\n\nWarning: package 'vcdExtra' was built under R version 4.3.3\n\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nWarning: package 'gnm' was built under R version 4.3.3\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(magrittr)"
  },
  {
    "objectID": "z3_learnR.html#exploration",
    "href": "z3_learnR.html#exploration",
    "title": "LearnR 3",
    "section": "Exploration",
    "text": "Exploration\nThe variable age is continuous, the response survived is binary, and so far, we don’t have any tools for that situation. When all you have is a chi-square test, everything looks like a contingency table, and if we just collapse age away, we can produce a familiar 2x2 table of counts:\n\n(tbl1 &lt;- xtabs(data = Donner, ~ sex + survived))\n\n        survived\nsex       0  1\n  Female 10 25\n  Male   32 23\n\n\nAnd see the proportions surviving in each sex\n\nprop.table(tbl1, margin = 1) %&gt;% round(2)\n\n        survived\nsex         0    1\n  Female 0.29 0.71\n  Male   0.58 0.42\n\n\nOf the original 55 males, 42% survived, and of the original 35 females, 71% survived,\n\nchisq.test(tbl1)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tbl1\nX-squared = 6.392, df = 1, p-value = 0.01146\n\n\nand the p-value for the difference in survival by sex is fairly small (but remember that a p-value doesn’t really make sense here without a population to point to).\nFrom the proportions, we can get the odds of survival for males and females, respectively:\n\n(male_odds &lt;- 0.42/0.58)\n\n[1] 0.7241379\n\n(female_odds &lt;- 0.71/0.29)\n\n[1] 2.448276\n\n\nAnd the odds ratio for survival by sex:\n\n(mf_odds_ratio &lt;- male_odds/female_odds)\n\n[1] 0.2957746\n\n\nIgnoring age, the odds of survival for males is 30% of the odds of survival for females.\nWe really don’t want to base any conclusions on this simple aggregation, however – we actually do want to use the information about the continuous variable age, and the relationship between sex and survival may be different at different age values. Let’s see how we might proceed treating age as continuous.\n\nLooking at the Data\nIf you looked at Module 1 Lab 0, you already saw a plot of the Donner Party data. Let’s look at a similar plot:\n\n  ggplot(data=Donner,aes(age,survived,color=sex,shape=sex)) + geom_point()\n\n\n\n\nAlso, remember that in Lab 0, we showed you these data with some jitter added so that the overlapping points are given a little separation. Here is the plot with jitter:\n\nggplot(data = Donner) + \n  # plot jittered data\n  geom_jitter(aes(x = age, \n                 y = survived,\n                 color = sex,\n                 shape = sex),\n              height = 0.05, width = 0.2)\n\n\n\n\nIt’s still not at all obvious what’s going on here. You can see from this jittered version of the plot that there do seem to be a higher proportion of females that survived than males (survived == 1 corresponds to survival), and maybe that there was a “sweet spot” in terms of the ages of the females that survived (i.e., between the ages of about 5 and 35, it looks like most of the females survived)."
  },
  {
    "objectID": "z3_learnR.html#the-logit-transformation",
    "href": "z3_learnR.html#the-logit-transformation",
    "title": "LearnR 3",
    "section": "The Logit Transformation",
    "text": "The Logit Transformation\nWe will use the logit (or “log-odds”) function to link the probabilities to the linear combination of predictors. The logit function looks like this:\n\\[\\mathrm{logit}(p) = \\log(p/(1-p))\\]\n\nlogit &lt;- function(p){log(p/(1-p))}\n\nThis is a smooth, invertible function from the \\([0,1]\\) interval to the whole real line, \\((-\\infty,\\infty)\\). Plug in a probability, get the (natural) log of the odds. You can see this transformation, as well as the probability to odds transformation, compared in the plot below. The probability to odds transformation takes us from \\([0,1]\\) to \\([0,\\infty)\\), and the log transformation of that takes us from \\([0,\\infty)\\) to \\((-\\infty,\\infty)\\).\nThe y-axis on the plot below ranges only from \\([-3, 3]\\), but remember that the odds function is actually defined all the way from 0 to \\(\\infty\\), and the logit ranges all the way from \\(-\\infty\\) to \\(\\infty\\). The identity transformation, \\(f(p) = p\\), is included for reference.\n\nprob &lt;- seq(0.05, 0.95, length = 100) # restricting the range of probabilities shown in order to keep the transformed values finite.   \nodds &lt;- prob/(1-prob)\nlogits &lt;- log(odds)\ntransformations &lt;- data.frame(identity = prob, odds, logits) %&gt;% gather() %&gt;% rename(transform_function = key)\nggplot(data = transformations, aes(x = rep(prob, 3), y = value, color = transform_function)) + \n  scale_y_continuous(limits = c(-3, 3), breaks = c(-3:3)) + \n  geom_line() + \n  geom_hline(yintercept = 0, lty = 2) + \n  geom_vline(xintercept = 0.5, lty = 2) + \n  xlab(\"original probabilities\") +\n  ylab(\"transformed values\") + \n  ggtitle(\"Possible transformations\n          from probability scale\")\n\n\n\n\nThe logit is not the only invertible transformation from the probability interval to the entire real line, but it has several desirable properties.\nFrom this plot, we can see that probabilities greater than 1/2 are mapped to positive logits and probabilities less than 1/2 are mapped to negative logits (remember \\(p = 1/2 =&gt; odds = 1 =&gt; \\log(odds) = 0\\)) so the sign of the logit of a probability has a straightforward interpretation – if the log-odds is positive, the event is more likely than not to occur.\nIn the context of our Donner problem, we can apply the logit function to the survival probability to get a new logistic regression model that might work better than the linear probability model we tried before. We’ll write the new model as:\n\\[\\mathrm{logit}(P(Survived|Sex, Age)) = \\beta_0 + \\beta_1Age + \\beta_2Sex + \\beta_3Age \\times Sex\\] That is, a non-linear function of survival probability is equal to the linear predictor.\nThe inverse of the logit function is called the logistic function. It looks like this:\n\\[f(x) = (e^x)/(1 + e^x)\\]\n\nlogistic &lt;- function(x){exp(x)/(1 + exp(x))}\n\nThe logit and logistic function are inverses in the same way that the log and exponential functions are inverses:\n\np &lt;- 0.7 # Start with a probability p\n(lp &lt;- logit(p))\n\n[1] 0.8472979\n\nlogistic(lp) # Recover the original p\n\n[1] 0.7\n\nx &lt;- -17 # Start with any real number\n(lx &lt;- logistic(x))\n\n[1] 4.139938e-08\n\nlogit(lx) # Recover the original x\n\n[1] -17\n\n\nApplying the logistic function to both sides of the model equation gives another equivalent way to write the model: \\[P(Survived|Sex, Age) = logistic(\\beta_0 + \\beta_1Age + \\beta_2Sex + \\beta_3Age \\times Sex)\\] Hence the name “logistic regression”. Now we have survival probability on it’s own, but it’s equal to nonlinear function of the linear predictor.\nWe could also pull out the log from the logit (log-odds), and write the model as\n\\[odds(Survived|Sex, Age) = \\exp{(\\beta_0 + \\beta_1Age + \\beta_2Sex + \\beta_3Age \\times Sex)}\\] This leads to a useful interpretation. To see this, let’s drop back to a simpler logistic regression model with one predictor \\(X\\) and a binary response \\(Y\\), where \\(p = P(Y = 1)\\). The equation for this simple model is\n\\[\\log(p/(1 - p)) = \\beta_0 + \\beta_1X\\]\nif we exponentiate both sides to get back to the odds scale, we have\n\\[p/(1-p) = e^{\\beta_0 + \\beta_1X}\\]\nLet’s sort out what happens to \\(\\log(p/(1-p))\\) when \\(X\\) changes by one:\n\\([\\beta_0 + \\beta_1(X+1)] - [\\beta_0 + \\beta_1X] = \\beta_1\\)\nSo a one-unit increase in \\(X\\) corresponds to a \\(\\beta_1\\)-unit change in the log odds. And, because we can exponentiate both sides, a one-unit increase in \\(X\\) corresponds to a multiplicative change of \\(e^{\\beta_1}\\). You already saw this result for the Donner Party data from the Sleuth3 library, and we’ll give another example of interpreting logistic regression coefficients later in this lab."
  },
  {
    "objectID": "z3_learnR.html#fitting-the-binary-logistic-regression-model",
    "href": "z3_learnR.html#fitting-the-binary-logistic-regression-model",
    "title": "LearnR 3",
    "section": "Fitting the Binary Logistic Regression Model",
    "text": "Fitting the Binary Logistic Regression Model\nYou already saw an R demonstration of the logistic regression model fit to the Donner Party data from the Sleuth3 package. Let’s look at the model fit to the full Donner Party dataset in R:\n\nglm1 = glm(survived ~ sex * age, family = binomial(link = \"logit\"), data = Donner)\nsummary(glm1)\n\n\nCall:\nglm(formula = survived ~ sex * age, family = binomial(link = \"logit\"), \n    data = Donner)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  1.85515    0.67108   2.764   0.0057 **\nsexMale     -1.62177    0.82673  -1.962   0.0498 * \nage         -0.04565    0.02480  -1.841   0.0657 . \nsexMale:age  0.01957    0.03120   0.627   0.5305   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 124.37  on 89  degrees of freedom\nResidual deviance: 110.73  on 86  degrees of freedom\nAIC: 118.73\n\nNumber of Fisher Scoring iterations: 4\n\n\nIt doesn’t look like the interaction term, sexMale:age is needed, so we’ll refit the model without it, and use that model to examine a few things and make interpretations.\n\nglm2 &lt;- glm(survived ~ sex + age, family = binomial(link = \"logit\"), data = Donner)\nsummary(glm2)\n\n\nCall:\nglm(formula = survived ~ sex + age, family = binomial(link = \"logit\"), \n    data = Donner)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   1.5992     0.5041   3.172  0.00151 **\nsexMale      -1.2068     0.4790  -2.519  0.01176 * \nage          -0.0338     0.0151  -2.238  0.02525 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 124.37  on 89  degrees of freedom\nResidual deviance: 111.13  on 87  degrees of freedom\nAIC: 117.13\n\nNumber of Fisher Scoring iterations: 4\n\n\nIn the same way that we examined the (poor) fit of the multiple linear regression model above, let’s examine the (better) fit of this logistic regression model and see what we notice:\n\n# Obtain 95% pointwise confidence bands from predict.glm()\nglm_pred &lt;- predict.glm(glm2, type=\"link\", se.fit=TRUE)\nlow &lt;- glm_pred$fit - 1.96 * glm_pred$se.fit\nupp &lt;- glm_pred$fit + 1.96 * glm_pred$se.fit\n\n# back-transform everything to the data scale\nglm_fit &lt;- logistic(glm_pred$fit)\nglm_lower &lt;- logistic(low)\nglm_upper &lt;- logistic(upp)\n\n# augment the Donner data frame\naugment_Donner &lt;- as.data.frame(cbind(Donner, glm_fit, glm_lower, glm_upper))\n\n# Big plot\nggplot(data = augment_Donner) + \n  # plot jittered data\n  geom_jitter(aes(x = age, \n                 y = survived,\n                 color = sex,\n                 shape = sex),\n                 height = 0.05, width = 0.2) + \n  \n  # plot fitted lines\n  geom_line(aes(x = age, \n                y = glm_fit, \n                 color = sex)) + \n\n# plot 95% pointwise confidence bands\n  geom_ribbon(aes(x = age, \n                  fill = sex, \n                  ymin = glm_lower, \n                  ymax = glm_upper),\n              alpha = 0.2) +\n  \n  # plot reference lines at 0 and 1 (minimum and maximum possible probabilities)\n  \n  geom_hline(yintercept = 0, lty = 2, alpha = 0.4) +\n  geom_hline(yintercept = 1, lty = 2, alpha = 0.4) +\n  facet_grid(.~sex) + \n  ggtitle(\"Generalized linear model for Donner Party\")\n\n\n\n\nIn this plot, we show the fitted line for both males and females, and the 95% confidence bands for both. Just like our confidence bands from the linear model we fit before, these are pointwise confidence bands. They do not represent 95% confidence that the band contains the entire survival curve, only 95% confidence that the survival probability is within the band at each individual point. Simultaneous confidence bands for curves are much more theoretically challenging to construct than pointwise bands, but intuitively, a 95% simultaneous confidence band for the whole curve would be wider than the 95% pointwise band shown here. As explained above, prediction intervals do not make sense in principle for binary data, so we do not calculate any from this model.\nBinary logistic regression models can be used as classifiers – where a new combination of explanatory information is mapped to a 0 or a 1. Recall from Data Analytics I that a prediction from a multiple linear regression model is our best guess at a new observation from a Normal population distribution, conditioned on specific, known values for the explanatory variables. In the present case, a prediction should be our best guess at a new observation (i.e., survived or died) from a binary distribution, conditioned on an age and a sex for a new individual (supposing we didn’t already know the survival status of all the possible Donner Party members). The predictive distribution for a new observation at each point is Bernoulli(p), where p is the fitted probability at the specific values of the new combination of explanatory information. Obtaining a new prediction at a given setting of the predictor variables involves either drawing from this Bernoulli distribution, or setting a probability threshold (typically 1/2) and predicting a value of 1 if the fitted probability exceeds the threshold, 0 otherwise. For example, you could predict that a man like Jacob Donner, age 65, would not survive according to the 1/2 probability threshold:\n\nlike_Jacob_Donner &lt;- data.frame(age = 65, sex = \"Male\")\nlike_Jacob_Donner$age %&lt;&gt;% as.numeric()\n# fitted probability that a 65 year old man would survive\npredict(glm2, newdata = like_Jacob_Donner, type = \"response\")\n\n       1 \n0.141301 \n\n\nIt’s really important to notice that neither the fitted lines nor the 95% pointwise confidence bands go outside the horizontal band given by the y = 0 and y = 1 lines. This is what we should expect – a much more satisfactory result from the logistic regression model than from the multiple linear regression model. Conceptually, this is because the logistic regression model is designed for modeling data such as those in the Donner Party example, where the response variable is binary. Mathematically, it is because the logistic function that maps the fitted values and confidence bounds back from the log-odds scale to the probability scale is only capable of outputting values between 0 and 1."
  },
  {
    "objectID": "z3_learnR.html#assessing-model-fit",
    "href": "z3_learnR.html#assessing-model-fit",
    "title": "LearnR 3",
    "section": "Assessing Model Fit",
    "text": "Assessing Model Fit\nIn the case of binary responses, it’s not necessary or even all that meaningful to examine residuals. Since the only response values are 0’s and 1’s, and the fitted values (once we back transform to the data scale) are probabilities, there is not a wide range of possible values for the residuals and they tend to be not very informative.\nIn the case of the Donner Party data, and as you can see from the plot above, the model fit is not terrific. For example, the model did not seem to pick up the “sweet spot” we noted earlier – females between the ages of about 5 and 25 seem to have all survived. Perhaps a more curvy or wiggly function would provide a better fit? Consider the following plot:\n\nggplot(data = augment_Donner) + \n  # plot jittered data\n  geom_jitter(aes(x = age, \n                 y = survived,\n                 color = sex,\n                 shape = sex),\n                 height = 0.05, width = 0.2) + \n  \n  # plot loess smoother\n  geom_smooth(aes(x = age, \n                y = survived,\n                 color = sex)) + \n  \nfacet_grid(.~sex) + \n  ggtitle(\"Loess Smooth for Donner Party\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe loess smoother that ggplot() used to fit a line and plot confidence bands is called a non-parametric scatterplot smoother. It does seem that these non-monotonic wiggly lines do a better job of modeling survival probability as a function of age, although both the fitted line and 95% confidence bands go outside of the 0 and 1 boundaries, which is troubling.\nUnfortunately, the details of loess and other non-parametric approaches are beyond the scope of this class. A big drawback to this non-parametric model fit, however, is that we can’t make any inferences about the relationships among age, sex and survival from it. That is, all we have is the plot above and a wiggly fitted line which we could use to make predictions. For all our talk about the process of making predictions from a logistic regression model, we can’t actually make much use of a purely predictive model for these particular data – whose survival are we to predict, anyway?!\nInstead, we’ll go back to our imperfect logistic regression model and make inferences about the relationships among sex, age and survival using that model. There’s a famous quote, attributed to George Box (1919-2013), a well-respected British statistician who spent a large part of his academic career in the US:\n\n“All models are wrong, but some are useful.”\n\nThis is certainly the case with our logistic regression model. It may not be the “correct” model, or even the “best” one for the Donner Party data (you might try adding a quadratic term in age and see if that looks better than either of the preceding approaches). Nevertheless, we can use it to make some useful statements about the data."
  },
  {
    "objectID": "z3_learnR.html#interpreting-the-fitted-model",
    "href": "z3_learnR.html#interpreting-the-fitted-model",
    "title": "LearnR 3",
    "section": "Interpreting The Fitted Model",
    "text": "Interpreting The Fitted Model\nLet’s look again at the summary information from the logistic regression model we fit:\n\nsummary(glm2)\n\n\nCall:\nglm(formula = survived ~ sex + age, family = binomial(link = \"logit\"), \n    data = Donner)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   1.5992     0.5041   3.172  0.00151 **\nsexMale      -1.2068     0.4790  -2.519  0.01176 * \nage          -0.0338     0.0151  -2.238  0.02525 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 124.37  on 89  degrees of freedom\nResidual deviance: 111.13  on 87  degrees of freedom\nAIC: 117.13\n\nNumber of Fisher Scoring iterations: 4\n\n\nIt appears that both age and sex are important for helping us to understand the odds of survival using this model. Since sexMale is an indicator function, we can write the fitted model in two parts, one for females:\nlog(odds of survival) = 1.60 - 0.03age.\nAnd one for males:\nlog(odds of survival) = 0.39 - 0.03age.\nIn this second sub-model, 0.3924 = 1.5992 - 1.2068, and we rounded to the hundredths to get 0.39.\nIt’s from these model fits that we can obtain some estimate of the relative odds of survival for men and women of the same age. As you saw in the narrated lecture materials, because there’s no interaction term in this model, we can just exponentiate the coefficient on sexMale (-1.2068) and obtain an estimate of the multiplicative difference in odds of survival for a male and female of the same age. We can also exponentiate the upper and lower bounds of the 95% confidence interval for the regression coefficient on sexMale.\n\nexp(-1.2068)\n\n[1] 0.299153\n\nexp(confint(glm2))\n\nWaiting for profiling to be done...\n\n\n                2.5 %     97.5 %\n(Intercept) 1.9486563 14.3550499\nsexMale     0.1128658  0.7481205\nage         0.9367677  0.9945734\n\n\nThe odds of survival for a male are estimated to be roughly 30% of the odds of survival for a female, when comparing individuals of the same age (remember this is exactly the result we got from the contingency table when we ignored age, because in our final logistic regression model we’ve decided not to include any interaction between age and sex). A 95% confidence interval for this multiplicative difference runs from 11% to 75%.\nYou can also use the logistic back-transformation to make a comparison in terms of the probability of survival:\n\nlogistic(-1.2068)\n\n[1] 0.2302677\n\nci &lt;- confint(glm2)\n\nWaiting for profiling to be done...\n\nlogistic(ci)\n\n                2.5 %    97.5 %\n(Intercept) 0.6608625 0.9348748\nsexMale     0.1014191 0.4279571\nage         0.4836758 0.4986397\n\n\nThe probability of survival for a male is estimated to be 0.23 lower than that of a female, when comparing two individuals of the same age. A 95% confidence interval for this (additive) difference runs from 0.10 to 0.43."
  },
  {
    "objectID": "z3_learnR.html#data-scale-vs-model-scale-back-transformation",
    "href": "z3_learnR.html#data-scale-vs-model-scale-back-transformation",
    "title": "LearnR 3",
    "section": "Data Scale vs Model Scale, Back-transformation",
    "text": "Data Scale vs Model Scale, Back-transformation\nNotice that the two interpretations just given are both on back-transformed scales. In general, people have a better appreciation for and understanding of probabilities and odds than they do (have) of log-odds.\n\nThe logistic regression model is fit on the log-odds scale – we call this the model scale. When we back-transform to probabilities, we get back to the data scale (i.e., the same scale as the data).\n\nThe odds scale is intermediate between the the data scale and the model scale, and it affords another option for how we interpret and communicate the results of a logistic regression model."
  },
  {
    "objectID": "z3_learnR.html#r-markdown-for-submission",
    "href": "z3_learnR.html#r-markdown-for-submission",
    "title": "LearnR 3",
    "section": "R Markdown for submission",
    "text": "R Markdown for submission\nLast week’s instructions are repeated here for your convenience.\nNOTE: If you are reading these instructions from the HTML preview in the Viewer pane, they will be overwritten in the following steps. You can avoid this by “popping out” the current HTML preview into an external browser window, by clicking the the “Show in new window” icon (to the right of the broom icon in the Viewer pane).\nAlternatively, you can read the instructions right from the .Rmd script, keeping in mind that you will need to switch back to the lab script tab to view the rest of the instructions once you create a new script. You could also copy and paste the whole Lab 3 Assignment Instructions section into your new document while you’re working on it, and then delete the instructions before you submit it.\nClick “File” -&gt; “New File” -&gt; “R Markdown”, and dialog box will pop up. Change the title to “Lab Assignment 3” and name yourself as author. Select PDF as the Default Output Format, then click OK. The header of your new file should look something like this:\n---\ntitle: \"Lab Assignment 3\"\nauthor: \"Ronald Fisher\"\ndate: \"2024-04-04\"\noutput: pdf_document\n---\nThe file will initially contain some examples to get you started with RMarkdown, which you should replace with your lab content. Save the notebook as something like “Lab_Assignment_3” using “File” –&gt; “Save As…”\nIn your new .Rmd script, answer the questions in the “Questions” section below. Include all the code you need to produce the requested outputs. Your script should include a top-level section heading for each question, for example:\n# Question 1\n\nstuff here\n\n# Question 2\n\nother stuff\nWhether or not you include the text of the questions in your script is up to you.\n\nDo be sure to include, near the top of your script, a code chunk that loads any non-default packages you use (such as vcdExtra or Sleuth3).\n\nWithin the question sections, you can chunk your code in whatever way seems reasonable. Incorporate any written answers outside the code chunks using Markdown formatting as needed (see “Help” -&gt; “RMarkdown Quick Reference” for text formatting help).\nTo ensure that your .Rmd script will be fully self-contained (i.e. it will not depend on objects that were defined during the lab, and could be run as-is if you sent it to someone else), you should clear the workspace before you begin.\n\nTo clear the workspace, click the broom icon in the Environment pane.\n\nOnce you’ve answered the questions in your new .Rmd script and you have verified your code is self contained, you should Run -&gt; Run All Chunks and generate a .pdf file of your document, to check that everything looks like you want it to. Having concluded that your .Rmd script produces a pdf document that includes all the output you want, submit both the Lab_Assignment_3.Rmd file and the pdf document on Canvas as your Lab Assignment 3.\nFeel free to post on the discussion board if you have any questions or encounter any difficulties with this process."
  },
  {
    "objectID": "z3_learnR.html#questions",
    "href": "z3_learnR.html#questions",
    "title": "LearnR 3",
    "section": "Questions",
    "text": "Questions\nThe data set ResumeNames in the AER package contains information about 4870 fictitious resumes, sent to real employers as part of an experiment about racial discrimination in hiring. The binary response for each resume is whether the employer called back. Although the original research question was about racial discrimination, there are many kinds of questions you might answer using these data.\n\nInstall and load the AER package, and read the help file for the ResumeNames data.\nCome up with a question about the probability of callback (the binary response) that can be answered using at least one (but no more than 5) of the 26 available predictor variables.\n\nConsider: - Restricting your analysis to an interesting subset of the data - Transforming and combining input variables - Exploring interactions\nSome example questions (which you may use or modify if you want)\n\n(How) does the relationship between callback probability and resume quality differ by applicant race?\nHow many additional years of experience is having a white name vs a black name “worth” in terms of callback probability?\nDoes callback probability differ between applicants who meet stated job requirements and applicants who don’t? For instance, you might compare applicants who do or not meet the stated minimum number of years of experience, or applicants who do or do not list computer skills when applying to jobs for which computer skills are ostensibly required.\nDoes callback probability differ between male and female applicants by industry or position?\n\nBe creative!\n\nUse a logisitic regression model to address the question you posed in 2. Be sure to examine the fit of your model, and write a few sentences about your interpretation of the model as it addresses the question you posed."
  },
  {
    "objectID": "z5_learnR.html",
    "href": "z5_learnR.html",
    "title": "LearnR 5",
    "section": "",
    "text": "Please load these libraries that you’ll need for this lab:\n\nlibrary(arm)\n\nWarning: package 'arm' was built under R version 4.3.3\n\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is C:/Users/cmeck/Desktop/D_Anal/Anal_II/Anal_II_Notes/Data_Analysis_II_Notes\n\nlibrary(Sleuth3)\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ dplyr::select() masks MASS::select()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(vcdExtra)\n\nWarning: package 'vcdExtra' was built under R version 4.3.3\n\n\nLoading required package: vcd\n\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\nLoading required package: gnm\n\n\nWarning: package 'gnm' was built under R version 4.3.3\n\n\n\nAttaching package: 'vcdExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nIn this lab, we’ll go over log linear regression in the case of count data in a little more detail than you saw in the narrated lectures. We’ll cover the deviance goodness of fit test (which, remember, is an informal test here) and the drop in deviance test in R; and we’ll discuss residuals and model evaluation. You’ll see more about over dispersion in the case of Poisson counts, and we’ll cover the negative binomial model for over dispersion. We’ll conclude with another example of log linear regression for general contingency tables."
  },
  {
    "objectID": "z5_learnR.html#some-data-exploration",
    "href": "z5_learnR.html#some-data-exploration",
    "title": "LearnR 5",
    "section": "Some data exploration",
    "text": "Some data exploration\nWe’ll start with some exploration of the data.\n\nggplot(data = salamanders, aes(x = PctCover, y = Salamanders)) + geom_point() + ggtitle(\"Salamanders vs Percent Cover\")\n\n\n\n\nThis first figure seems to indicate that there are two distinct types of sites—those with PctCover below about 53% and those with PctCover greater than 75%. It also seems clear that only sites with PctCover greater than 75% have Salamander counts higher than 2.\nHere’s another plot:\n\nggplot(data = salamanders, aes(x = ForestAge, y = Salamanders)) + geom_point() + ggtitle(\"Salamanders vs Forest Age\")\n\n\n\n\nThere doesn’t appear to be anything remarkable about this plot, though it’s important to notice the range of the x-axis scale – it’s fairly large. This suggests that we might observe something more informative if we look at ForestAge on the log scale.\n\nggplot(data = salamanders, aes(x = log(ForestAge + 1/2), y = Salamanders)) + geom_point() + ggtitle(\"Salamanders vs log(Forest Age)\")\n\n\n\n\nNotice that we used x = log(ForestAge + 1/2) in the ggplot() function call because some of the ForestAge values are zero, and log(0) is undefined.\nThis plot seems to indicate a clearer relationship between ForestAge and Salamanders – as ForestAge increases on the log scale, the salamander counts tend to increase and they get more variable. Let’s also look at the relationship between the two explanatory variables:\n\n(ggplot(data = salamanders, aes(x = log(ForestAge + 1/2), y = PctCover)) + geom_point() + ggtitle(\"Percent Cover vs log(Forest Age\"))\n\n\n\n\nThis plot tells and interesting story – almost all of the older sites are those with PctCover greater than 75%. For the younger sites, there is a increasing (perhaps curvilinear) relationship with PctCover.\nRemember that multicollinearity is a problem when two or more explanatory variables are highly correlated. Let’s check the correlation between log-transformed PctCover and the ForestAge variable.\n\nsalamanders %&lt;&gt;% mutate(Site = Site, Salamanders = Salamanders, ForestAge = ForestAge, logPctCover = log(PctCover + 1/2))\ncor(salamanders$ForestAge,salamanders$logPctCover)\n\n[1] 0.5352604\n\n\nThe correlation is not actually so high that we should be concerned about it. Remember, too, that correlation tells us about the linear association between two variables. In terms of a linear association, these two variables are moderately correlated – even though from our last plot we see that there is a strong relationship between them.\nBecause of the distinctive cut-point in the PctForest variable, we’ll create a new variable, called CovGroup, that just takes the value 0 if PctCover &lt; 75) and 1 otherwise.\n\nsalamanders$CovGroup &lt;- ifelse(salamanders$PctCover &lt; 75,0,1)\n\nFinally, as a last bit of exploration, let’s just take a look at the histogram of Salamander counts.\n\nggplot(salamanders,aes(Salamanders)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nYou can see that there are a lot of zeroes in among the Salamander counts. One method for analyzing Poisson count data is to take the log of the counts, and then perform a multiple linear regression. We don’t recommend that here, since there are so many zeroes – you’d have to first add a small amount to the zero counts, then take logs, and that means that interpretations will be difficult.\nInstead, we’ll turn to fitting the log linear model (another special case of a generalized linear model)."
  },
  {
    "objectID": "z5_learnR.html#model-fitting",
    "href": "z5_learnR.html#model-fitting",
    "title": "LearnR 5",
    "section": "Model Fitting",
    "text": "Model Fitting\nIn this part of the lab, we’ll fit several different models, and make some comparisons among them. This is a little bit of data snooping, and we will likely end up with biased estimates in the model we do select (recall the simulation you saw back in Data Analytics I). So, please, consider this to be the academic exercise it’s intended to be!\n\n# model with PctCover only\npois_mod1 &lt;- glm(data = salamanders, Salamanders ~ PctCover, family = poisson)\n\n# model the log(Forest Age + 1/2) only\npois_mod2 &lt;- glm(data = salamanders, Salamanders ~ log(ForestAge + 1/2), family = poisson)\n\n# model with both explanatory variables\npois_mod3 &lt;- glm(data = salamanders, Salamanders ~ PctCover + log(ForestAge + 1/2), family = poisson)\n\n# model with both explanatory variables, plus their interaction\npois_mod4 &lt;- glm(data = salamanders, Salamanders ~ PctCover * log(ForestAge + 1/2), family = poisson)\n\n# comparison of the models\nLRstats(pois_mod1, pois_mod2, pois_mod3, pois_mod4)\n\n\n\n\n\n\nAIC\nBIC\nLR Chisq\nDf\nPr(&gt;Chisq)\n\n\n\n\npois_mod1\n210.3639\n214.0642\n121.3050\n45\n0\n\n\npois_mod2\n243.1239\n246.8242\n154.0650\n45\n0\n\n\npois_mod3\n212.0690\n217.6195\n121.0101\n44\n0\n\n\npois_mod4\n213.7578\n221.1584\n120.6989\n43\n0\n\n\n\n\n\n\nIn terms of the AIC comparisons, there’s not much difference between models 1, 2 and 4; and the same is true if we use the BIC comparisons. This is somewhat surprising – from the exploratory plots above, you might have suspected that both PctCover and ForestAge, and maybe even their interaction might be important. Let’s take a look at the summary information for pois_mod4.\n\nsummary(pois_mod4)\n\n\nCall:\nglm(formula = Salamanders ~ PctCover * log(ForestAge + 1/2), \n    family = poisson, data = salamanders)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)                   -0.969454   0.820929  -1.181  0.23763   \nPctCover                       0.029067   0.011184   2.599  0.00935 **\nlog(ForestAge + 1/2)          -0.213107   0.293009  -0.727  0.46704   \nPctCover:log(ForestAge + 1/2)  0.001957   0.003423   0.572  0.56741   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 190.22  on 46  degrees of freedom\nResidual deviance: 120.70  on 43  degrees of freedom\nAIC: 213.76\n\nNumber of Fisher Scoring iterations: 5\n\n\nIt looks like there pretty clear evidence of over dispersion, the residual deviance divided by the residual degrees of freedom is\n\n120.7/43\n\n[1] 2.806977\n\n\nThat’s pretty big. Before we talk a lot more about model fitting and model comparison in terms of what explanatory information to include, we should address the over dispersion. Remember that we can do this using the family = quasipoisson argument to the glm() function, or we can do this using the glm.nb function. We’ll leave it to you to try the quasi-poisson approach, and we’ll go over the negative binomial approach here. We’ll simply fit the same four models as above, but using the negative binomial likelihood rather than the Poisson likelihood.\n\n# model with PctCover only\nnb_mod1 &lt;- glm.nb(data = salamanders, Salamanders ~ PctCover)\n\n# model the log(Forest Age + 1/2) only\nnb_mod2 &lt;- glm.nb(data = salamanders, Salamanders ~ log(ForestAge + 1/2))\n\n# model with both explanatory variables\nnb_mod3 &lt;- glm.nb(data = salamanders, Salamanders ~ PctCover + log(ForestAge + 1/2))\n\n# model with both explanatory variables, plus their interaction\nnb_mod4 &lt;- glm.nb(data = salamanders, Salamanders ~ PctCover * log(ForestAge + 1/2))\n\n# comparison of the models\nLRstats(nb_mod1, nb_mod2, nb_mod3, nb_mod4)\n\n\n\n\n\n\nAIC\nBIC\nLR Chisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nnb_mod1\n176.9625\n182.5130\n47.60608\n45\n0.3670830\n\n\nnb_mod2\n188.2859\n193.8363\n47.99615\n45\n0.3523296\n\n\nnb_mod3\n178.8296\n186.2302\n47.56748\n44\n0.3295507\n\n\nnb_mod4\n180.5434\n189.7941\n47.48537\n43\n0.2948841\n\n\n\n\n\n\nAgain, there’s no clear winner here in terms of either AIC or BIC, so we’ll go with Occam’s Razor and look at results for the simplest model, nb_mod1.\n\nsummary(nb_mod1)\n\n\nCall:\nglm.nb(formula = Salamanders ~ PctCover, data = salamanders, \n    init.theta = 1.26199236, link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.416365   0.527696  -2.684  0.00727 ** \nPctCover     0.031513   0.006655   4.735 2.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.262) family taken to be 1)\n\n    Null deviance: 75.691  on 46  degrees of freedom\nResidual deviance: 47.606  on 45  degrees of freedom\nAIC: 176.96\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.262 \n          Std. Err.:  0.478 \n\n 2 x log-likelihood:  -170.963 \n\n\nRemember that we still didn’t use the CovGroup indicator variable that we created above. Let’s fit a couple more models using that variable, and see where we are.\n\n# model with CovGroup only \nnb_mod5 &lt;- glm.nb(data = salamanders, Salamanders ~ CovGroup)\n\n# model with CovGroup*log(ForestAge + 1/2)\nnb_mod6 &lt;- glm.nb(data = salamanders, Salamanders ~ CovGroup * log(ForestAge + 1/2))\n\n# compare with the other models\nLRstats(nb_mod1,nb_mod2,nb_mod3,nb_mod4,nb_mod5,nb_mod6)\n\n\n\n\n\n\nAIC\nBIC\nLR Chisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nnb_mod1\n176.9625\n182.5130\n47.60608\n45\n0.3670830\n\n\nnb_mod2\n188.2859\n193.8363\n47.99615\n45\n0.3523296\n\n\nnb_mod3\n178.8296\n186.2302\n47.56748\n44\n0.3295507\n\n\nnb_mod4\n180.5434\n189.7941\n47.48537\n43\n0.2948841\n\n\nnb_mod5\n176.7513\n182.3018\n47.17214\n45\n0.3838253\n\n\nnb_mod6\n180.7331\n189.9838\n47.16892\n43\n0.3060094\n\n\n\n\n\n\nNow it seems like there’s really not much difference among any of these models (except that many nb_mod2 is a clear-ish loser) in terms of AIC or BIC. We’ll continue with nb_mod1 at this point. Let’s take a look at the model and it’s fit.\n\nsummary(nb_mod1)\n\n\nCall:\nglm.nb(formula = Salamanders ~ PctCover, data = salamanders, \n    init.theta = 1.26199236, link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.416365   0.527696  -2.684  0.00727 ** \nPctCover     0.031513   0.006655   4.735 2.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.262) family taken to be 1)\n\n    Null deviance: 75.691  on 46  degrees of freedom\nResidual deviance: 47.606  on 45  degrees of freedom\nAIC: 176.96\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.262 \n          Std. Err.:  0.478 \n\n 2 x log-likelihood:  -170.963 \n\nsalamanders$resid &lt;- residuals(nb_mod1)\nfits &lt;- predict.glm(nb_mod1,scale=\"data\",se.fit=TRUE)\nsalamanders$fits &lt;- exp(fits$fit)\nsalamanders$low &lt;- exp(fits$fit-1.96*fits$se.fit)\nsalamanders$upp &lt;- exp(fits$fit+1.96*fits$se.fit)\nggplot(salamanders,aes(PctCover,Salamanders)) + geom_point() +      \n  geom_line(aes(x = PctCover, \n                 y = fits)) + \n  geom_ribbon(aes(x = PctCover, \n                  ymin = low, \n                  ymax = upp),\n                  alpha = 0.2)\n\n\n\n\nThe model fit is not bad – remember that the solid line in the figure above is the model estimate of the Poisson rate parameter at each value of PctCover, and the shaded bands are 95% pointwise confidence intervals. The model output gives strong evidence of a positive association between PctCover and the number of salamanders at a site.\nNotice also that the residual deviance divided by its degrees of freedom for nb_mod1 is very close to 1. The negative binomial model also gives an estimate of the “Theta” parameter. In the negative binomial model, if the expected count is \\(E(Y)\\), the variance of the count is \\(Var(Y) = E(Y) + [E(Y)^2]/\\theta\\).\nSince the negative binomial model uses the log-link as does the Poisson model, the interpretation of the coefficient on PctCover is the same as in the Poisson model – an increase of 1% in forest cover is associated with an \\(exp(0.0315) = 1.03\\)-fold increase in the expected number of salamanders. In other words, a 1% increase in forest cover is associated with a 3% increase in expected number of salamanders at a site."
  },
  {
    "objectID": "z5_learnR.html#model-evaluation",
    "href": "z5_learnR.html#model-evaluation",
    "title": "LearnR 5",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nRecall that the deviance goodness of fit test compares a fitted model to a saturated model, or one in which there are as many parameters as there are data points. In these goodness of fit comparisons, the null hypothesis corresponds to the fitted model (which is a reduced model relative to the saturated model), and the alternative hypothesis corresponds to the saturated model. In the case of Poisson counts, the goodness of fit should only be used as a guideline, not as a firm decision making tool.\n\nLRstats(nb_mod1)\n\n\n\n\n\n\nAIC\nBIC\nLR Chisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nnb_mod1\n176.9625\n182.513\n47.60608\n45\n0.367083\n\n\n\n\n\n\nThis large p-value may be an indication of an adequate model, but there are many small counts in the salamanders data, so we should not rely heavily on this result."
  },
  {
    "objectID": "z5_learnR.html#drop-in-deviance-test",
    "href": "z5_learnR.html#drop-in-deviance-test",
    "title": "LearnR 5",
    "section": "Drop-in-deviance test",
    "text": "Drop-in-deviance test\nJust a reminder that the drop in deviance test is different from the deviance goodness of fit test. Whereas the deviance goodness of fit test provides a comparison between a single fitted model and a saturated model, a drop in deviance test provides a way to compare two fitted models when one of those models is nested within the other one. Put another way, the drop in deviance test is a comparison between a reduced model (null hypothesis) and a full model (alternative hypothesis) – and we use it in cases where the reduced model is reduced from (or nested in) the full model.\nUsing the models we have already fit, let’s use the anova() function to perform a drop in deviance test comparing the model that only contains PctCover to the one that contains PctCover, ForestAge and their interaction.\n\nanova(nb_mod1,nb_mod4,test=\"Chisq\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\ntheta\nResid. df\n2 x log-lik.\nTest\ndf\nLR stat.\nPr(Chi)\n\n\n\n\nPctCover\n1.261992\n45\n-170.9625\n\nNA\nNA\nNA\n\n\nPctCover * log(ForestAge + 1/2)\n1.279151\n43\n-170.5434\n1 vs 2\n2\n0.419181\n0.8109163\n\n\n\n\n\n\nThe p-value of the drop in deviance test is rather large, giving us strong evidence that the simpler model is sufficient in this case, as compared to the more complicated model."
  },
  {
    "objectID": "z5_learnR.html#looking-at-residuals",
    "href": "z5_learnR.html#looking-at-residuals",
    "title": "LearnR 5",
    "section": "Looking at Residuals",
    "text": "Looking at Residuals\nAs in the case of binomial logistic regression, it can be helpful to look at the deviance and/or Pearson residuals from our negative binomial (or Poisson) regression mode to (a) evaluate the model fit and (b) check for outliers. Provided that the counts are fairly large, both the deviance and Pearson residuals should look like draws from a standard Normal distribution, so too many residuals outside of the [-2,2] interval may be cause for concern. Here, we’ll look at plots of both the deviance and Pearson residuals.\n\nsalamanders$residuals_deviance &lt;- residuals(nb_mod1)\nsalamanders$residuals_pearson &lt;- residuals(nb_mod1, type = \"pearson\")\nggplot(data = salamanders, aes(PctCover,residuals_deviance)) + geom_point()\n\n\n\nggplot(data = salamanders, aes(PctCover,residuals_pearson)) + geom_point()\n\n\n\nggplot(data = salamanders, aes(residuals_deviance,residuals_pearson)) + geom_point()\n\n\n\n\nFirst, you should notice that there are some distinctive patterns in both the deviance and Pearson residual plots. These kinds of patterns are quite common when we are dealing with count data, and they are result of the discreteness of those counts. In these residual plots, we are more concerned with detecting potential outliers, and there do not appear to be any here.\nNext, the plot of the two types of residuals against each other shows the strong relationship between them – in this case it’s not a linear relationship as it was in the case of binomial logistic regression – but it’s a strong relationship nonetheless."
  },
  {
    "objectID": "z5_learnR.html#questions",
    "href": "z5_learnR.html#questions",
    "title": "LearnR 5",
    "section": "Questions",
    "text": "Questions\n\nFit a model to the gay marriage data that includes all two-way interactions. What do you conclude from this model? Be specific and try to address questions having to do with the association among the three variables.\nFit a model that includes all two-way and the three-way interactions. Is there anything problematic about this model? Please explain."
  },
  {
    "objectID": "z7_learnR.html",
    "href": "z7_learnR.html",
    "title": "LearnR 7",
    "section": "",
    "text": "In this lab we’ll look at some simulated data that provide a nice demonstration of the interpretation problem presented by generalized linear mixed modeling. We’ll also go through another example of fitting a Generalized Linear Mixed Model (GLMM) and making sense of the output.\nTo start off, please load these libraries that you’ll need for this lab. Please note that robustbase is new, so you will likely have to install it first.\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(robustbase) # contains data we'll use\nlibrary(ggplot2)\nlibrary(vcdExtra)\n\nWarning: package 'vcdExtra' was built under R version 4.3.3\n\n\nLoading required package: vcd\n\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\nLoading required package: gnm\n\n\nWarning: package 'gnm' was built under R version 4.3.3\n\n\n\nAttaching package: 'vcdExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(lme4)     # access the mixed functions\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack"
  },
  {
    "objectID": "z7_learnR.html#picking-the-random-effects",
    "href": "z7_learnR.html#picking-the-random-effects",
    "title": "LearnR 7",
    "section": "Picking the Random Effects",
    "text": "Picking the Random Effects\nIn the model we just fit, we used a random intercepts model that just includes a random effect for patient. Specifically what this means that is the intercept term will be estimated differently for each patient.\nDepending on the particular problem, this may or may not be a reasonable way to think about the variation in the data. In the context of this epilepsy example, it could be that the different patients show different rates of change in their numbers of seizures – this would suggest that each patient have his or her own effect of visit. We can accomplish this by putting in a random effect for visit.\nNext we’ll fit a random intercepts and slopes model with Visit as the random slope. Again, this seems like a reasonable thing to do if we expect that some subjects will have a different rate of increase or decrease of number of seizures over time than other subjects.\n\nmod2 &lt;- glmer(Seizures ~ Trt + Base + Age + Visit + (1 + Visit|ID), family = poisson, data = epil_long)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00368596 (tol = 0.002, component 1)\n\n# oops this didn't converge --- try the initialization trick again:\n(init &lt;- getME(mod2, name = c(\"theta\", \"fixef\")))\n\n$theta\n      ID.(Intercept) ID.Visit.(Intercept)             ID.Visit \n          0.60038496          -0.06873505           0.12686483 \n\n$fixef\n (Intercept) Trtprogabide         Base          Age        Visit \n  0.61274912  -0.25315623   0.02736649   0.01545712  -0.05904358 \n\nmod2 &lt;- glmer(Seizures ~ Trt + Base + Age + Visit + (1 + Visit|ID), family = poisson, data = epil_long, start = init)\nsummary(mod2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Seizures ~ Trt + Base + Age + Visit + (1 + Visit | ID)\n   Data: epil_long\n\n     AIC      BIC   logLik deviance df.resid \n  1333.1   1360.8   -658.5   1317.1      228 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9215 -0.7439 -0.0919  0.5534  6.3674 \n\nRandom effects:\n Groups Name        Variance Std.Dev. Corr \n ID     (Intercept) 0.36044  0.6004        \n        Visit       0.02082  0.1443   -0.48\nNumber of obs: 236, groups:  ID, 59\n\nFixed effects:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.61284    0.41536   1.475   0.1401    \nTrtprogabide -0.25314    0.15324  -1.652   0.0986 .  \nBase          0.02737    0.00278   9.843   &lt;2e-16 ***\nAge           0.01545    0.01273   1.214   0.2247    \nVisit        -0.05904    0.03252  -1.816   0.0694 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Trtprg Base   Age   \nTrtprogabid -0.290                     \nBase        -0.398 -0.003              \nAge         -0.923  0.122  0.198       \nVisit       -0.128 -0.018  0.009 -0.056\n\n\nThe estimated variance for the Visit random effect is pretty small (0.02), but the fixed effect estimate for Visit is pretty small too (-0.06), so that random effect variance may still be important to consider. Remember that we can compare the two models with the AIC and BIC functions:\n\nAIC(mod1, mod2)\n\n\n\n\n\n\ndf\nAIC\n\n\n\n\nmod1\n6\n1349.228\n\n\nmod2\n8\n1333.084\n\n\n\n\n\nBIC(mod1, mod2)\n\n\n\n\n\n\ndf\nBIC\n\n\n\n\nmod1\n6\n1370.011\n\n\nmod2\n8\n1360.794\n\n\n\n\n\n\nBoth criteria indicate that the model with random slopes for Visit is somewhat preferred over the model with only random intercepts, though simplicity and that the AIC and BIC are not very different (i.e., each for mod1 vs mod2) would argue for using the model with only one random effect. As mentioned in last lab, in a real problem situation, we really want to think more carefully about what the model should be before fitting anything to the data. Choosing which model after fitting many models is a type of data snooping and should generally be avoided."
  },
  {
    "objectID": "z7_learnR.html#looking-at-the-results",
    "href": "z7_learnR.html#looking-at-the-results",
    "title": "LearnR 7",
    "section": "Looking at the results",
    "text": "Looking at the results\nLet’s go back to the random intercept only model and interpret/understand the model output. This model is already a bit complex but is somewhat simpler than the random intercepts and random slopes model (mod2).\n\nsummary(mod1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Seizures ~ Trt + Base + Age + Visit + (1 | ID)\n   Data: epil_long\n\n     AIC      BIC   logLik deviance df.resid \n  1349.2   1370.0   -668.6   1337.2      230 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2867 -0.8240 -0.1332  0.5549  7.2848 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.2856   0.5344  \nNumber of obs: 236, groups:  ID, 59\n\nFixed effects:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.666765   0.410081   1.626  0.10396    \nTrtprogabide -0.261043   0.154119  -1.694  0.09031 .  \nBase          0.027235   0.002799   9.731  &lt; 2e-16 ***\nAge           0.014254   0.012548   1.136  0.25597    \nVisit        -0.058725   0.020178  -2.910  0.00361 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Trtprg Base   Age   \nTrtprogabid -0.289                     \nBase        -0.392 -0.006              \nAge         -0.931  0.114  0.190       \nVisit       -0.119  0.000  0.000  0.000\n\n\nBefore we comment on the model summary information, let’s perform some diagnostics to see if we feel like we have a good model to work with.\n\nResidual Diagnostics\nLet’s also take a look at some residual plots.\n\nepil_long$resid &lt;- residuals(mod1)\nepil_long$fits &lt;- exp(predict(mod1))\n\n# first some plots of the residuals vs. explanatory variables \nggplot(epil_long,aes(Visit,resid)) + geom_point() + facet_wrap(~Trt)\n\n\n\nggplot(epil_long,aes(Base,resid)) + geom_point() + facet_wrap(~Trt)\n\n\n\nggplot(epil_long,aes(Age,resid)) + geom_point() + facet_wrap(~Trt)\n\n\n\n# now a look at the normality of the random effects\nqqnorm(unlist((ranef(mod1)$ID)))\nqqline(unlist((ranef(mod1)$ID)))\n\n\n\n# finally, fitted values versus observations\nggplot(epil_long,aes(Seizures,fits)) + geom_point() + facet_wrap(~Trt) + geom_abline()\n\n\n\nggplot(epil_long,aes(log(Seizures+0.1),log(fits+0.1))) + geom_point() + facet_wrap(~Trt) + geom_abline()\n\n\n\n\nThere are a number of things to discuss based on these plots:\n\nIt looks like there’s evidence of some curvilinearity in the relationship between Visit and Seizures; the nature of that curvilinearity might be different for the two treatment groups (look at plot of resid versus Visit). You could add Visit2 term, the square of Visit, or you could revert Visit back to a factor variable.\nThere is what appears to be one large outlier in the placebo group. If we had access to the original researchers, we might have been able to find out about that unusual value.\nIt seems that the random effects are not strictly Normal, especially in the upper tail. This may be because we haven’t gotten the fixed effects part of the model correctly specified.\nThere are some zeroes among the Seizure values, and the model is not fitting those very well – we may need to think about a zero-inflated model and/or a model for over dispersion."
  },
  {
    "objectID": "z7_learnR.html#more-model-fitting",
    "href": "z7_learnR.html#more-model-fitting",
    "title": "LearnR 7",
    "section": "More Model Fitting",
    "text": "More Model Fitting\nThere is a glmer.nb() function in the lme4 package, so we’ll try using that below. There are zero-inflated GLMM models, but they get rather complicated. We’ll leave it to you to explore those if you’re interested. Also, you might want to investigate what happens when you put terms in the model for the square of Visit and the interactions between Trt and the Visit variables (i.e., Visit and it’s square). Again, we’ll leave this to you for further exploration.\n\n# first, glmer.nb with the same form as above\nmod3 &lt;- glmer.nb(Seizures ~ Trt + Age + Base + Visit + (1|ID), data = epil_long)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0181557 (tol = 0.002, component 1)\n\ninit &lt;- getME(mod3,name=c(\"theta\",\"fixef\"))\nmod3 &lt;- glmer.nb(Seizures ~ Trt + Age + Base + Visit + (1|ID), data = epil_long, start = init)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00295823 (tol = 0.002, component 1)\n\nsummary(mod3)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: Negative Binomial(7.4718)  ( log )\nFormula: Seizures ~ Trt + Age + Base + Visit + (1 | ID)\n   Data: epil_long\n\n     AIC      BIC   logLik deviance df.resid \n  1269.2   1293.4   -627.6   1255.2      229 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9219 -0.6026 -0.1096  0.4506  3.6929 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.2522   0.5021  \nNumber of obs: 236, groups:  ID, 59\n\nFixed effects:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.662022   0.416604   1.589   0.1120    \nTrtprogabide -0.260516   0.154359  -1.688   0.0915 .  \nAge           0.014057   0.012567   1.119   0.2633    \nBase          0.027184   0.002806   9.687   &lt;2e-16 ***\nVisit        -0.052231   0.033086  -1.579   0.1144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Trtprg Age    Base  \nTrtprogabid -0.286                     \nAge         -0.919  0.115              \nBase        -0.387 -0.006  0.189       \nVisit       -0.205  0.004  0.010  0.001\n\n\nLet’s compare this model to the Poisson GLMM above (mod1):\n\nAIC(mod1,mod3)\n\n\n\n\n\n\ndf\nAIC\n\n\n\n\nmod1\n6\n1349.228\n\n\nmod3\n7\n1269.198\n\n\n\n\n\nBIC(mod1,mod3)\n\n\n\n\n\n\ndf\nBIC\n\n\n\n\nmod1\n6\n1370.011\n\n\nmod3\n7\n1293.445\n\n\n\n\n\n\nOK, it seems like the negative binomial model is more appropriate here, so we’ll proceed with that. If you look at the summary information for mod3 it now appears that there’s still only marginal evidence in support of a treatment effect (p = 0.09). Next, we’ve replicated the residual diagnostics that we showed above, but for the mod3 object.\n\nepil_long$resid &lt;- residuals(mod3)\nepil_long$fits &lt;- exp(predict(mod3))\n\n# first some plots of the residuals vs. explanatory variables \nggplot(epil_long,aes(Visit,resid)) + geom_point() + facet_wrap(~Trt)\n\n\n\nggplot(epil_long,aes(Base,resid)) + geom_point() + facet_wrap(~Trt)\n\n\n\nggplot(epil_long,aes(Age,resid)) + geom_point() + facet_wrap(~Trt)\n\n\n\n# now a look at the normality of the random effects\nqqnorm(unlist((ranef(mod1)$ID)))\nqqline(unlist((ranef(mod1)$ID)))\n\n\n\n# finally, fitted values versus observations\nggplot(epil_long,aes(Seizures,fits)) + geom_point() + facet_wrap(~Trt) + geom_abline()\n\n\n\nggplot(epil_long,aes(log(Seizures+0.1),log(fits+0.1))) + geom_point() + facet_wrap(~Trt) + geom_abline()\n\n\n\n\nWe still see many of the same problems we noted above, but despite that we won’t proceed beyond this model for the purpose of this Lab. If this were the final model from which we were to report results, we might say something like:\n\nWe fit a generalized linear mixed model to the epilespy data, assuming that the number of seizures follows a negative binomial distribution and with a log link. The fixed effects are age, baseline number of seizures and visit, and the random effect is subject, to account for the repeated measurements for each subject. From this model, there is no evidence of an effect of the progabide treatment (p = 0.09), and the only factor that appears to be associated with the number of seizures post-treatment is the baseline number of seizures.\n\nOne final note. If our final model does provide evidence of a treatment effect, we can go ahead and report the p-value. It will be difficult to quantify the treatment effect, however, because you’ll have to do that in the context of conditioning on the random subjects."
  }
]