{
  "hash": "a4c7e87d9cd365a9aac91e8cc6ad214a",
  "result": {
    "markdown": "---\ntitle: \"LearnR 7\"\nexecute: \n  freeze: auto\n---\n\n\n\nIn this lab we'll look at some simulated data that provide a nice demonstration of the interpretation problem presented by generalized linear mixed modeling. We'll also go through another example of fitting a Generalized Linear Mixed Model (GLMM) and making sense of the output.\n\nTo start off, please load these libraries that you'll need for this lab. Please note that `robustbase` is new, so you will likely have to install it first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.3.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(robustbase) # contains data we'll use\nlibrary(ggplot2)\nlibrary(vcdExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'vcdExtra' was built under R version 4.3.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: vcd\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'vcd' was built under R version 4.3.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: grid\nLoading required package: gnm\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'gnm' was built under R version 4.3.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'vcdExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n```\n:::\n\n```{.r .cell-code}\nlibrary(magrittr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n```\n:::\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n:::\n\n```{.r .cell-code}\nlibrary(lme4)     # access the mixed functions\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n:::\n\n\n# Simulation of Machine Defects\n\nLet's suppose that we have 10 machines producing a product and that we record the number of defective items that each machine produces at four different times (times 1, 2, 3, 4). In this scenario, we might reasonably expect there to be similarities in numbers of defects for the same machine and differences in numbers of defects from different machines. In this way, thinking of machine as a grouping variable, we would include it in an analysis as a random effect. The response (number of defects) is a count, and in this simulation, we'll use the Poisson distribution to generate the numbers of defects. \n\nWe'll simulate data from a Poisson model, and use a mixed effects Poisson regression model (a particular kind of GLMM) to fit the data and demonstrate a few things. We'll also fit a linear mixed effects model so we can demonstrate the important difference between GLMM and LMM in terms of the interpretation of fixed effects. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(090901) ## set seed so we all get the same results\n\nMachine <- as.factor(rep(1:10, rep(4, 10))) # 10 machines, each with 4 defect counts\nTime <- rep(1:4, 10) # Time variable: 1,2,3,4 for each Machine\n\nRE <- rnorm(10, 0, 2) # generate 10 random effects, one for each Machine\n                              # random effect variance is  4.\neta <- rep(RE, rep(4, 10)) # replicate the random effects four times for each Machine\n\n# See what we have so far:\ncbind(Machine, Time, eta)[1:12, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Machine Time        eta\n [1,]       1    1 -1.9025790\n [2,]       1    2 -1.9025790\n [3,]       1    3 -1.9025790\n [4,]       1    4 -1.9025790\n [5,]       2    1  4.9604863\n [6,]       2    2  4.9604863\n [7,]       2    3  4.9604863\n [8,]       2    4  4.9604863\n [9,]       3    1 -0.3009368\n[10,]       3    2 -0.3009368\n[11,]       3    3 -0.3009368\n[12,]       3    4 -0.3009368\n```\n:::\n:::\n\n\nThe R output above shows the data for the first three machines. Each machine has four time points and one random effect that remains the same for each time point. Now let's simulate defects from a mixed effects Poisson regression model where the true coefficient for time is 1.1 when $\\lambda$ is on the log scale.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100233)\nllambda <-  1.1 * Time + eta # lambda on the log scale\nlambda <- exp(llambda)\nDefects <- rpois(40, lambda)\nSimu_dat <- data.frame(Machine,Time,eta,Defects)\nhead(Simu_dat)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|Machine | Time|       eta| Defects|\n|:-------|----:|---------:|-------:|\n|1       |    1| -1.902579|       0|\n|1       |    2| -1.902579|       1|\n|1       |    3| -1.902579|       3|\n|1       |    4| -1.902579|      10|\n|2       |    1|  4.960486|     407|\n|2       |    2|  4.960486|    1233|\n\n</div>\n:::\n:::\n\n\nNow, let's take a look at the data, on the data scale:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = Simu_dat, aes(Time, Defects, group=Machine)) + geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nIt's actually difficult to visualize the data on the data scale because there's one machine with *a lot* of defects, and so all the defects for the other machines are squashed down below about 1000 on the y-axis. Let's recreate the plot using the log of the defects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = Simu_dat, aes(Time, log(Defects+0.1), group=Machine)) + geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nNow, let's fit a GLMM to the simulated data and after that we'll generate a plot of the results. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim1 <- glmer(Defects ~ Time + (1|Machine),\n\tdata = Simu_dat, family = poisson)\nsummary(sim1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Defects ~ Time + (1 | Machine)\n   Data: Simu_dat\n\n     AIC      BIC   logLik deviance df.resid \n   305.4    310.4   -149.7    299.4       37 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.6119 -0.6806 -0.1589  0.4385  2.6663 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n Machine (Intercept) 3.673    1.916   \nNumber of obs: 40, groups:  Machine, 10\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.17030    0.60862    0.28     0.78    \nTime         1.10981    0.01003  110.67   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n     (Intr)\nTime -0.059\n```\n:::\n:::\n\n\nRemember that we simulated these data with $\\beta_0 = 0$, $\\beta_1 = 1.1$ and the random effect variance, $\\sigma^2_{\\eta} = 4$. The model summary gives $\\hat{\\beta}_0 = 0.17 (0.61)$; $\\hat{\\beta}_1 = 1.11 (0.01)$ and $\\hat{\\sigma}^2_{\\eta} = 3.67$ The numbers in parentheses after the fixed effect estimates are the standard errors corresponding to those estimates. The GLMM does a very good job of estimating the  the parameters, which should not be all that surprising since this is a relatively simple simulation. \n\nThe bigger take-away is still to come. Now, we'll plot the fitted model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSimu_dat$fits <- predict(sim1)\nggplot(Simu_dat,aes(Time,log(Defects+0.1),by=Machine)) + geom_point() + geom_line(aes(Time,fits))\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis plot shows the fitted model for each Machine. Let's now add the line that we obtain from just the fixed effects estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Simu_dat,aes(Time,log(Defects+0.1),by=Machine)) + geom_point() + geom_line(aes(Time,fits)) + geom_abline(intercept = 0.17, slope = 1.11,color = \"red\",linewidth=1.5)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThis all looks OK -- the fitted line from just the fixed effect estimates (red, thicker line) certainly looks like the average of the fitted lines for all of the machines.\n\n> If you are content to describe the results on the model scale, in this case in terms of log(Defects), then you can interpret the fixed effects estimates without first conditioning on the random effects.\n\nWhat if we look at this plot back on the data scale, however?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Simu_dat,aes(Time,Defects,by=Machine)) + geom_point() + geom_line(aes(Time,exp(fits))) + geom_line(aes(x =Time,y=exp(0.17+1.11*Time)),color=\"red\",linewidth=1.5)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nNow, back on the data scale, we see the problem -- the fixed effects estimates by themselves do not well-represent everything that these data contain. The one machine that has unusually high numbers of defects is *not at all* well-represented by the thick, red line. If you were to communicate the results using the fixed effects estimates only, you would miss a major part of this story: there's a big problem with one of the machines! \n\nLet's dig a little deeper still. What if we remove that one machine with the large number of defects from the plot above?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntapply(Simu_dat$Defects,Simu_dat$Machine,sum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    1     2     3     4     5     6     7     8     9    10 \n   14 17016    80   100   136    17   209    50   183   604 \n```\n:::\n:::\n\n\nIt's Machine 2 that has all the problems. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(subset(Simu_dat,Machine!=2),aes(Time,Defects,by=Machine)) + geom_point() + geom_line(aes(Time,exp(fits))) + geom_line(aes(x =Time,y=exp(0.17+1.11*Time)),color=\"red\",size=1.5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nEven among the remaining machines, you can see that the fixed effect estimates do not really represent what's going on here -- there remains another machine that in particular doesn't look like the others in terms of numbers of defects.\n\nEven though these are simulated data, you can see that in this particular case, the interesting answer *isn't* about the fixed effects estimates, it's about the identification of one Machine (maybe more!) that has an unusual number of defects relative to the others.\n\nThere's a good display in Agresti, Figure 13.1 on page 496, that provides another demonstration of how the fixed effect estimates from the GLMM does not well-represent all the structure in the data. \n\n> Because of the presence of the random effect, and the non-linearity of the model on the data scale, the results from GLMM must be interpreted conditionally upon the random effects. \n\n# Epilepsy Example\n\nNow that we've examined some simulated data, let's look at some real data to see how GLMM can be applied to actual data. The epilepsy data set contains information on the number of epilepsy attacks patients had during four follow-up periods post-treatment. Each patient received either an experimental anti-seizure treatment or a placebo, and for each patient additional information -- baseline number of epilepsy attacks in the 8 weeks prior to randomization and age -- is also included. The question of interest here is whether the treatment, a drug called **progabide**, reduced the number of seizures.\n\nLet's start by taking a look at the data and its help file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ?epilepsy\ndata(epilepsy)\nhead(epilepsy)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|  ID| Y1| Y2| Y3| Y4| Base| Age|Trt     | Ysum| Age10| Base4|\n|---:|--:|--:|--:|--:|----:|---:|:-------|----:|-----:|-----:|\n| 104|  5|  3|  3|  3|   11|  31|placebo |   14|   3.1|  2.75|\n| 106|  3|  5|  3|  3|   11|  30|placebo |   14|   3.0|  2.75|\n| 107|  2|  4|  0|  5|    6|  25|placebo |   11|   2.5|  1.50|\n| 114|  4|  4|  1|  4|    8|  36|placebo |   13|   3.6|  2.00|\n| 116|  7| 18|  9| 21|   66|  22|placebo |   55|   2.2| 16.50|\n| 118|  5|  2|  8|  7|   27|  29|placebo |   22|   2.9|  6.75|\n\n</div>\n:::\n\n```{.r .cell-code}\nxtabs(formula=~Trt,data=epilepsy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrt\n  placebo progabide \n       28        31 \n```\n:::\n:::\n\n\nThe variable `Ysum` contains the cumulative sum of epilepsy attacks beginning at the treatment period to the end of the 8-week study while the variable `Base` contains the total epilepsy attacks in the 8 weeks prior to treatment. We might first investigate if the average number of seizures per week after treatment is related to the average number of seizures per week prior to treatment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=epilepsy) +\n  geom_point(aes(x = Base / 8, Ysum / 8)) +\n  xlab(\"Seizures per Week before Treatment\") +\n  ylab(\"Seizures per Week after Treatment\") + facet_wrap(~Trt) +\n  geom_abline(slope = 1,intercept = 0) ## reference line\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nFrom these plots, there does seem to be a relationship between average number of seizures before and after treatment though it's somewhat difficult to see everything we might want to see because much of the data are clumped at low numbers of seizures. Let's try plotting the data on the log-log scale, adding a small number to each of the counts to avoid the log(0) problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=epilepsy) +\n  geom_point(aes(x = log(Base+0.1), y = log(Ysum+0.1))) + \n  xlab(\"Seizures per Week before Treatment (log scale)\") + \n  ylab(\"Seizures per Week after Treatment (log scale)\") +\n  facet_wrap(~Trt) +\n  geom_abline(slope = 1,intercept = 0)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nIt certainly seems as though patients who have a lot of seizures before treatment also have a lot of them after treatment, regardless of whether they actually receive the treatment or the placebo.\n\nThere are any number of things we might do next. For example, we can also look at a table of improvements to see what proportion of patients on the placebo improved after the treatment period and what proportion of patients on the treatment improved after the treatment period.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepilepsy$improvement <- ifelse(epilepsy$Ysum/8 < epilepsy$Base/8,\n                               \"improved\",\"didn't\")\nxtabs(formula=~Trt+improvement, data=epilepsy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           improvement\nTrt         didn't improved\n  placebo       16       12\n  progabide     10       21\n```\n:::\n:::\n\n\nSo 12/28 or 43% improved on placebo, whereas 21/31 or 68% improved on\nprogabide. Let's look at odds of improvement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuccess = c(12, 21) ## number who improved on the placebo, progabide\nfail = c(16, 10) ## number who did not improve on the placebo, progabide\nresp = cbind(success, fail)\ntrt = c(0, 1) ## coding 0 as the placebo group, 1 as progabide\nout = glm(resp ~ trt, family=binomial) ## simple GLM \nsummary(out)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = resp ~ trt, family = binomial)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -0.2877     0.3819  -0.753   0.4513  \ntrt           1.0296     0.5417   1.901   0.0573 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3.7305e+00  on 1  degrees of freedom\nResidual deviance: 8.8818e-16  on 0  degrees of freedom\nAIC: 11.552\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n\n```{.r .cell-code}\n1-pchisq(3.73, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05344339\n```\n:::\n:::\n\n\nWith a p-value of 0.05344, there is moderate to weak evidence of a treatment effect. The odds of improvement on progabide are estimated to be 2.8 times the odds of improvement on placebo.\n\n**However**, notice that so far, we have disregarded a lot of the information here (Age of patient and during which of the time periods the seizures occurred). If the researcher's question of interest is only in reducing the total number of seizures in the 8-week follow-up period, then perhaps one of the methods above would suffice. The structure of the experiment, however, with repeated measurements per subject, suggests that we might also learn something about the progression of seizures over the eight week post-treatment period. For this, we'll turn to a GLMM using subject as the random effect. \n\nWe first have to gather all of the responses, Y1, Y2, Y3, and Y4, together into one column (called `Seizures`), and add a new column (`Visit`) to keep track of which seizure counts correspond to which visits, post-treatment. Then we'll be able to look at plots of the counts across time. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nepil_long <- pivot_longer(epilepsy, c(Y1, Y2, Y3, Y4), names_to=\"Visit\", values_to=\"Seizures\")\nepil_long %<>% arrange(.,ID)  # sort by patient ID\nepil_long %<>% mutate(.,ID = as.factor(ID),Visit = rep(1:4,59))\n        # change ID to factor and Visit to numeric\nhead(epil_long)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|ID  | Base| Age|Trt       | Ysum| Age10| Base4|improvement | Visit| Seizures|\n|:---|----:|---:|:---------|----:|-----:|-----:|:-----------|-----:|--------:|\n|101 |   76|  18|progabide |   42|   1.8|  19.0|improved    |     1|       11|\n|101 |   76|  18|progabide |   42|   1.8|  19.0|improved    |     2|       14|\n|101 |   76|  18|progabide |   42|   1.8|  19.0|improved    |     3|        9|\n|101 |   76|  18|progabide |   42|   1.8|  19.0|improved    |     4|        8|\n|102 |   38|  32|progabide |   28|   3.2|   9.5|improved    |     1|        8|\n|102 |   38|  32|progabide |   28|   3.2|   9.5|improved    |     2|        7|\n\n</div>\n:::\n\n```{.r .cell-code}\nggplot(epil_long,aes(Visit,Seizures,by=ID)) + geom_line() + facet_wrap(~Trt)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nWell, these look a  little bit like spaghetti. Let's check on the log scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(epil_long,aes(Visit,log(Seizures+0.1),by=ID)) + geom_line() + facet_wrap(~Trt)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nStill spaghetti, but at least we can see a couple of things:\n\n1. It's not clear that progabide is generally better, and for one patient it's particularly bad (though we should check the baseline number of seizures for that patient).\n\n2. It's not clear that there are differences in seizure numbers through time on either medication (although it's a little hard to tell when some of the counts jump down to zero).\n\nJust to make a thorough examination of the data, let's check for any differences in the Ages or in the Baseline numbers of seizures between the two groups. This was a randomized study, so there *shouldn't* be differences in these covariates (i.e., the randomization should balance these out), but it never hurts to check. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = epilepsy, aes(x = Trt, y = Base)) + geom_jitter(width=.01)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = epilepsy, aes(x = Trt, y = Age)) + geom_jitter(width=.01)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n\nThere's not really any evidence that age or baseline number of attacks differ between the placebo and progabide group.  There is one subject in the progabide group with an unusually high number of seizures at baseline -- you can check to see if this is the same patient with the unusually high number after treatment. \n\nLet's fit a GLMM and see if our visual inspections are validated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- glmer(Seizures ~ Trt + Base + Age + Visit +\n\t\t(1|ID), family = poisson, data = epil_long)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Seizures ~ Trt + Base + Age + Visit + (1 | ID)\n   Data: epil_long\n\n     AIC      BIC   logLik deviance df.resid \n  1349.2   1370.0   -668.6   1337.2      230 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2867 -0.8240 -0.1332  0.5549  7.2848 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.2856   0.5344  \nNumber of obs: 236, groups:  ID, 59\n\nFixed effects:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.666765   0.410081   1.626  0.10396    \nTrtprogabide -0.261043   0.154119  -1.694  0.09031 .  \nBase          0.027235   0.002799   9.731  < 2e-16 ***\nAge           0.014254   0.012548   1.136  0.25597    \nVisit        -0.058725   0.020178  -2.910  0.00361 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Trtprg Base   Age   \nTrtprogabid -0.289                     \nBase        -0.392 -0.006              \nAge         -0.931  0.114  0.190       \nVisit       -0.119  0.000  0.000  0.000\n```\n:::\n:::\n\n\n## Picking the Random Effects\n\nIn the model we just fit, we used a random intercepts model that just includes a random effect for patient. Specifically what this means that is the intercept term will be estimated differently for each patient. \n\nDepending on the particular problem, this may or may not be a reasonable way to think about the variation in the data.  In the context of this epilepsy example, it could be that the different patients show different *rates of change* in their numbers of seizures -- this would suggest that each patient have his or her own effect of visit. We can accomplish this by putting in a random effect for visit.\n\nNext we'll fit a random intercepts and slopes model with `Visit` as the random slope. Again, this seems like a reasonable thing to do if we expect that some subjects will have a different rate of increase or decrease of number of seizures over time than other subjects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 <- glmer(Seizures ~ Trt + Base + Age + Visit + (1 + Visit|ID), family = poisson, data = epil_long)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00368596 (tol = 0.002, component 1)\n```\n:::\n\n```{.r .cell-code}\n# oops this didn't converge --- try the initialization trick again:\n(init <- getME(mod2, name = c(\"theta\", \"fixef\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$theta\n      ID.(Intercept) ID.Visit.(Intercept)             ID.Visit \n          0.60038496          -0.06873505           0.12686483 \n\n$fixef\n (Intercept) Trtprogabide         Base          Age        Visit \n  0.61274912  -0.25315623   0.02736649   0.01545712  -0.05904358 \n```\n:::\n\n```{.r .cell-code}\nmod2 <- glmer(Seizures ~ Trt + Base + Age + Visit + (1 + Visit|ID), family = poisson, data = epil_long, start = init)\nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Seizures ~ Trt + Base + Age + Visit + (1 + Visit | ID)\n   Data: epil_long\n\n     AIC      BIC   logLik deviance df.resid \n  1333.1   1360.8   -658.5   1317.1      228 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9215 -0.7439 -0.0919  0.5534  6.3674 \n\nRandom effects:\n Groups Name        Variance Std.Dev. Corr \n ID     (Intercept) 0.36044  0.6004        \n        Visit       0.02082  0.1443   -0.48\nNumber of obs: 236, groups:  ID, 59\n\nFixed effects:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.61284    0.41536   1.475   0.1401    \nTrtprogabide -0.25314    0.15324  -1.652   0.0986 .  \nBase          0.02737    0.00278   9.843   <2e-16 ***\nAge           0.01545    0.01273   1.214   0.2247    \nVisit        -0.05904    0.03252  -1.816   0.0694 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Trtprg Base   Age   \nTrtprogabid -0.290                     \nBase        -0.398 -0.003              \nAge         -0.923  0.122  0.198       \nVisit       -0.128 -0.018  0.009 -0.056\n```\n:::\n:::\n\n\nThe estimated variance for the `Visit` random effect is pretty small (0.02), but the fixed effect estimate for `Visit` is pretty small too (-0.06), so that random effect variance may still be important to consider. Remember that we can compare the two models with the `AIC` and `BIC` functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(mod1, mod2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|     | df|      AIC|\n|:----|--:|--------:|\n|mod1 |  6| 1349.228|\n|mod2 |  8| 1333.084|\n\n</div>\n:::\n\n```{.r .cell-code}\nBIC(mod1, mod2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|     | df|      BIC|\n|:----|--:|--------:|\n|mod1 |  6| 1370.011|\n|mod2 |  8| 1360.794|\n\n</div>\n:::\n:::\n\n\nBoth criteria indicate that the model with random slopes for `Visit` is somewhat preferred over the model with only random intercepts, though simplicity and that the AIC and BIC are not very different (i.e., each for `mod1` vs `mod2`) would argue for using the model with only one random effect. As mentioned in last lab, in a real problem situation, we really want to think more carefully about what the model should be **before** fitting anything to the data. Choosing which model after fitting many models is a type of data snooping and should generally be avoided.\n\n## Looking at the results\n\nLet's go back to the random intercept only model and interpret/understand the model output. This model is already a bit complex but is somewhat simpler than the random intercepts and random slopes model (`mod2`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Seizures ~ Trt + Base + Age + Visit + (1 | ID)\n   Data: epil_long\n\n     AIC      BIC   logLik deviance df.resid \n  1349.2   1370.0   -668.6   1337.2      230 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2867 -0.8240 -0.1332  0.5549  7.2848 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.2856   0.5344  \nNumber of obs: 236, groups:  ID, 59\n\nFixed effects:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.666765   0.410081   1.626  0.10396    \nTrtprogabide -0.261043   0.154119  -1.694  0.09031 .  \nBase          0.027235   0.002799   9.731  < 2e-16 ***\nAge           0.014254   0.012548   1.136  0.25597    \nVisit        -0.058725   0.020178  -2.910  0.00361 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Trtprg Base   Age   \nTrtprogabid -0.289                     \nBase        -0.392 -0.006              \nAge         -0.931  0.114  0.190       \nVisit       -0.119  0.000  0.000  0.000\n```\n:::\n:::\n\n\nBefore we comment on the model summary information, let's perform some diagnostics to see if we feel like we have a good model to work with.\n\n### Residual Diagnostics\n\nLet's also take a look at some residual plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepil_long$resid <- residuals(mod1)\nepil_long$fits <- exp(predict(mod1))\n\n# first some plots of the residuals vs. explanatory variables \nggplot(epil_long,aes(Visit,resid)) + geom_point() + facet_wrap(~Trt)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(epil_long,aes(Base,resid)) + geom_point() + facet_wrap(~Trt)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-24-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(epil_long,aes(Age,resid)) + geom_point() + facet_wrap(~Trt)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-24-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# now a look at the normality of the random effects\nqqnorm(unlist((ranef(mod1)$ID)))\nqqline(unlist((ranef(mod1)$ID)))\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-24-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# finally, fitted values versus observations\nggplot(epil_long,aes(Seizures,fits)) + geom_point() + facet_wrap(~Trt) + geom_abline()\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-24-5.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(epil_long,aes(log(Seizures+0.1),log(fits+0.1))) + geom_point() + facet_wrap(~Trt) + geom_abline()\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-24-6.png){width=672}\n:::\n:::\n\n\nThere are a number of things to discuss based on these plots:\n\n1. It looks like there's evidence of some curvilinearity in the relationship between `Visit` and `Seizures`; the nature of that curvilinearity might be different for the two treatment groups (look at plot of `resid` versus `Visit`).  You could add `Visit2` term, the square of `Visit`, or you could revert `Visit` back to a factor variable.\n\n2. There is what appears to be one large outlier in the placebo group. If we had access to the original researchers, we might have been able to find out about that unusual value.\n\n3. It seems that the random effects are not strictly Normal, especially in the upper tail. This may be because we haven't gotten the fixed effects part of the model correctly specified.\n\n4. There are some zeroes among the `Seizure` values, and the model is not fitting those very well -- we may need to think about a zero-inflated model and/or a model for over dispersion.\n\n## More Model Fitting\n\nThere is a `glmer.nb()` function in the `lme4` package, so we'll try using that below. There are zero-inflated GLMM models, but they get rather complicated. We'll leave it to you to explore those if you're interested. Also, you might want to investigate what happens when you put terms in the model for the square of `Visit` and the interactions between `Trt` and the `Visit` variables (i.e., `Visit` and it's square). Again, we'll leave this to you for further exploration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first, glmer.nb with the same form as above\nmod3 <- glmer.nb(Seizures ~ Trt + Age + Base + Visit + (1|ID), data = epil_long)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0181557 (tol = 0.002, component 1)\n```\n:::\n\n```{.r .cell-code}\ninit <- getME(mod3,name=c(\"theta\",\"fixef\"))\nmod3 <- glmer.nb(Seizures ~ Trt + Age + Base + Visit + (1|ID), data = epil_long, start = init)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00295823 (tol = 0.002, component 1)\n```\n:::\n\n```{.r .cell-code}\nsummary(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: Negative Binomial(7.4718)  ( log )\nFormula: Seizures ~ Trt + Age + Base + Visit + (1 | ID)\n   Data: epil_long\n\n     AIC      BIC   logLik deviance df.resid \n  1269.2   1293.4   -627.6   1255.2      229 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9219 -0.6026 -0.1096  0.4506  3.6929 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID     (Intercept) 0.2522   0.5021  \nNumber of obs: 236, groups:  ID, 59\n\nFixed effects:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.662022   0.416604   1.589   0.1120    \nTrtprogabide -0.260516   0.154359  -1.688   0.0915 .  \nAge           0.014057   0.012567   1.119   0.2633    \nBase          0.027184   0.002806   9.687   <2e-16 ***\nVisit        -0.052231   0.033086  -1.579   0.1144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Trtprg Age    Base  \nTrtprogabid -0.286                     \nAge         -0.919  0.115              \nBase        -0.387 -0.006  0.189       \nVisit       -0.205  0.004  0.010  0.001\n```\n:::\n:::\n\n\nLet's compare this model to the Poisson GLMM above (`mod1`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(mod1,mod3)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|     | df|      AIC|\n|:----|--:|--------:|\n|mod1 |  6| 1349.228|\n|mod3 |  7| 1269.198|\n\n</div>\n:::\n\n```{.r .cell-code}\nBIC(mod1,mod3)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|     | df|      BIC|\n|:----|--:|--------:|\n|mod1 |  6| 1370.011|\n|mod3 |  7| 1293.445|\n\n</div>\n:::\n:::\n\n\nOK, it seems like the negative binomial model is more appropriate here, so we'll proceed with that. If you look at the summary information for `mod3` it now appears that there's still only marginal evidence in support of a treatment effect (p = 0.09).  Next, we've replicated the residual diagnostics that we showed above, but for the `mod3` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepil_long$resid <- residuals(mod3)\nepil_long$fits <- exp(predict(mod3))\n\n# first some plots of the residuals vs. explanatory variables \nggplot(epil_long,aes(Visit,resid)) + geom_point() + facet_wrap(~Trt)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(epil_long,aes(Base,resid)) + geom_point() + facet_wrap(~Trt)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-27-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(epil_long,aes(Age,resid)) + geom_point() + facet_wrap(~Trt)\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-27-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# now a look at the normality of the random effects\nqqnorm(unlist((ranef(mod1)$ID)))\nqqline(unlist((ranef(mod1)$ID)))\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-27-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# finally, fitted values versus observations\nggplot(epil_long,aes(Seizures,fits)) + geom_point() + facet_wrap(~Trt) + geom_abline()\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-27-5.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(epil_long,aes(log(Seizures+0.1),log(fits+0.1))) + geom_point() + facet_wrap(~Trt) + geom_abline()\n```\n\n::: {.cell-output-display}\n![](z7_learnR_files/figure-html/unnamed-chunk-27-6.png){width=672}\n:::\n:::\n\n\nWe still see many of the same problems we noted above, but despite that we won't proceed beyond this model for the purpose of this Lab. *If* this were the final model from which we were to report results, we might say something like:\n\n> We fit a generalized linear mixed model to the epilespy data, assuming that the number of seizures follows a negative binomial distribution and with a log link. The fixed effects are age, baseline number of seizures and visit, and the random effect is subject, to account for the repeated measurements for each subject. From this model, there is no evidence of an effect of the progabide treatment (p = 0.09), and the only factor that appears to be associated with the number of seizures post-treatment is the baseline number of seizures.\n\nOne final note. *If* our final model does provide evidence of a treatment effect, we can go ahead and report the p-value. It will be difficult to quantify the treatment effect, however, because you'll have to do that in the context of conditioning on the random subjects. \n\n# Lab Questions\n\nIn the `epilepsy` data, we really need to use a mixed effects model to account for the fact that observations within a subject are almost certainly not independent. Let's suppose, however, that we are a naive statistician and do not realize that we need to account for this within subject correlation. Instead we fit a Poisson GLM and completely ignore the fact that we have non-independent observations (to be clear, this is **not** a correct way to do the analysis, but we are fitting the model to compare the results we get to the results from the more appropriate GLMM we fit above).\n\nPlease write 1-2 sentences in response to the following questions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.norand <- glm.nb(Seizures ~ Trt + Age + Base + Visit, data = epil_long)\nsummary(mod.norand)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm.nb(formula = Seizures ~ Trt + Age + Base + Visit, data = epil_long, \n    init.theta = 2.417515912, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.661252   0.292126   2.264   0.0236 *  \nTrtprogabide -0.189856   0.101516  -1.870   0.0615 .  \nAge           0.017817   0.008234   2.164   0.0305 *  \nBase          0.026865   0.001756  15.299   <2e-16 ***\nVisit        -0.044973   0.045012  -0.999   0.3177    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.4175) family taken to be 1)\n\n    Null deviance: 554.89  on 235  degrees of freedom\nResidual deviance: 265.34  on 231  degrees of freedom\nAIC: 1325.4\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.418 \n          Std. Err.:  0.325 \n\n 2 x log-likelihood:  -1313.417 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod3)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Estimate  Std. Error   z value     Pr(>|z|)\n(Intercept)   0.66202163 0.416603888  1.589091 1.120398e-01\nTrtprogabide -0.26051591 0.154359024 -1.687727 9.146363e-02\nAge           0.01405663 0.012566872  1.118547 2.633335e-01\nBase          0.02718444 0.002806305  9.686917 3.427190e-22\nVisit        -0.05223065 0.033086092 -1.578628 1.144213e-01\n```\n:::\n\n```{.r .cell-code}\nsummary(mod.norand)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Estimate  Std. Error    z value     Pr(>|z|)\n(Intercept)   0.66125191 0.292126465  2.2635810 2.359990e-02\nTrtprogabide -0.18985590 0.101516373 -1.8701998 6.145608e-02\nAge           0.01781657 0.008234001  2.1637805 3.048120e-02\nBase          0.02686460 0.001755952 15.2991685 7.743965e-53\nVisit        -0.04497306 0.045011777 -0.9991399 3.177269e-01\n```\n:::\n:::\n\n\n1.) How do the coefficient estimates for the fixed effects compare in the two models? Why do you think the coefficient estimates are approximately the same or very different?\n\n2.) How do the standard errors for the coefficients compare in the two models? Why do you think the standard errors are approximately the same or very different?\n\n3.) How do the p-values compare for the two models? \n\n4.) Based on your answers from (1) to (3), what is the danger in assuming independence between all the observations, when, in fact, some of the observations are not independent?\n\nLet's next look at the first 40 fitted values for the incorrectly fitted model without a random effect for subject and the GLMM with a random effect for subject.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(fitted(mod.norand), fitted(mod3))[1:40, ] |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       [,1]      [,2]\n1 16.262001 11.823995\n2 15.546851 11.222271\n3 14.863151 10.651169\n4 14.209517 10.109130\n5  7.518812  7.237358\n6  7.188159  6.869049\n```\n:::\n:::\n\n\n5.) How do the fitted values from the two models compare? Are the fitted values from one model consistently higher or lower than the other model?\n\n6.) Notice fitted values 37 through 40. These values correspond to the 10th subject in the data set (subject ID 112). Can you justify why these fitted values are much higher in the GLMM? In your justification, can you include a plot of some sort that highlights this particular subject?\n\n\n\n\n",
    "supporting": [
      "z7_learnR_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}